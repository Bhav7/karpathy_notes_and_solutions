{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the prior GPT video, we tokenized on the level of characters; however, in practice, it is often at \"chunk\" level\n",
    "\n",
    "\n",
    "A lot of the weirdness with LLMS is due to tokenization, i.e cant spell, cant process strings (ex. reverse str), worse at non-english languages, cant do basic math, trouble coding Python, will randomly stop after seeing \"|endoftext| string, warnings about \"trailing whitespace\", will break if asked about \"SolidGoldMagikarp\", why it prefers the use of YAML over JSON, why it LLM is not actually end-to-end language modelling\n",
    "\n",
    "Visualize Tokenization: https://tiktokenizer.vercel.app/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex.\n",
    "\n",
    "Egg. = 2 tokens\n",
    "\n",
    "I have an Egg; in this instance Egg. = 1 token for \"space\" Egg, wheras Egg by itself at start of sentence is 2 tokens\n",
    "\n",
    "Main Point: For the same word, depeneding on where it is in sentence, if it is upper or lower case, all of these can have very different tokens. The Model needs to learn from the large corpus of internet, that these represent very similiar same concepts\n",
    "\n",
    "For digit tokenization, can be arbitrary, where multiple digits can be represented by a single token, wherase single digits can require multiple tokens \n",
    "\n",
    "Tokenization is often much worse for non-english because the training sets used for tokenization are primarily english. Thus\n",
    "for an english sentence with 10 words, might have 10 tokens; wheras for a korean translation, could require many more tokens \n",
    "as the chunks of the sentence are a lot more broken up. Using more tokens for the exact same thing, bloats up the sequence length for the documents, and in the attention of the transformer, when these tokens are trying to attend to each other, you\n",
    "are running out of context in the maximum context length of that transformer\n",
    "\n",
    "One of the problems with GPT2 and Python, is that each for each coding indent (as needed for Pyython), it would apply multiple \"262\" token ids (represents whitespace), and this ends up bloating out the text and it is seperated across way too much of the sequence and we are running out the context length in the sequence (taking up way too much token space)\n",
    "\n",
    "\n",
    "When going from GPT2 to GPT4 tokenizer, a 300 token sequence, now equals around 180 token sequence, and this is because GPT4\n",
    "uses more tokens (i.e has more tokens in vocab); provides a more denser input to Transformer (now can see twice as much text as a context for what token to predict next because of this change). However, making vocab just infinetely large is not a good idea since both the embedding table increases alongside the output Softmax dimension.\n",
    "\n",
    "One important improvement in GPT4 tokenizer, is how they handled white space in Python code (requiring much fewer tokens as grouped alot of white space into single char), this allows for a more dense input to Transformer, allowing it to attend to more Code before it when trying to predict the next token in the seqeunece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unicode: 150K characters (i.e symbols, emojis, letters etc...) and what integers represent those characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To access unicode code point \n",
    "ord(\"h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128075"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord(\"ğŸ‘‹\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ord() expected a character, but string of length 5 found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43mord\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHello\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: ord() expected a character, but string of length 5 found"
     ]
    }
   ],
   "source": [
    "ord(\"Hello\")\n",
    "#Will not work, as the function takes in a single character and returns its unicode code point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50504,\n",
       " 45397,\n",
       " 54616,\n",
       " 49464,\n",
       " 50836,\n",
       " 32,\n",
       " 128075,\n",
       " 32,\n",
       " 40,\n",
       " 104,\n",
       " 101,\n",
       " 108,\n",
       " 108,\n",
       " 111,\n",
       " 32,\n",
       " 105,\n",
       " 110,\n",
       " 32,\n",
       " 75,\n",
       " 111,\n",
       " 114,\n",
       " 101,\n",
       " 97,\n",
       " 110,\n",
       " 33,\n",
       " 41]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ord(x) for x in \"ì•ˆë…•í•˜ì„¸ìš” ğŸ‘‹ (hello in Korean!)\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We dont use unicode code points for tokenization because a) large vocab(150K) and b) the unicode standard changes over time\n",
    "\n",
    "Instead we use encodings (a way to take unicode text and translate it into binary data), EX. UTF-8 (will take a unicode code point and translate it a Byte Stream of 1 to 4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\xec\\x95\\x88\\xeb\\x85\\x95\\xed\\x95\\x98\\xec\\x84\\xb8\\xec\\x9a\\x94 \\xf0\\x9f\\x91\\x8b (hello in Korean!)'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"ì•ˆë…•í•˜ì„¸ìš” ğŸ‘‹ (hello in Korean!)\".encode(\"utf-8\")\n",
    "#returns bytes object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[236,\n",
       " 149,\n",
       " 136,\n",
       " 235,\n",
       " 133,\n",
       " 149,\n",
       " 237,\n",
       " 149,\n",
       " 152,\n",
       " 236,\n",
       " 132,\n",
       " 184,\n",
       " 236,\n",
       " 154,\n",
       " 148,\n",
       " 32,\n",
       " 240,\n",
       " 159,\n",
       " 145,\n",
       " 139,\n",
       " 32,\n",
       " 40,\n",
       " 104,\n",
       " 101,\n",
       " 108,\n",
       " 108,\n",
       " 111,\n",
       " 32,\n",
       " 105,\n",
       " 110,\n",
       " 32,\n",
       " 75,\n",
       " 111,\n",
       " 114,\n",
       " 101,\n",
       " 97,\n",
       " 110,\n",
       " 33,\n",
       " 41]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using list gives raw bytes representing input string\n",
    "list(\"ì•ˆë…•í•˜ì„¸ìš” ğŸ‘‹ (hello in Korean!)\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cant use UTF-8 naively, because these are Byte Streams, and that would indicate a vocab length of 256 possible tokens. This is too small of a vocab size, which in turn would stretch out our text over very very long sequences of bytes, and in turn remembering we have a finite context length we can support in Transformer (due to computation reasons), and so will be unable to sufficiently attend to long sequence for next token prediction\n",
    "\n",
    "We want to stick with UTF-8 encoding but allow for a tunable hyperparameter that can be used to generate larger vocab size\n",
    "\n",
    "Thus, use Byte-Pair Encoding algo, which allows us to compress these Byte Sequences to a variable amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[104, 105, 236, 154, 148]"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list((\"hiìš”\").encode(\"utf-8\")) #2 ints for 2 english chars, 3 ints for 1 non-english char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Byte-Pair Encoding Algo.\n",
    "\n",
    "aaabcaaabac\n",
    "11 tokens, vocab = 4\n",
    "Find the pair of tokens which occur more often, and replace with a new Token which we append to vocab\n",
    "\n",
    "ZabdZabac\n",
    "Z=aa\n",
    "9 tokens, vocab = 5\n",
    "\n",
    "This process is repeated, Y= \"ab\"\n",
    "ZYdZYac\n",
    "7 tokens, vocab = 6\n",
    "\n",
    "Repeat, find the pair ZY is common, X = ZY\n",
    "XdXac\n",
    "5 tokens, vocab = 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "ï¼µï½ï½‰ï½ƒï½ï½„ï½…! ğŸ…¤ğŸ…ğŸ…˜ğŸ…’ğŸ…ğŸ…“ğŸ…”â€½ ğŸ‡ºâ€ŒğŸ‡³â€ŒğŸ‡®â€ŒğŸ‡¨â€ŒğŸ‡´â€ŒğŸ‡©â€ŒğŸ‡ª! ğŸ˜„ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to â€œsupport Unicodeâ€ in our software (whatever that meansâ€”like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I donâ€™t blame programmers for still finding the whole thing mysterious, even 30 years after Unicodeâ€™s inception.\n",
      "length: 533\n",
      "---\n",
      "[239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131, 239, 189, 143, 239, 189, 132, 239, 189, 133, 33, 32, 240, 159, 133, 164, 240, 159, 133, 157, 240, 159, 133, 152, 240, 159, 133, 146, 240, 159, 133, 158, 240, 159, 133, 147, 240, 159, 133, 148, 226, 128, 189, 32, 240, 159, 135, 186, 226, 128, 140, 240, 159, 135, 179, 226, 128, 140, 240, 159, 135, 174, 226, 128, 140, 240, 159, 135, 168, 226, 128, 140, 240, 159, 135, 180, 226, 128, 140, 240, 159, 135, 169, 226, 128, 140, 240, 159, 135, 170, 33, 32, 240, 159, 152, 132, 32, 84, 104, 101, 32, 118, 101, 114, 121, 32, 110, 97, 109, 101, 32, 115, 116, 114, 105, 107, 101, 115, 32, 102, 101, 97, 114, 32, 97, 110, 100, 32, 97, 119, 101, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 104, 101, 97, 114, 116, 115, 32, 111, 102, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 119, 111, 114, 108, 100, 119, 105, 100, 101, 46, 32, 87, 101, 32, 97, 108, 108, 32, 107, 110, 111, 119, 32, 119, 101, 32, 111, 117, 103, 104, 116, 32, 116, 111, 32, 226, 128, 156, 115, 117, 112, 112, 111, 114, 116, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 157, 32, 105, 110, 32, 111, 117, 114, 32, 115, 111, 102, 116, 119, 97, 114, 101, 32, 40, 119, 104, 97, 116, 101, 118, 101, 114, 32, 116, 104, 97, 116, 32, 109, 101, 97, 110, 115, 226, 128, 148, 108, 105, 107, 101, 32, 117, 115, 105, 110, 103, 32, 119, 99, 104, 97, 114, 95, 116, 32, 102, 111, 114, 32, 97, 108, 108, 32, 116, 104, 101, 32, 115, 116, 114, 105, 110, 103, 115, 44, 32, 114, 105, 103, 104, 116, 63, 41, 46, 32, 66, 117, 116, 32, 85, 110, 105, 99, 111, 100, 101, 32, 99, 97, 110, 32, 98, 101, 32, 97, 98, 115, 116, 114, 117, 115, 101, 44, 32, 97, 110, 100, 32, 100, 105, 118, 105, 110, 103, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 116, 104, 111, 117, 115, 97, 110, 100, 45, 112, 97, 103, 101, 32, 85, 110, 105, 99, 111, 100, 101, 32, 83, 116, 97, 110, 100, 97, 114, 100, 32, 112, 108, 117, 115, 32, 105, 116, 115, 32, 100, 111, 122, 101, 110, 115, 32, 111, 102, 32, 115, 117, 112, 112, 108, 101, 109, 101, 110, 116, 97, 114, 121, 32, 97, 110, 110, 101, 120, 101, 115, 44, 32, 114, 101, 112, 111, 114, 116, 115, 44, 32, 97, 110, 100, 32, 110, 111, 116, 101, 115, 32, 99, 97, 110, 32, 98, 101, 32, 109, 111, 114, 101, 32, 116, 104, 97, 110, 32, 97, 32, 108, 105, 116, 116, 108, 101, 32, 105, 110, 116, 105, 109, 105, 100, 97, 116, 105, 110, 103, 46, 32, 73, 32, 100, 111, 110, 226, 128, 153, 116, 32, 98, 108, 97, 109, 101, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 102, 111, 114, 32, 115, 116, 105, 108, 108, 32, 102, 105, 110, 100, 105, 110, 103, 32, 116, 104, 101, 32, 119, 104, 111, 108, 101, 32, 116, 104, 105, 110, 103, 32, 109, 121, 115, 116, 101, 114, 105, 111, 117, 115, 44, 32, 101, 118, 101, 110, 32, 51, 48, 32, 121, 101, 97, 114, 115, 32, 97, 102, 116, 101, 114, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 153, 115, 32, 105, 110, 99, 101, 112, 116, 105, 111, 110, 46]\n",
      "length: 616\n"
     ]
    }
   ],
   "source": [
    "text = \"ï¼µï½ï½‰ï½ƒï½ï½„ï½…! ğŸ…¤ğŸ…ğŸ…˜ğŸ…’ğŸ…ğŸ…“ğŸ…”â€½ ğŸ‡ºâ€ŒğŸ‡³â€ŒğŸ‡®â€ŒğŸ‡¨â€ŒğŸ‡´â€ŒğŸ‡©â€ŒğŸ‡ª! ğŸ˜„ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to â€œsupport Unicodeâ€ in our software (whatever that meansâ€”like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I donâ€™t blame programmers for still finding the whole thing mysterious, even 30 years after Unicodeâ€™s inception.\"\n",
    "tokens = text.encode(\"utf-8\")\n",
    "tokens = list(map(int, tokens)) #Get list of raw bytes as integers\n",
    "print(\"---\")\n",
    "print(text)\n",
    "print(\"length:\", len(text))\n",
    "print(\"---\")\n",
    "print(tokens)\n",
    "print(\"length:\", len(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[240, 159, 152, 132]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We have more tokens then text because alot of the complex unicode characters at the start of the sequence become multiple bytes (up to 4)\n",
    "list(\"ğŸ˜„\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1 #if not pair in counts.dict assign 0\n",
    "    return counts\n",
    "\n",
    "#Note: We consider all pair combinations, using a stride of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(20, (101, 32)), (15, (240, 159)), (12, (226, 128)), (12, (105, 110)), (10, (115, 32)), (10, (97, 110)), (10, (32, 97)), (9, (32, 116)), (8, (116, 104)), (7, (159, 135)), (7, (159, 133)), (7, (97, 114)), (6, (239, 189)), (6, (140, 240)), (6, (128, 140)), (6, (116, 32)), (6, (114, 32)), (6, (111, 114)), (6, (110, 103)), (6, (110, 100)), (6, (109, 101)), (6, (104, 101)), (6, (101, 114)), (6, (32, 105)), (5, (117, 115)), (5, (115, 116)), (5, (110, 32)), (5, (100, 101)), (5, (44, 32)), (5, (32, 115)), (4, (116, 105)), (4, (116, 101)), (4, (115, 44)), (4, (114, 105)), (4, (111, 117)), (4, (111, 100)), (4, (110, 116)), (4, (110, 105)), (4, (105, 99)), (4, (104, 97)), (4, (103, 32)), (4, (101, 97)), (4, (100, 32)), (4, (99, 111)), (4, (97, 109)), (4, (85, 110)), (4, (32, 119)), (4, (32, 111)), (4, (32, 102)), (4, (32, 85)), (3, (118, 101)), (3, (116, 115)), (3, (116, 114)), (3, (116, 111)), (3, (114, 116)), (3, (114, 115)), (3, (114, 101)), (3, (111, 102)), (3, (111, 32)), (3, (108, 108)), (3, (108, 101)), (3, (108, 32)), (3, (101, 115)), (3, (101, 110)), (3, (97, 116)), (3, (46, 32)), (3, (32, 240)), (3, (32, 112)), (3, (32, 109)), (3, (32, 100)), (3, (32, 98)), (2, (128, 153)), (2, (121, 32)), (2, (119, 104)), (2, (119, 101)), (2, (117, 112)), (2, (116, 97)), (2, (115, 117)), (2, (114, 121)), (2, (114, 111)), (2, (114, 97)), (2, (112, 114)), (2, (112, 112)), (2, (112, 111)), (2, (112, 108)), (2, (111, 110)), (2, (111, 103)), (2, (110, 115)), (2, (110, 111)), (2, (109, 109)), (2, (108, 105)), (2, (107, 101)), (2, (105, 116)), (2, (105, 111)), (2, (105, 107)), (2, (105, 100)), (2, (104, 116)), (2, (104, 111)), (2, (103, 114)), (2, (103, 104)), (2, (102, 116)), (2, (102, 111)), (2, (102, 32)), (2, (101, 226)), (2, (101, 118)), (2, (101, 112)), (2, (100, 111)), (2, (100, 105)), (2, (100, 97)), (2, (99, 97)), (2, (98, 101)), (2, (97, 108)), (2, (33, 32)), (2, (32, 114)), (2, (32, 110)), (2, (32, 99)), (1, (239, 188)), (1, (189, 143)), (1, (189, 142)), (1, (189, 137)), (1, (189, 133)), (1, (189, 132)), (1, (189, 131)), (1, (189, 32)), (1, (188, 181)), (1, (186, 226)), (1, (181, 239)), (1, (180, 226)), (1, (179, 226)), (1, (174, 226)), (1, (170, 33)), (1, (169, 226)), (1, (168, 226)), (1, (164, 240)), (1, (159, 152)), (1, (158, 240)), (1, (157, 240)), (1, (157, 32)), (1, (156, 115)), (1, (153, 116)), (1, (153, 115)), (1, (152, 240)), (1, (152, 132)), (1, (148, 226)), (1, (148, 108)), (1, (147, 240)), (1, (146, 240)), (1, (143, 239)), (1, (142, 239)), (1, (137, 239)), (1, (135, 186)), (1, (135, 180)), (1, (135, 179)), (1, (135, 174)), (1, (135, 170)), (1, (135, 169)), (1, (135, 168)), (1, (133, 164)), (1, (133, 158)), (1, (133, 157)), (1, (133, 152)), (1, (133, 148)), (1, (133, 147)), (1, (133, 146)), (1, (133, 33)), (1, (132, 239)), (1, (132, 32)), (1, (131, 239)), (1, (128, 189)), (1, (128, 157)), (1, (128, 156)), (1, (128, 148)), (1, (122, 101)), (1, (121, 115)), (1, (121, 101)), (1, (120, 101)), (1, (119, 111)), (1, (119, 105)), (1, (119, 99)), (1, (119, 97)), (1, (119, 32)), (1, (118, 105)), (1, (117, 116)), (1, (117, 114)), (1, (117, 103)), (1, (116, 119)), (1, (116, 116)), (1, (116, 108)), (1, (116, 63)), (1, (115, 226)), (1, (115, 111)), (1, (115, 105)), (1, (115, 101)), (1, (115, 97)), (1, (114, 117)), (1, (114, 108)), (1, (114, 100)), (1, (114, 95)), (1, (112, 116)), (1, (112, 97)), (1, (111, 122)), (1, (111, 119)), (1, (111, 116)), (1, (111, 108)), (1, (110, 226)), (1, (110, 110)), (1, (110, 101)), (1, (110, 99)), (1, (110, 97)), (1, (110, 46)), (1, (109, 121)), (1, (109, 111)), (1, (109, 105)), (1, (108, 117)), (1, (108, 100)), (1, (108, 97)), (1, (107, 110)), (1, (105, 118)), (1, (105, 109)), (1, (105, 108)), (1, (105, 103)), (1, (104, 105)), (1, (103, 115)), (1, (103, 101)), (1, (103, 46)), (1, (102, 105)), (1, (102, 101)), (1, (101, 120)), (1, (101, 109)), (1, (101, 46)), (1, (101, 44)), (1, (100, 119)), (1, (100, 45)), (1, (99, 104)), (1, (99, 101)), (1, (98, 115)), (1, (98, 108)), (1, (97, 119)), (1, (97, 103)), (1, (97, 102)), (1, (97, 98)), (1, (97, 32)), (1, (95, 116)), (1, (87, 101)), (1, (84, 104)), (1, (83, 116)), (1, (73, 32)), (1, (66, 117)), (1, (63, 41)), (1, (51, 48)), (1, (48, 32)), (1, (45, 112)), (1, (41, 46)), (1, (40, 119)), (1, (32, 226)), (1, (32, 121)), (1, (32, 118)), (1, (32, 117)), (1, (32, 108)), (1, (32, 107)), (1, (32, 104)), (1, (32, 101)), (1, (32, 87)), (1, (32, 84)), (1, (32, 83)), (1, (32, 73)), (1, (32, 66)), (1, (32, 51)), (1, (32, 40))]\n"
     ]
    }
   ],
   "source": [
    "stats = get_stats(tokens)\n",
    "print(sorted(((v,k) for k,v in stats.items()), reverse = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('e', ' ')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(101), chr(32)\n",
    "#Higlights alot of \"e \" in the text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_pair = max(stats, key = stats.get) #Max returns the maximum K, and we provide it a function stats.get which simply returns the V of the K\n",
    "top_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we will mint a new token (256) which replaces the most common pair (101,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def merge(ids, most_common_pair, new_idx):\n",
    "#     skip_flag = False\n",
    "#     new_ids = []\n",
    "#     for idx in range(len(ids)):\n",
    "#         if skip_flag == True: #This is needed to ensure that when we replace the most common pair (101,32); 101 corresponds to idx = 0 with (256), in the next iteration we dont add back 32 to new_idx as idx = 0 would now correspond to it\n",
    "#             skip_flag = False\n",
    "#             continue\n",
    "#         if idx == len(ids) - 1:\n",
    "#             new_ids.append(ids[idx])\n",
    "#             continue\n",
    "#         pair = (ids[idx], ids[idx+1])\n",
    "#         if pair == most_common_pair:\n",
    "#             new_ids.append(new_idx)\n",
    "#             skip_flag = True\n",
    "#         else:\n",
    "#             new_ids.append(ids[idx])\n",
    "#     return new_ids \n",
    "\n",
    "\"KARPATHY Implementation; both work\"\n",
    "def merge(ids, pair, idx):\n",
    "  # in the list of ints (ids), replace all consecutive occurences of pair with the new token idx\n",
    "  newids = []\n",
    "  i = 0\n",
    "  while i < len(ids):\n",
    "    # if we are not at the very last position AND the pair matches, replace it\n",
    "    if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "      newids.append(idx)\n",
    "      i += 2\n",
    "    else:\n",
    "      newids.append(ids[i])\n",
    "      i += 1\n",
    "  return newids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 6, 99, 9, 1]\n"
     ]
    }
   ],
   "source": [
    "print(merge([5,6,6,7,9,1], (6,7), 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens2 = merge(tokens, top_pair, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "596"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens) - len(tokens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making the training text longer to have more representative token statistics\n",
    "# text from https://www.reedbeta.com/blog/programmers-intro-to-unicode/\n",
    "text = \"\"\"A Programmerâ€™s Introduction to Unicode March 3, 2017 Â· Coding Â· 22 Comments  ï¼µï½ï½‰ï½ƒï½ï½„ï½…! ğŸ…¤ğŸ…ğŸ…˜ğŸ…’ğŸ…ğŸ…“ğŸ…”â€½ ğŸ‡º\\u200cğŸ‡³\\u200cğŸ‡®\\u200cğŸ‡¨\\u200cğŸ‡´\\u200cğŸ‡©\\u200cğŸ‡ª! ğŸ˜„ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to â€œsupport Unicodeâ€ in our software (whatever that meansâ€”like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I donâ€™t blame programmers for still finding the whole thing mysterious, even 30 years after Unicodeâ€™s inception.  A few months ago, I got interested in Unicode and decided to spend some time learning more about it in detail. In this article, Iâ€™ll give an introduction to it from a programmerâ€™s point of view.  Iâ€™m going to focus on the character set and whatâ€™s involved in working with strings and files of Unicode text. However, in this article Iâ€™m not going to talk about fonts, text layout/shaping/rendering, or localization in detailâ€”those are separate issues, beyond my scope (and knowledge) here.  Diversity and Inherent Complexity The Unicode Codespace Codespace Allocation Scripts Usage Frequency Encodings UTF-8 UTF-16 Combining Marks Canonical Equivalence Normalization Forms Grapheme Clusters And Moreâ€¦ Diversity and Inherent Complexity As soon as you start to study Unicode, it becomes clear that it represents a large jump in complexity over character sets like ASCII that you may be more familiar with. Itâ€™s not just that Unicode contains a much larger number of characters, although thatâ€™s part of it. Unicode also has a great deal of internal structure, features, and special cases, making it much more than what one might expect a mere â€œcharacter setâ€ to be. Weâ€™ll see some of that later in this article.  When confronting all this complexity, especially as an engineer, itâ€™s hard not to find oneself asking, â€œWhy do we need all this? Is this really necessary? Couldnâ€™t it be simplified?â€  However, Unicode aims to faithfully represent the entire worldâ€™s writing systems. The Unicode Consortiumâ€™s stated goal is â€œenabling people around the world to use computers in any languageâ€. And as you might imagine, the diversity of written languages is immense! To date, Unicode supports 135 different scripts, covering some 1100 languages, and thereâ€™s still a long tail of over 100 unsupported scripts, both modern and historical, which people are still working to add.  Given this enormous diversity, itâ€™s inevitable that representing it is a complicated project. Unicode embraces that diversity, and accepts the complexity inherent in its mission to include all human writing systems. It doesnâ€™t make a lot of trade-offs in the name of simplification, and it makes exceptions to its own rules where necessary to further its mission.  Moreover, Unicode is committed not just to supporting texts in any single language, but also to letting multiple languages coexist within one textâ€”which introduces even more complexity.  Most programming languages have libraries available to handle the gory low-level details of text manipulation, but as a programmer, youâ€™ll still need to know about certain Unicode features in order to know when and how to apply them. It may take some time to wrap your head around it all, but donâ€™t be discouragedâ€”think about the billions of people for whom your software will be more accessible through supporting text in their language. Embrace the complexity!  The Unicode Codespace Letâ€™s start with some general orientation. The basic elements of Unicodeâ€”its â€œcharactersâ€, although that term isnâ€™t quite rightâ€”are called code points. Code points are identified by number, customarily written in hexadecimal with the prefix â€œU+â€, such as U+0041 â€œAâ€ latin capital letter a or U+03B8 â€œÎ¸â€ greek small letter theta. Each code point also has a short name, and quite a few other properties, specified in the Unicode Character Database.  The set of all possible code points is called the codespace. The Unicode codespace consists of 1,114,112 code points. However, only 128,237 of themâ€”about 12% of the codespaceâ€”are actually assigned, to date. Thereâ€™s plenty of room for growth! Unicode also reserves an additional 137,468 code points as â€œprivate useâ€ areas, which have no standardized meaning and are available for individual applications to define for their own purposes.  Codespace Allocation To get a feel for how the codespace is laid out, itâ€™s helpful to visualize it. Below is a map of the entire codespace, with one pixel per code point. Itâ€™s arranged in tiles for visual coherence; each small square is 16Ã—16 = 256 code points, and each large square is a â€œplaneâ€ of 65,536 code points. There are 17 planes altogether.  Map of the Unicode codespace (click to zoom)  White represents unassigned space. Blue is assigned code points, green is private-use areas, and the small red area is surrogates (more about those later). As you can see, the assigned code points are distributed somewhat sparsely, but concentrated in the first three planes.  Plane 0 is also known as the â€œBasic Multilingual Planeâ€, or BMP. The BMP contains essentially all the characters needed for modern text in any script, including Latin, Cyrillic, Greek, Han (Chinese), Japanese, Korean, Arabic, Hebrew, Devanagari (Indian), and many more.  (In the past, the codespace was just the BMP and no moreâ€”Unicode was originally conceived as a straightforward 16-bit encoding, with only 65,536 code points. It was expanded to its current size in 1996. However, the vast majority of code points in modern text belong to the BMP.)  Plane 1 contains historical scripts, such as Sumerian cuneiform and Egyptian hieroglyphs, as well as emoji and various other symbols. Plane 2 contains a large block of less-common and historical Han characters. The remaining planes are empty, except for a small number of rarely-used formatting characters in Plane 14; planes 15â€“16 are reserved entirely for private use.  Scripts Letâ€™s zoom in on the first three planes, since thatâ€™s where the action is:  Map of scripts in Unicode planes 0â€“2 (click to zoom)  This map color-codes the 135 different scripts in Unicode. You can see how Han () and Korean () take up most of the range of the BMP (the left large square). By contrast, all of the European, Middle Eastern, and South Asian scripts fit into the first row of the BMP in this diagram.  Many areas of the codespace are adapted or copied from earlier encodings. For example, the first 128 code points of Unicode are just a copy of ASCII. This has clear benefits for compatibilityâ€”itâ€™s easy to losslessly convert texts from smaller encodings into Unicode (and the other direction too, as long as no characters outside the smaller encoding are used).  Usage Frequency One more interesting way to visualize the codespace is to look at the distribution of usageâ€”in other words, how often each code point is actually used in real-world texts. Below is a heat map of planes 0â€“2 based on a large sample of text from Wikipedia and Twitter (all languages). Frequency increases from black (never seen) through red and yellow to white.  Heat map of code point usage frequency in Unicode planes 0â€“2 (click to zoom)  You can see that the vast majority of this text sample lies in the BMP, with only scattered usage of code points from planes 1â€“2. The biggest exception is emoji, which show up here as the several bright squares in the bottom row of plane 1.  Encodings Weâ€™ve seen that Unicode code points are abstractly identified by their index in the codespace, ranging from U+0000 to U+10FFFF. But how do code points get represented as bytes, in memory or in a file?  The most convenient, computer-friendliest (and programmer-friendliest) thing to do would be to just store the code point index as a 32-bit integer. This works, but it consumes 4 bytes per code point, which is sort of a lot. Using 32-bit ints for Unicode will cost you a bunch of extra storage, memory, and performance in bandwidth-bound scenarios, if you work with a lot of text.  Consequently, there are several more-compact encodings for Unicode. The 32-bit integer encoding is officially called UTF-32 (UTF = â€œUnicode Transformation Formatâ€), but itâ€™s rarely used for storage. At most, it comes up sometimes as a temporary internal representation, for examining or operating on the code points in a string.  Much more commonly, youâ€™ll see Unicode text encoded as either UTF-8 or UTF-16. These are both variable-length encodings, made up of 8-bit or 16-bit units, respectively. In these schemes, code points with smaller index values take up fewer bytes, which saves a lot of memory for typical texts. The trade-off is that processing UTF-8/16 texts is more programmatically involved, and likely slower.  UTF-8 In UTF-8, each code point is stored using 1 to 4 bytes, based on its index value.  UTF-8 uses a system of binary prefixes, in which the high bits of each byte mark whether itâ€™s a single byte, the beginning of a multi-byte sequence, or a continuation byte; the remaining bits, concatenated, give the code point index. This table shows how it works:  UTF-8 (binary)\\tCode point (binary)\\tRange 0xxxxxxx\\txxxxxxx\\tU+0000â€“U+007F 110xxxxx 10yyyyyy\\txxxxxyyyyyy\\tU+0080â€“U+07FF 1110xxxx 10yyyyyy 10zzzzzz\\txxxxyyyyyyzzzzzz\\tU+0800â€“U+FFFF 11110xxx 10yyyyyy 10zzzzzz 10wwwwww\\txxxyyyyyyzzzzzzwwwwww\\tU+10000â€“U+10FFFF A handy property of UTF-8 is that code points below 128 (ASCII characters) are encoded as single bytes, and all non-ASCII code points are encoded using sequences of bytes 128â€“255. This has a couple of nice consequences. First, any strings or files out there that are already in ASCII can also be interpreted as UTF-8 without any conversion. Second, lots of widely-used string programming idiomsâ€”such as null termination, or delimiters (newlines, tabs, commas, slashes, etc.)â€”will just work on UTF-8 strings. ASCII bytes never occur inside the encoding of non-ASCII code points, so searching byte-wise for a null terminator or a delimiter will do the right thing.  Thanks to this convenience, itâ€™s relatively simple to extend legacy ASCII programs and APIs to handle UTF-8 strings. UTF-8 is very widely used in the Unix/Linux and Web worlds, and many programmers argue UTF-8 should be the default encoding everywhere.  However, UTF-8 isnâ€™t a drop-in replacement for ASCII strings in all respects. For instance, code that iterates over the â€œcharactersâ€ in a string will need to decode UTF-8 and iterate over code points (or maybe grapheme clustersâ€”more about those later), not bytes. When you measure the â€œlengthâ€ of a string, youâ€™ll need to think about whether you want the length in bytes, the length in code points, the width of the text when rendered, or something else.  UTF-16 The other encoding that youâ€™re likely to encounter is UTF-16. It uses 16-bit words, with each code point stored as either 1 or 2 words.  Like UTF-8, we can express the UTF-16 encoding rules in the form of binary prefixes:  UTF-16 (binary)\\tCode point (binary)\\tRange xxxxxxxxxxxxxxxx\\txxxxxxxxxxxxxxxx\\tU+0000â€“U+FFFF 110110xxxxxxxxxx 110111yyyyyyyyyy\\txxxxxxxxxxyyyyyyyyyy + 0x10000\\tU+10000â€“U+10FFFF A more common way that people talk about UTF-16 encoding, though, is in terms of code points called â€œsurrogatesâ€. All the code points in the range U+D800â€“U+DFFFâ€”or in other words, the code points that match the binary prefixes 110110 and 110111 in the table aboveâ€”are reserved specifically for UTF-16 encoding, and donâ€™t represent any valid characters on their own. Theyâ€™re only meant to occur in the 2-word encoding pattern above, which is called a â€œsurrogate pairâ€. Surrogate code points are illegal in any other context! Theyâ€™re not allowed in UTF-8 or UTF-32 at all.  Historically, UTF-16 is a descendant of the original, pre-1996 versions of Unicode, in which there were only 65,536 code points. The original intention was that there would be no different â€œencodingsâ€; Unicode was supposed to be a straightforward 16-bit character set. Later, the codespace was expanded to make room for a long tail of less-common (but still important) Han characters, which the Unicode designers didnâ€™t originally plan for. Surrogates were then introduced, asâ€”to put it bluntlyâ€”a kludge, allowing 16-bit encodings to access the new code points.  Today, Javascript uses UTF-16 as its standard string representation: if you ask for the length of a string, or iterate over it, etc., the result will be in UTF-16 words, with any code points outside the BMP expressed as surrogate pairs. UTF-16 is also used by the Microsoft Win32 APIs; though Win32 supports either 8-bit or 16-bit strings, the 8-bit version unaccountably still doesnâ€™t support UTF-8â€”only legacy code-page encodings, like ANSI. This leaves UTF-16 as the only way to get proper Unicode support in Windows. (Update: in Win10 version 1903, they finally added UTF-8 support to the 8-bit APIs! ğŸ˜Š)  By the way, UTF-16â€™s words can be stored either little-endian or big-endian. Unicode has no opinion on that issue, though it does encourage the convention of putting U+FEFF zero width no-break space at the top of a UTF-16 file as a byte-order mark, to disambiguate the endianness. (If the file doesnâ€™t match the systemâ€™s endianness, the BOM will be decoded as U+FFFE, which isnâ€™t a valid code point.)  Combining Marks In the story so far, weâ€™ve been focusing on code points. But in Unicode, a â€œcharacterâ€ can be more complicated than just an individual code point!  Unicode includes a system for dynamically composing characters, by combining multiple code points together. This is used in various ways to gain flexibility without causing a huge combinatorial explosion in the number of code points.  In European languages, for example, this shows up in the application of diacritics to letters. Unicode supports a wide range of diacritics, including acute and grave accents, umlauts, cedillas, and many more. All these diacritics can be applied to any letter of any alphabetâ€”and in fact, multiple diacritics can be used on a single letter.  If Unicode tried to assign a distinct code point to every possible combination of letter and diacritics, things would rapidly get out of hand. Instead, the dynamic composition system enables you to construct the character you want, by starting with a base code point (the letter) and appending additional code points, called â€œcombining marksâ€, to specify the diacritics. When a text renderer sees a sequence like this in a string, it automatically stacks the diacritics over or under the base letter to create a composed character.  For example, the accented character â€œÃâ€ can be expressed as a string of two code points: U+0041 â€œAâ€ latin capital letter a plus U+0301 â€œâ—ŒÌâ€ combining acute accent. This string automatically gets rendered as a single character: â€œÃâ€.  Now, Unicode does also include many â€œprecomposedâ€ code points, each representing a letter with some combination of diacritics already applied, such as U+00C1 â€œÃâ€ latin capital letter a with acute or U+1EC7 â€œá»‡â€ latin small letter e with circumflex and dot below. I suspect these are mostly inherited from older encodings that were assimilated into Unicode, and kept around for compatibility. In practice, there are precomposed code points for most of the common letter-with-diacritic combinations in European-script languages, so they donâ€™t use dynamic composition that much in typical text.  Still, the system of combining marks does allow for an arbitrary number of diacritics to be stacked on any base character. The reductio-ad-absurdum of this is Zalgo text, which works by Í–ÍŸÍ…rÍaá¹‹Ì«Ì Ì–ÍˆÌ—dÍ–Ì»Ì¹Ã³mÌªÍ™Í•Ì—ÌÄ¼Í‡Ì°Í“Ì³Ì«Ã½Í“Ì¥ÌŸÍ Ì•sÌ«tÌ«Ì±Í•Ì—Ì°Ì¼Ì˜ÍœaÌ¼Ì©Í–Í‡Ì ÍˆÌ£ÍcÌ™ÍkÌ–Ì±Ì¹ÍÍ˜iÌ¢nÌ¨ÌºÌÍ‡Í‡ÌŸÍ™Ä£Ì«Ì®ÍÌ»ÌŸÍ… Ì•nÌ¼ÌºÍˆÍuÌ®Í™mÌºÌ­ÌŸÌ—ÍeÌÍ“Ì°Ì¤Í“Ì«rÌµoÌ–á¹·sÒ‰ÌªÍÌ­Ì¬ÌÌ¤ Ì®Í‰ÌÌÌ—ÌŸÍ dÌ´ÌŸÌœÌ±Í•ÍšiÍ‡Ì«Ì¼Ì¯Ì­ÌœÍ¡á¸Í™Ì»Ì¼cÌ²Ì²Ì¹rÌ¨Ì Ì¹Ì£Ì°Ì¦iÌ±tÌ¤Ì»Ì¤ÍÍ™Ì˜Ì•iÌµÌœÌ­Ì¤Ì±ÍcÌµs Í˜oÌ±Ì²ÍˆÌ™Í–Í‡Ì²Í¢nÍ˜ ÌœÍˆeÌ¬Ì²Ì Ì©acÍ•ÌºÌ Í‰hÌ·Ìª ÌºÌ£Í–Ì±á¸»Ì«Ì¬ÌÌ¹á¸™Ì™ÌºÍ™Ì­Í“Ì²tÌÌÍ‡Ì²Í‰ÍtÌ·Í”ÌªÍ‰Ì²Ì»Ì Í™eÌ¦Ì»ÍˆÍ‰Í‡rÍ‡Ì­Ì­Ì¬Í–,Ì–Ì ÌœÍ™Í“Ì£Ì­sÌ˜Ì˜ÍˆoÌ±Ì°Ì¤Ì²Í… Ì›Ì¬ÌœÌ™tÌ¼Ì¦Í•Ì±Ì¹Í•Ì¥hÌ³Ì²ÍˆÍÍ…aÌ¦tÌ»Ì² Ì»ÌŸÌ­Ì¦Ì–tÌ›Ì°Ì©hÌ Í•Ì³ÌÌ«Í•eÍˆÌ¤Ì˜Í–ÌÍ˜yÒ‰ÌÍ™ Ì·Í‰Í”Ì°Ì oÌÌ°vÍˆÍˆÌ³Ì˜ÍœerÌ¶fÌ°ÍˆÍ”á¸»Í•Ì˜Ì«ÌºÌ²oÌ²Ì­Í™Í Í…wÌ±Ì³Ìº ÍœtÌ¸hÍ‡Ì­Í•Ì³ÍeÌ–Ì¯ÌŸÌ  ÍÌÌœÍ”Ì©ÌªÍœÄ¼ÍÌªÌ²ÍšiÌÌ²Ì¹Ì™Ì©Ì¹nÌ¨Ì¦Ì©Ì–á¸™Ì¼Ì²Ì¼Í¢Í… Ì¬ÍsÌ¼ÍšÌ˜ÌÍpÍ™Ì˜Ì»aÌ™cÒ‰Í‰ÌœÌ¤ÍˆÌ¯Ì–iÌ¥Í¡nÌ¦Ì Ì±ÍŸgÌ¸Ì—Ì»Ì¦Ì­Ì®ÌŸÍ… Ì³ÌªÌ Í–Ì³Ì¯Ì•aÌ«ÍœnÍdÍ¡ Ì£Ì¦Ì™Í…cÌªÌ—rÌ´Í™Ì®Ì¦Ì¹Ì³eÍ‡ÍšÌÍ”Ì¹Ì«ÍŸaÌ™ÌºÌ™È›Í”ÍÌ˜Ì¹Í…eÌ¥Ì©Í aÍ–ÌªÌœÌ®Í™Ì¹nÌ¢Í‰Ì Í‡Í‰Í“Ì¦Ì¼ÌaÌ³Í–ÌªÌ¤Ì±pÌ–Í”Í”ÌŸÍ‡ÍÍ pÌ±ÍÌºÄ™Ì²ÍÍˆÌ°Ì²Ì¤Ì«aÌ¯ÍœrÌ¨Ì®Ì«Ì£Ì˜aÌ©Ì¯Í–nÌ¹Ì¦Ì°ÍÌ£ÌÌcÌ¨Ì¦Ì±Í”ÍÍÍ–eÌ¬Í“Í˜ Ì¤Ì°Ì©Í™Ì¤Ì¬Í™oÌµÌ¼Ì»Ì¬Ì»Í‡Ì®ÌªfÌ´ Ì¡Ì™Ì­Í“Í–ÌªÌ¤â€œÌ¸Í™Ì Ì¼cÌ³Ì—ÍœoÍÌ¼Í™Í”Ì®rÌÌ«ÌºÌÌ¥Ì¬ruÌºÌ»Ì¯Í‰Ì­Ì»Ì¯pÌ°Ì¥Í“Ì£Ì«Ì™Ì¤Í¢tÌ³ÍÌ³Ì–Í…iÌ¶ÍˆÌÍ™Ì¼Ì™Ì¹oÌ¡Í”nÌ™ÌºÌ¹Ì–Ì©ÍÍ…â€Ì¨Ì—Í–ÍšÌ©.Ì¯Í“  A few other places where dynamic character composition shows up in Unicode:  Vowel-pointing notation in Arabic and Hebrew. In these languages, words are normally spelled with some of their vowels left out. They then have diacritic notation to indicate the vowels (used in dictionaries, language-teaching materials, childrenâ€™s books, and such). These diacritics are expressed with combining marks.  A Hebrew example, with niqqud:\\t×Ö¶×ª ×“Ö·×œÖ°×ªÖ´Ö¼×™ ×”Öµ×–Ö´×™×– ×”Öµ× Ö´×™×¢Ö·, ×§Ö¶×˜Ö¶×‘ ×œÖ´×©Ö°××›Ö·Ö¼×ªÖ´Ö¼×™ ×™Ö¸×©××•Ö¹×“ Normal writing (no niqqud):\\t××ª ×“×œ×ª×™ ×”×–×™×– ×”× ×™×¢, ×§×˜×‘ ×œ×©×›×ª×™ ×™×©×•×“ Devanagari, the script used to write Hindi, Sanskrit, and many other South Asian languages, expresses certain vowels as combining marks attached to consonant letters. For example, â€œà¤¹â€ + â€œ\\u200bà¤¿â€ = â€œà¤¹à¤¿â€ (â€œhâ€ + â€œiâ€ = â€œhiâ€). Korean characters stand for syllables, but they are composed of letters called jamo that stand for the vowels and consonants in the syllable. While there are code points for precomposed Korean syllables, itâ€™s also possible to dynamically compose them by concatenating their jamo. For example, â€œá„’â€ + â€œá…¡â€ + â€œá†«â€ = â€œí•œâ€ (â€œhâ€ + â€œaâ€ + â€œnâ€ = â€œhanâ€). Canonical Equivalence In Unicode, precomposed characters exist alongside the dynamic composition system. A consequence of this is that there are multiple ways to express â€œthe sameâ€ stringâ€”different sequences of code points that result in the same user-perceived characters. For example, as we saw earlier, we can express the character â€œÃâ€ either as the single code point U+00C1, or as the string of two code points U+0041 U+0301.  Another source of ambiguity is the ordering of multiple diacritics in a single character. Diacritic order matters visually when two diacritics apply to the same side of the base character, e.g. both above: â€œÇ¡â€ (dot, then macron) is different from â€œÄÌ‡â€ (macron, then dot). However, when diacritics apply to different sides of the character, e.g. one above and one below, then the order doesnâ€™t affect rendering. Moreover, a character with multiple diacritics might have one of the diacritics precomposed and others expressed as combining marks.  For example, the Vietnamese letter â€œá»‡â€ can be expressed in five different ways:  Fully precomposed: U+1EC7 â€œá»‡â€ Partially precomposed: U+1EB9 â€œáº¹â€ + U+0302 â€œâ—ŒÌ‚â€ Partially precomposed: U+00EA â€œÃªâ€ + U+0323 â€œâ—ŒÌ£â€ Fully decomposed: U+0065 â€œeâ€ + U+0323 â€œâ—ŒÌ£â€ + U+0302 â€œâ—ŒÌ‚â€ Fully decomposed: U+0065 â€œeâ€ + U+0302 â€œâ—ŒÌ‚â€ + U+0323 â€œâ—ŒÌ£â€ Unicode refers to set of strings like this as â€œcanonically equivalentâ€. Canonically equivalent strings are supposed to be treated as identical for purposes of searching, sorting, rendering, text selection, and so on. This has implications for how you implement operations on text. For example, if an app has a â€œfind in fileâ€ operation and the user searches for â€œá»‡â€, it should, by default, find occurrences of any of the five versions of â€œá»‡â€ above!  Normalization Forms To address the problem of â€œhow to handle canonically equivalent stringsâ€, Unicode defines several normalization forms: ways of converting strings into a canonical form so that they can be compared code-point-by-code-point (or byte-by-byte).  The â€œNFDâ€ normalization form fully decomposes every character down to its component base and combining marks, taking apart any precomposed code points in the string. It also sorts the combining marks in each character according to their rendered position, so e.g. diacritics that go below the character come before the ones that go above the character. (It doesnâ€™t reorder diacritics in the same rendered position, since their order matters visually, as previously mentioned.)  The â€œNFCâ€ form, conversely, puts things back together into precomposed code points as much as possible. If an unusual combination of diacritics is called for, there may not be any precomposed code point for it, in which case NFC still precomposes what it can and leaves any remaining combining marks in place (again ordered by rendered position, as in NFD).  There are also forms called NFKD and NFKC. The â€œKâ€ here refers to compatibility decompositions, which cover characters that are â€œsimilarâ€ in some sense but not visually identical. However, Iâ€™m not going to cover that here.  Grapheme Clusters As weâ€™ve seen, Unicode contains various cases where a thing that a user thinks of as a single â€œcharacterâ€ might actually be made up of multiple code points under the hood. Unicode formalizes this using the notion of a grapheme cluster: a string of one or more code points that constitute a single â€œuser-perceived characterâ€.  UAX #29 defines the rules for what, precisely, qualifies as a grapheme cluster. Itâ€™s approximately â€œa base code point followed by any number of combining marksâ€, but the actual definition is a bit more complicated; it accounts for things like Korean jamo, and emoji ZWJ sequences.  The main thing grapheme clusters are used for is text editing: theyâ€™re often the most sensible unit for cursor placement and text selection boundaries. Using grapheme clusters for these purposes ensures that you canâ€™t accidentally chop off some diacritics when you copy-and-paste text, that left/right arrow keys always move the cursor by one visible character, and so on.  Another place where grapheme clusters are useful is in enforcing a string length limitâ€”say, on a database field. While the true, underlying limit might be something like the byte length of the string in UTF-8, you wouldnâ€™t want to enforce that by just truncating bytes. At a minimum, youâ€™d want to â€œround downâ€ to the nearest code point boundary; but even better, round down to the nearest grapheme cluster boundary. Otherwise, you might be corrupting the last character by cutting off a diacritic, or interrupting a jamo sequence or ZWJ sequence.  And Moreâ€¦ Thereâ€™s much more that could be said about Unicode from a programmerâ€™s perspective! I havenâ€™t gotten into such fun topics as case mapping, collation, compatibility decompositions and confusables, Unicode-aware regexes, or bidirectional text. Nor have I said anything yet about implementation issuesâ€”how to efficiently store and look-up data about the sparsely-assigned code points, or how to optimize UTF-8 decoding, string comparison, or NFC normalization. Perhaps Iâ€™ll return to some of those things in future posts.  Unicode is a fascinating and complex system. It has a many-to-one mapping between bytes and code points, and on top of that a many-to-one (or, under some circumstances, many-to-many) mapping between code points and â€œcharactersâ€. It has oddball special cases in every corner. But no one ever claimed that representing all written languages was going to be easy, and itâ€™s clear that weâ€™re never going back to the bad old days of a patchwork of incompatible encodings.  Further reading:  The Unicode Standard UTF-8 Everywhere Manifesto Dark corners of Unicode by Eevee ICU (International Components for Unicode)â€”C/C++/Java libraries implementing many Unicode algorithms and related things Python 3 Unicode Howto Google Noto Fontsâ€”set of fonts intended to cover all assigned code points\"\"\"\n",
    "tokens = text.encode(\"utf-8\") # raw bytes\n",
    "tokens = list(map(int, tokens)) # convert to a list of integers in range 0..255 for convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging (101, 32) into a new token 256\n",
      "merging (105, 110) into a new token 257\n",
      "merging (115, 32) into a new token 258\n",
      "merging (116, 104) into a new token 259\n",
      "merging (101, 114) into a new token 260\n",
      "merging (99, 111) into a new token 261\n",
      "merging (116, 32) into a new token 262\n",
      "merging (226, 128) into a new token 263\n",
      "merging (44, 32) into a new token 264\n",
      "merging (97, 110) into a new token 265\n",
      "merging (111, 114) into a new token 266\n",
      "merging (100, 32) into a new token 267\n",
      "merging (97, 114) into a new token 268\n",
      "merging (101, 110) into a new token 269\n",
      "merging (257, 103) into a new token 270\n",
      "merging (261, 100) into a new token 271\n",
      "merging (121, 32) into a new token 272\n",
      "merging (46, 32) into a new token 273\n",
      "merging (97, 108) into a new token 274\n",
      "merging (259, 256) into a new token 275\n"
     ]
    }
   ],
   "source": [
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1 \n",
    "    return counts\n",
    "\n",
    "def merge(ids, pair, idx):\n",
    "  newids = []\n",
    "  i = 0\n",
    "  while i < len(ids):\n",
    "    if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "      newids.append(idx)\n",
    "      i += 2\n",
    "    else:\n",
    "      newids.append(ids[i])\n",
    "      i += 1\n",
    "  return newids\n",
    "# ---\n",
    "vocab_size = 276 #The desired final vocab size; it is a hyperparameter\n",
    "num_merges = vocab_size - 256 #Their are by default 256 chars, so 276 - 256 tells us how many loops to do for minting new tokens\n",
    "ids = list(tokens) #creates a copy of list\n",
    "\n",
    "merges = {} # (int, int) -> int\n",
    "for i in range(num_merges):\n",
    "   stats = get_stats(ids)\n",
    "   top_pair = max(stats, key = stats.get)\n",
    "   idx = 256 + i\n",
    "   print(f\"merging {top_pair} into a new token {idx}\")\n",
    "   ids = merge(ids, top_pair, idx)\n",
    "   merges[top_pair] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens length: 24597\n",
      "ids length 19438\n",
      "compression ratio: 1.27\n"
     ]
    }
   ],
   "source": [
    "#Getting compression statistics of original sequence and new sequence with expanded vocab\n",
    "print(\"tokens length:\", len(tokens))\n",
    "print(\"ids length\", len(ids))\n",
    "print(f\"compression ratio: {len(tokens) / len(ids):.2f}X\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The Tokenizer is a completely seperate, independent module from the LLM. It has its own training dataset of text (which could be different from that of the LLM), on which you train the vocabulary using the Byte Pair Encoding (BPE) algo. It can then be used for encoding and decoding purposes\n",
    "\n",
    "When training the tokenizer, we dont only care about english, we could care about japanese or code vs. not code; and so it would be useful to look into different mixtures of different kinds of languages/code. This variety will determine how many merges of it there will be and that determines the type of density that type of data has in the token space. I.E more Japanese data during training, will have more Japanese tokens merged, and so Japanese will have shorter sequences, which will benefit the LLM which has a finite context length it can work on the token space"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recursion based approach\n",
    "def my_decode(ids):\n",
    "    #given ids (list of integers), return Python string\n",
    "    vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "    for  k,v in merges.items():\n",
    "        vocab[v] = k\n",
    "\n",
    "    def decoder(id):\n",
    "        if not(id in merges.values()):\n",
    "            return vocab[id]\n",
    "        else:\n",
    "            t1, t2 = vocab[id]\n",
    "            return decoder(t1) + decoder(t2)\n",
    "        \n",
    "    decoded = b\"\".join([decoder(id) for id in ids])\n",
    "    decoded = decoded.decode(\"utf-8\", errors=\"replace\")\n",
    "    return decoded\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A Programmerâ€™s Introduction to Unicode March 3, 2017 Â· Coding Â· 22 Comments  ï¼µï½ï½‰ï½ƒï½ï½„ï½…! ğŸ…¤ğŸ…ğŸ…˜ğŸ…’ğŸ…ğŸ…“ğŸ…”â€½ ğŸ‡º\\u200cğŸ‡³\\u200cğŸ‡®\\u200cğŸ‡¨\\u200cğŸ‡´\\u200cğŸ‡©\\u200cğŸ‡ª! ğŸ˜„ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to â€œsupport Unicodeâ€ in our software (whatever that meansâ€”like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I donâ€™t blame programmers for still finding the whole thing mysterious, even 30 years after Unicodeâ€™s inception.  A few months ago, I got interested in Unicode and decided to spend some time learning more about it in detail. In this article, Iâ€™ll give an introduction to it from a programmerâ€™s point of view.  Iâ€™m going to focus on the character set and whatâ€™s involved in working with strings and files of Unicode text. However, in this article Iâ€™m not going to talk about fonts, text layout/shaping/rendering, or localization in detailâ€”those are separate issues, beyond my scope (and knowledge) here.  Diversity and Inherent Complexity The Unicode Codespace Codespace Allocation Scripts Usage Frequency Encodings UTF-8 UTF-16 Combining Marks Canonical Equivalence Normalization Forms Grapheme Clusters And Moreâ€¦ Diversity and Inherent Complexity As soon as you start to study Unicode, it becomes clear that it represents a large jump in complexity over character sets like ASCII that you may be more familiar with. Itâ€™s not just that Unicode contains a much larger number of characters, although thatâ€™s part of it. Unicode also has a great deal of internal structure, features, and special cases, making it much more than what one might expect a mere â€œcharacter setâ€ to be. Weâ€™ll see some of that later in this article.  When confronting all this complexity, especially as an engineer, itâ€™s hard not to find oneself asking, â€œWhy do we need all this? Is this really necessary? Couldnâ€™t it be simplified?â€  However, Unicode aims to faithfully represent the entire worldâ€™s writing systems. The Unicode Consortiumâ€™s stated goal is â€œenabling people around the world to use computers in any languageâ€. And as you might imagine, the diversity of written languages is immense! To date, Unicode supports 135 different scripts, covering some 1100 languages, and thereâ€™s still a long tail of over 100 unsupported scripts, both modern and historical, which people are still working to add.  Given this enormous diversity, itâ€™s inevitable that representing it is a complicated project. Unicode embraces that diversity, and accepts the complexity inherent in its mission to include all human writing systems. It doesnâ€™t make a lot of trade-offs in the name of simplification, and it makes exceptions to its own rules where necessary to further its mission.  Moreover, Unicode is committed not just to supporting texts in any single language, but also to letting multiple languages coexist within one textâ€”which introduces even more complexity.  Most programming languages have libraries available to handle the gory low-level details of text manipulation, but as a programmer, youâ€™ll still need to know about certain Unicode features in order to know when and how to apply them. It may take some time to wrap your head around it all, but donâ€™t be discouragedâ€”think about the billions of people for whom your software will be more accessible through supporting text in their language. Embrace the complexity!  The Unicode Codespace Letâ€™s start with some general orientation. The basic elements of Unicodeâ€”its â€œcharactersâ€, although that term isnâ€™t quite rightâ€”are called code points. Code points are identified by number, customarily written in hexadecimal with the prefix â€œU+â€, such as U+0041 â€œAâ€ latin capital letter a or U+03B8 â€œÎ¸â€ greek small letter theta. Each code point also has a short name, and quite a few other properties, specified in the Unicode Character Database.  The set of all possible code points is called the codespace. The Unicode codespace consists of 1,114,112 code points. However, only 128,237 of themâ€”about 12% of the codespaceâ€”are actually assigned, to date. Thereâ€™s plenty of room for growth! Unicode also reserves an additional 137,468 code points as â€œprivate useâ€ areas, which have no standardized meaning and are available for individual applications to define for their own purposes.  Codespace Allocation To get a feel for how the codespace is laid out, itâ€™s helpful to visualize it. Below is a map of the entire codespace, with one pixel per code point. Itâ€™s arranged in tiles for visual coherence; each small square is 16Ã—16 = 256 code points, and each large square is a â€œplaneâ€ of 65,536 code points. There are 17 planes altogether.  Map of the Unicode codespace (click to zoom)  White represents unassigned space. Blue is assigned code points, green is private-use areas, and the small red area is surrogates (more about those later). As you can see, the assigned code points are distributed somewhat sparsely, but concentrated in the first three planes.  Plane 0 is also known as the â€œBasic Multilingual Planeâ€, or BMP. The BMP contains essentially all the characters needed for modern text in any script, including Latin, Cyrillic, Greek, Han (Chinese), Japanese, Korean, Arabic, Hebrew, Devanagari (Indian), and many more.  (In the past, the codespace was just the BMP and no moreâ€”Unicode was originally conceived as a straightforward 16-bit encoding, with only 65,536 code points. It was expanded to its current size in 1996. However, the vast majority of code points in modern text belong to the BMP.)  Plane 1 contains historical scripts, such as Sumerian cuneiform and Egyptian hieroglyphs, as well as emoji and various other symbols. Plane 2 contains a large block of less-common and historical Han characters. The remaining planes are empty, except for a small number of rarely-used formatting characters in Plane 14; planes 15â€“16 are reserved entirely for private use.  Scripts Letâ€™s zoom in on the first three planes, since thatâ€™s where the action is:  Map of scripts in Unicode planes 0â€“2 (click to zoom)  This map color-codes the 135 different scripts in Unicode. You can see how Han () and Korean () take up most of the range of the BMP (the left large square). By contrast, all of the European, Middle Eastern, and South Asian scripts fit into the first row of the BMP in this diagram.  Many areas of the codespace are adapted or copied from earlier encodings. For example, the first 128 code points of Unicode are just a copy of ASCII. This has clear benefits for compatibilityâ€”itâ€™s easy to losslessly convert texts from smaller encodings into Unicode (and the other direction too, as long as no characters outside the smaller encoding are used).  Usage Frequency One more interesting way to visualize the codespace is to look at the distribution of usageâ€”in other words, how often each code point is actually used in real-world texts. Below is a heat map of planes 0â€“2 based on a large sample of text from Wikipedia and Twitter (all languages). Frequency increases from black (never seen) through red and yellow to white.  Heat map of code point usage frequency in Unicode planes 0â€“2 (click to zoom)  You can see that the vast majority of this text sample lies in the BMP, with only scattered usage of code points from planes 1â€“2. The biggest exception is emoji, which show up here as the several bright squares in the bottom row of plane 1.  Encodings Weâ€™ve seen that Unicode code points are abstractly identified by their index in the codespace, ranging from U+0000 to U+10FFFF. But how do code points get represented as bytes, in memory or in a file?  The most convenient, computer-friendliest (and programmer-friendliest) thing to do would be to just store the code point index as a 32-bit integer. This works, but it consumes 4 bytes per code point, which is sort of a lot. Using 32-bit ints for Unicode will cost you a bunch of extra storage, memory, and performance in bandwidth-bound scenarios, if you work with a lot of text.  Consequently, there are several more-compact encodings for Unicode. The 32-bit integer encoding is officially called UTF-32 (UTF = â€œUnicode Transformation Formatâ€), but itâ€™s rarely used for storage. At most, it comes up sometimes as a temporary internal representation, for examining or operating on the code points in a string.  Much more commonly, youâ€™ll see Unicode text encoded as either UTF-8 or UTF-16. These are both variable-length encodings, made up of 8-bit or 16-bit units, respectively. In these schemes, code points with smaller index values take up fewer bytes, which saves a lot of memory for typical texts. The trade-off is that processing UTF-8/16 texts is more programmatically involved, and likely slower.  UTF-8 In UTF-8, each code point is stored using 1 to 4 bytes, based on its index value.  UTF-8 uses a system of binary prefixes, in which the high bits of each byte mark whether itâ€™s a single byte, the beginning of a multi-byte sequence, or a continuation byte; the remaining bits, concatenated, give the code point index. This table shows how it works:  UTF-8 (binary)\\tCode point (binary)\\tRange 0xxxxxxx\\txxxxxxx\\tU+0000â€“U+007F 110xxxxx 10yyyyyy\\txxxxxyyyyyy\\tU+0080â€“U+07FF 1110xxxx 10yyyyyy 10zzzzzz\\txxxxyyyyyyzzzzzz\\tU+0800â€“U+FFFF 11110xxx 10yyyyyy 10zzzzzz 10wwwwww\\txxxyyyyyyzzzzzzwwwwww\\tU+10000â€“U+10FFFF A handy property of UTF-8 is that code points below 128 (ASCII characters) are encoded as single bytes, and all non-ASCII code points are encoded using sequences of bytes 128â€“255. This has a couple of nice consequences. First, any strings or files out there that are already in ASCII can also be interpreted as UTF-8 without any conversion. Second, lots of widely-used string programming idiomsâ€”such as null termination, or delimiters (newlines, tabs, commas, slashes, etc.)â€”will just work on UTF-8 strings. ASCII bytes never occur inside the encoding of non-ASCII code points, so searching byte-wise for a null terminator or a delimiter will do the right thing.  Thanks to this convenience, itâ€™s relatively simple to extend legacy ASCII programs and APIs to handle UTF-8 strings. UTF-8 is very widely used in the Unix/Linux and Web worlds, and many programmers argue UTF-8 should be the default encoding everywhere.  However, UTF-8 isnâ€™t a drop-in replacement for ASCII strings in all respects. For instance, code that iterates over the â€œcharactersâ€ in a string will need to decode UTF-8 and iterate over code points (or maybe grapheme clustersâ€”more about those later), not bytes. When you measure the â€œlengthâ€ of a string, youâ€™ll need to think about whether you want the length in bytes, the length in code points, the width of the text when rendered, or something else.  UTF-16 The other encoding that youâ€™re likely to encounter is UTF-16. It uses 16-bit words, with each code point stored as either 1 or 2 words.  Like UTF-8, we can express the UTF-16 encoding rules in the form of binary prefixes:  UTF-16 (binary)\\tCode point (binary)\\tRange xxxxxxxxxxxxxxxx\\txxxxxxxxxxxxxxxx\\tU+0000â€“U+FFFF 110110xxxxxxxxxx 110111yyyyyyyyyy\\txxxxxxxxxxyyyyyyyyyy + 0x10000\\tU+10000â€“U+10FFFF A more common way that people talk about UTF-16 encoding, though, is in terms of code points called â€œsurrogatesâ€. All the code points in the range U+D800â€“U+DFFFâ€”or in other words, the code points that match the binary prefixes 110110 and 110111 in the table aboveâ€”are reserved specifically for UTF-16 encoding, and donâ€™t represent any valid characters on their own. Theyâ€™re only meant to occur in the 2-word encoding pattern above, which is called a â€œsurrogate pairâ€. Surrogate code points are illegal in any other context! Theyâ€™re not allowed in UTF-8 or UTF-32 at all.  Historically, UTF-16 is a descendant of the original, pre-1996 versions of Unicode, in which there were only 65,536 code points. The original intention was that there would be no different â€œencodingsâ€; Unicode was supposed to be a straightforward 16-bit character set. Later, the codespace was expanded to make room for a long tail of less-common (but still important) Han characters, which the Unicode designers didnâ€™t originally plan for. Surrogates were then introduced, asâ€”to put it bluntlyâ€”a kludge, allowing 16-bit encodings to access the new code points.  Today, Javascript uses UTF-16 as its standard string representation: if you ask for the length of a string, or iterate over it, etc., the result will be in UTF-16 words, with any code points outside the BMP expressed as surrogate pairs. UTF-16 is also used by the Microsoft Win32 APIs; though Win32 supports either 8-bit or 16-bit strings, the 8-bit version unaccountably still doesnâ€™t support UTF-8â€”only legacy code-page encodings, like ANSI. This leaves UTF-16 as the only way to get proper Unicode support in Windows. (Update: in Win10 version 1903, they finally added UTF-8 support to the 8-bit APIs! ğŸ˜Š)  By the way, UTF-16â€™s words can be stored either little-endian or big-endian. Unicode has no opinion on that issue, though it does encourage the convention of putting U+FEFF zero width no-break space at the top of a UTF-16 file as a byte-order mark, to disambiguate the endianness. (If the file doesnâ€™t match the systemâ€™s endianness, the BOM will be decoded as U+FFFE, which isnâ€™t a valid code point.)  Combining Marks In the story so far, weâ€™ve been focusing on code points. But in Unicode, a â€œcharacterâ€ can be more complicated than just an individual code point!  Unicode includes a system for dynamically composing characters, by combining multiple code points together. This is used in various ways to gain flexibility without causing a huge combinatorial explosion in the number of code points.  In European languages, for example, this shows up in the application of diacritics to letters. Unicode supports a wide range of diacritics, including acute and grave accents, umlauts, cedillas, and many more. All these diacritics can be applied to any letter of any alphabetâ€”and in fact, multiple diacritics can be used on a single letter.  If Unicode tried to assign a distinct code point to every possible combination of letter and diacritics, things would rapidly get out of hand. Instead, the dynamic composition system enables you to construct the character you want, by starting with a base code point (the letter) and appending additional code points, called â€œcombining marksâ€, to specify the diacritics. When a text renderer sees a sequence like this in a string, it automatically stacks the diacritics over or under the base letter to create a composed character.  For example, the accented character â€œÃâ€ can be expressed as a string of two code points: U+0041 â€œAâ€ latin capital letter a plus U+0301 â€œâ—ŒÌâ€ combining acute accent. This string automatically gets rendered as a single character: â€œÃâ€.  Now, Unicode does also include many â€œprecomposedâ€ code points, each representing a letter with some combination of diacritics already applied, such as U+00C1 â€œÃâ€ latin capital letter a with acute or U+1EC7 â€œá»‡â€ latin small letter e with circumflex and dot below. I suspect these are mostly inherited from older encodings that were assimilated into Unicode, and kept around for compatibility. In practice, there are precomposed code points for most of the common letter-with-diacritic combinations in European-script languages, so they donâ€™t use dynamic composition that much in typical text.  Still, the system of combining marks does allow for an arbitrary number of diacritics to be stacked on any base character. The reductio-ad-absurdum of this is Zalgo text, which works by Í–ÍŸÍ…rÍaá¹‹Ì«Ì Ì–ÍˆÌ—dÍ–Ì»Ì¹Ã³mÌªÍ™Í•Ì—ÌÄ¼Í‡Ì°Í“Ì³Ì«Ã½Í“Ì¥ÌŸÍ Ì•sÌ«tÌ«Ì±Í•Ì—Ì°Ì¼Ì˜ÍœaÌ¼Ì©Í–Í‡Ì ÍˆÌ£ÍcÌ™ÍkÌ–Ì±Ì¹ÍÍ˜iÌ¢nÌ¨ÌºÌÍ‡Í‡ÌŸÍ™Ä£Ì«Ì®ÍÌ»ÌŸÍ… Ì•nÌ¼ÌºÍˆÍuÌ®Í™mÌºÌ­ÌŸÌ—ÍeÌÍ“Ì°Ì¤Í“Ì«rÌµoÌ–á¹·sÒ‰ÌªÍÌ­Ì¬ÌÌ¤ Ì®Í‰ÌÌÌ—ÌŸÍ dÌ´ÌŸÌœÌ±Í•ÍšiÍ‡Ì«Ì¼Ì¯Ì­ÌœÍ¡á¸Í™Ì»Ì¼cÌ²Ì²Ì¹rÌ¨Ì Ì¹Ì£Ì°Ì¦iÌ±tÌ¤Ì»Ì¤ÍÍ™Ì˜Ì•iÌµÌœÌ­Ì¤Ì±ÍcÌµs Í˜oÌ±Ì²ÍˆÌ™Í–Í‡Ì²Í¢nÍ˜ ÌœÍˆeÌ¬Ì²Ì Ì©acÍ•ÌºÌ Í‰hÌ·Ìª ÌºÌ£Í–Ì±á¸»Ì«Ì¬ÌÌ¹á¸™Ì™ÌºÍ™Ì­Í“Ì²tÌÌÍ‡Ì²Í‰ÍtÌ·Í”ÌªÍ‰Ì²Ì»Ì Í™eÌ¦Ì»ÍˆÍ‰Í‡rÍ‡Ì­Ì­Ì¬Í–,Ì–Ì ÌœÍ™Í“Ì£Ì­sÌ˜Ì˜ÍˆoÌ±Ì°Ì¤Ì²Í… Ì›Ì¬ÌœÌ™tÌ¼Ì¦Í•Ì±Ì¹Í•Ì¥hÌ³Ì²ÍˆÍÍ…aÌ¦tÌ»Ì² Ì»ÌŸÌ­Ì¦Ì–tÌ›Ì°Ì©hÌ Í•Ì³ÌÌ«Í•eÍˆÌ¤Ì˜Í–ÌÍ˜yÒ‰ÌÍ™ Ì·Í‰Í”Ì°Ì oÌÌ°vÍˆÍˆÌ³Ì˜ÍœerÌ¶fÌ°ÍˆÍ”á¸»Í•Ì˜Ì«ÌºÌ²oÌ²Ì­Í™Í Í…wÌ±Ì³Ìº ÍœtÌ¸hÍ‡Ì­Í•Ì³ÍeÌ–Ì¯ÌŸÌ  ÍÌÌœÍ”Ì©ÌªÍœÄ¼ÍÌªÌ²ÍšiÌÌ²Ì¹Ì™Ì©Ì¹nÌ¨Ì¦Ì©Ì–á¸™Ì¼Ì²Ì¼Í¢Í… Ì¬ÍsÌ¼ÍšÌ˜ÌÍpÍ™Ì˜Ì»aÌ™cÒ‰Í‰ÌœÌ¤ÍˆÌ¯Ì–iÌ¥Í¡nÌ¦Ì Ì±ÍŸgÌ¸Ì—Ì»Ì¦Ì­Ì®ÌŸÍ… Ì³ÌªÌ Í–Ì³Ì¯Ì•aÌ«ÍœnÍdÍ¡ Ì£Ì¦Ì™Í…cÌªÌ—rÌ´Í™Ì®Ì¦Ì¹Ì³eÍ‡ÍšÌÍ”Ì¹Ì«ÍŸaÌ™ÌºÌ™È›Í”ÍÌ˜Ì¹Í…eÌ¥Ì©Í aÍ–ÌªÌœÌ®Í™Ì¹nÌ¢Í‰Ì Í‡Í‰Í“Ì¦Ì¼ÌaÌ³Í–ÌªÌ¤Ì±pÌ–Í”Í”ÌŸÍ‡ÍÍ pÌ±ÍÌºÄ™Ì²ÍÍˆÌ°Ì²Ì¤Ì«aÌ¯ÍœrÌ¨Ì®Ì«Ì£Ì˜aÌ©Ì¯Í–nÌ¹Ì¦Ì°ÍÌ£ÌÌcÌ¨Ì¦Ì±Í”ÍÍÍ–eÌ¬Í“Í˜ Ì¤Ì°Ì©Í™Ì¤Ì¬Í™oÌµÌ¼Ì»Ì¬Ì»Í‡Ì®ÌªfÌ´ Ì¡Ì™Ì­Í“Í–ÌªÌ¤â€œÌ¸Í™Ì Ì¼cÌ³Ì—ÍœoÍÌ¼Í™Í”Ì®rÌÌ«ÌºÌÌ¥Ì¬ruÌºÌ»Ì¯Í‰Ì­Ì»Ì¯pÌ°Ì¥Í“Ì£Ì«Ì™Ì¤Í¢tÌ³ÍÌ³Ì–Í…iÌ¶ÍˆÌÍ™Ì¼Ì™Ì¹oÌ¡Í”nÌ™ÌºÌ¹Ì–Ì©ÍÍ…â€Ì¨Ì—Í–ÍšÌ©.Ì¯Í“  A few other places where dynamic character composition shows up in Unicode:  Vowel-pointing notation in Arabic and Hebrew. In these languages, words are normally spelled with some of their vowels left out. They then have diacritic notation to indicate the vowels (used in dictionaries, language-teaching materials, childrenâ€™s books, and such). These diacritics are expressed with combining marks.  A Hebrew example, with niqqud:\\t×Ö¶×ª ×“Ö·×œÖ°×ªÖ´Ö¼×™ ×”Öµ×–Ö´×™×– ×”Öµ× Ö´×™×¢Ö·, ×§Ö¶×˜Ö¶×‘ ×œÖ´×©Ö°××›Ö·Ö¼×ªÖ´Ö¼×™ ×™Ö¸×©××•Ö¹×“ Normal writing (no niqqud):\\t××ª ×“×œ×ª×™ ×”×–×™×– ×”× ×™×¢, ×§×˜×‘ ×œ×©×›×ª×™ ×™×©×•×“ Devanagari, the script used to write Hindi, Sanskrit, and many other South Asian languages, expresses certain vowels as combining marks attached to consonant letters. For example, â€œà¤¹â€ + â€œ\\u200bà¤¿â€ = â€œà¤¹à¤¿â€ (â€œhâ€ + â€œiâ€ = â€œhiâ€). Korean characters stand for syllables, but they are composed of letters called jamo that stand for the vowels and consonants in the syllable. While there are code points for precomposed Korean syllables, itâ€™s also possible to dynamically compose them by concatenating their jamo. For example, â€œá„’â€ + â€œá…¡â€ + â€œá†«â€ = â€œí•œâ€ (â€œhâ€ + â€œaâ€ + â€œnâ€ = â€œhanâ€). Canonical Equivalence In Unicode, precomposed characters exist alongside the dynamic composition system. A consequence of this is that there are multiple ways to express â€œthe sameâ€ stringâ€”different sequences of code points that result in the same user-perceived characters. For example, as we saw earlier, we can express the character â€œÃâ€ either as the single code point U+00C1, or as the string of two code points U+0041 U+0301.  Another source of ambiguity is the ordering of multiple diacritics in a single character. Diacritic order matters visually when two diacritics apply to the same side of the base character, e.g. both above: â€œÇ¡â€ (dot, then macron) is different from â€œÄÌ‡â€ (macron, then dot). However, when diacritics apply to different sides of the character, e.g. one above and one below, then the order doesnâ€™t affect rendering. Moreover, a character with multiple diacritics might have one of the diacritics precomposed and others expressed as combining marks.  For example, the Vietnamese letter â€œá»‡â€ can be expressed in five different ways:  Fully precomposed: U+1EC7 â€œá»‡â€ Partially precomposed: U+1EB9 â€œáº¹â€ + U+0302 â€œâ—ŒÌ‚â€ Partially precomposed: U+00EA â€œÃªâ€ + U+0323 â€œâ—ŒÌ£â€ Fully decomposed: U+0065 â€œeâ€ + U+0323 â€œâ—ŒÌ£â€ + U+0302 â€œâ—ŒÌ‚â€ Fully decomposed: U+0065 â€œeâ€ + U+0302 â€œâ—ŒÌ‚â€ + U+0323 â€œâ—ŒÌ£â€ Unicode refers to set of strings like this as â€œcanonically equivalentâ€. Canonically equivalent strings are supposed to be treated as identical for purposes of searching, sorting, rendering, text selection, and so on. This has implications for how you implement operations on text. For example, if an app has a â€œfind in fileâ€ operation and the user searches for â€œá»‡â€, it should, by default, find occurrences of any of the five versions of â€œá»‡â€ above!  Normalization Forms To address the problem of â€œhow to handle canonically equivalent stringsâ€, Unicode defines several normalization forms: ways of converting strings into a canonical form so that they can be compared code-point-by-code-point (or byte-by-byte).  The â€œNFDâ€ normalization form fully decomposes every character down to its component base and combining marks, taking apart any precomposed code points in the string. It also sorts the combining marks in each character according to their rendered position, so e.g. diacritics that go below the character come before the ones that go above the character. (It doesnâ€™t reorder diacritics in the same rendered position, since their order matters visually, as previously mentioned.)  The â€œNFCâ€ form, conversely, puts things back together into precomposed code points as much as possible. If an unusual combination of diacritics is called for, there may not be any precomposed code point for it, in which case NFC still precomposes what it can and leaves any remaining combining marks in place (again ordered by rendered position, as in NFD).  There are also forms called NFKD and NFKC. The â€œKâ€ here refers to compatibility decompositions, which cover characters that are â€œsimilarâ€ in some sense but not visually identical. However, Iâ€™m not going to cover that here.  Grapheme Clusters As weâ€™ve seen, Unicode contains various cases where a thing that a user thinks of as a single â€œcharacterâ€ might actually be made up of multiple code points under the hood. Unicode formalizes this using the notion of a grapheme cluster: a string of one or more code points that constitute a single â€œuser-perceived characterâ€.  UAX #29 defines the rules for what, precisely, qualifies as a grapheme cluster. Itâ€™s approximately â€œa base code point followed by any number of combining marksâ€, but the actual definition is a bit more complicated; it accounts for things like Korean jamo, and emoji ZWJ sequences.  The main thing grapheme clusters are used for is text editing: theyâ€™re often the most sensible unit for cursor placement and text selection boundaries. Using grapheme clusters for these purposes ensures that you canâ€™t accidentally chop off some diacritics when you copy-and-paste text, that left/right arrow keys always move the cursor by one visible character, and so on.  Another place where grapheme clusters are useful is in enforcing a string length limitâ€”say, on a database field. While the true, underlying limit might be something like the byte length of the string in UTF-8, you wouldnâ€™t want to enforce that by just truncating bytes. At a minimum, youâ€™d want to â€œround downâ€ to the nearest code point boundary; but even better, round down to the nearest grapheme cluster boundary. Otherwise, you might be corrupting the last character by cutting off a diacritic, or interrupting a jamo sequence or ZWJ sequence.  And Moreâ€¦ Thereâ€™s much more that could be said about Unicode from a programmerâ€™s perspective! I havenâ€™t gotten into such fun topics as case mapping, collation, compatibility decompositions and confusables, Unicode-aware regexes, or bidirectional text. Nor have I said anything yet about implementation issuesâ€”how to efficiently store and look-up data about the sparsely-assigned code points, or how to optimize UTF-8 decoding, string comparison, or NFC normalization. Perhaps Iâ€™ll return to some of those things in future posts.  Unicode is a fascinating and complex system. It has a many-to-one mapping between bytes and code points, and on top of that a many-to-one (or, under some circumstances, many-to-many) mapping between code points and â€œcharactersâ€. It has oddball special cases in every corner. But no one ever claimed that representing all written languages was going to be easy, and itâ€™s clear that weâ€™re never going back to the bad old days of a patchwork of incompatible encodings.  Further reading:  The Unicode Standard UTF-8 Everywhere Manifesto Dark corners of Unicode by Eevee ICU (International Components for Unicode)â€”C/C++/Java libraries implementing many Unicode algorithms and related things Python 3 Unicode Howto Google Noto Fontsâ€”set of fonts intended to cover all assigned code points'"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Programmerâ€™s Introduction to Unicode March 3, 2017 Â· Coding Â· 22 Comments  ï¼µï½ï½‰ï½ƒï½ï½„ï½…! ğŸ…¤ğŸ…ğŸ…˜ğŸ…’ğŸ…ğŸ…“ğŸ…”â€½ ğŸ‡ºâ€ŒğŸ‡³â€ŒğŸ‡®â€ŒğŸ‡¨â€ŒğŸ‡´â€ŒğŸ‡©â€ŒğŸ‡ª! ğŸ˜„ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to â€œsupport Unicodeâ€ in our software (whatever that meansâ€”like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I donâ€™t blame programmers for still finding the whole thing mysterious, even 30 years after Unicodeâ€™s inception.  A few months ago, I got interested in Unicode and decided to spend some time learning more about it in detail. In this article, Iâ€™ll give an introduction to it from a programmerâ€™s point of view.  Iâ€™m going to focus on the character set and whatâ€™s involved in working with strings and files of Unicode text. However, in this article Iâ€™m not going to talk about fonts, text layout/shaping/rendering, or localization in detailâ€”those are separate issues, beyond my scope (and knowledge) here.  Diversity and Inherent Complexity The Unicode Codespace Codespace Allocation Scripts Usage Frequency Encodings UTF-8 UTF-16 Combining Marks Canonical Equivalence Normalization Forms Grapheme Clusters And Moreâ€¦ Diversity and Inherent Complexity As soon as you start to study Unicode, it becomes clear that it represents a large jump in complexity over character sets like ASCII that you may be more familiar with. Itâ€™s not just that Unicode contains a much larger number of characters, although thatâ€™s part of it. Unicode also has a great deal of internal structure, features, and special cases, making it much more than what one might expect a mere â€œcharacter setâ€ to be. Weâ€™ll see some of that later in this article.  When confronting all this complexity, especially as an engineer, itâ€™s hard not to find oneself asking, â€œWhy do we need all this? Is this really necessary? Couldnâ€™t it be simplified?â€  However, Unicode aims to faithfully represent the entire worldâ€™s writing systems. The Unicode Consortiumâ€™s stated goal is â€œenabling people around the world to use computers in any languageâ€. And as you might imagine, the diversity of written languages is immense! To date, Unicode supports 135 different scripts, covering some 1100 languages, and thereâ€™s still a long tail of over 100 unsupported scripts, both modern and historical, which people are still working to add.  Given this enormous diversity, itâ€™s inevitable that representing it is a complicated project. Unicode embraces that diversity, and accepts the complexity inherent in its mission to include all human writing systems. It doesnâ€™t make a lot of trade-offs in the name of simplification, and it makes exceptions to its own rules where necessary to further its mission.  Moreover, Unicode is committed not just to supporting texts in any single language, but also to letting multiple languages coexist within one textâ€”which introduces even more complexity.  Most programming languages have libraries available to handle the gory low-level details of text manipulation, but as a programmer, youâ€™ll still need to know about certain Unicode features in order to know when and how to apply them. It may take some time to wrap your head around it all, but donâ€™t be discouragedâ€”think about the billions of people for whom your software will be more accessible through supporting text in their language. Embrace the complexity!  The Unicode Codespace Letâ€™s start with some general orientation. The basic elements of Unicodeâ€”its â€œcharactersâ€, although that term isnâ€™t quite rightâ€”are called code points. Code points are identified by number, customarily written in hexadecimal with the prefix â€œU+â€, such as U+0041 â€œAâ€ latin capital letter a or U+03B8 â€œÎ¸â€ greek small letter theta. Each code point also has a short name, and quite a few other properties, specified in the Unicode Character Database.  The set of all possible code points is called the codespace. The Unicode codespace consists of 1,114,112 code points. However, only 128,237 of themâ€”about 12% of the codespaceâ€”are actually assigned, to date. Thereâ€™s plenty of room for growth! Unicode also reserves an additional 137,468 code points as â€œprivate useâ€ areas, which have no standardized meaning and are available for individual applications to define for their own purposes.  Codespace Allocation To get a feel for how the codespace is laid out, itâ€™s helpful to visualize it. Below is a map of the entire codespace, with one pixel per code point. Itâ€™s arranged in tiles for visual coherence; each small square is 16Ã—16 = 256 code points, and each large square is a â€œplaneâ€ of 65,536 code points. There are 17 planes altogether.  Map of the Unicode codespace (click to zoom)  White represents unassigned space. Blue is assigned code points, green is private-use areas, and the small red area is surrogates (more about those later). As you can see, the assigned code points are distributed somewhat sparsely, but concentrated in the first three planes.  Plane 0 is also known as the â€œBasic Multilingual Planeâ€, or BMP. The BMP contains essentially all the characters needed for modern text in any script, including Latin, Cyrillic, Greek, Han (Chinese), Japanese, Korean, Arabic, Hebrew, Devanagari (Indian), and many more.  (In the past, the codespace was just the BMP and no moreâ€”Unicode was originally conceived as a straightforward 16-bit encoding, with only 65,536 code points. It was expanded to its current size in 1996. However, the vast majority of code points in modern text belong to the BMP.)  Plane 1 contains historical scripts, such as Sumerian cuneiform and Egyptian hieroglyphs, as well as emoji and various other symbols. Plane 2 contains a large block of less-common and historical Han characters. The remaining planes are empty, except for a small number of rarely-used formatting characters in Plane 14; planes 15â€“16 are reserved entirely for private use.  Scripts Letâ€™s zoom in on the first three planes, since thatâ€™s where the action is:  Map of scripts in Unicode planes 0â€“2 (click to zoom)  This map color-codes the 135 different scripts in Unicode. You can see how Han () and Korean () take up most of the range of the BMP (the left large square). By contrast, all of the European, Middle Eastern, and South Asian scripts fit into the first row of the BMP in this diagram.  Many areas of the codespace are adapted or copied from earlier encodings. For example, the first 128 code points of Unicode are just a copy of ASCII. This has clear benefits for compatibilityâ€”itâ€™s easy to losslessly convert texts from smaller encodings into Unicode (and the other direction too, as long as no characters outside the smaller encoding are used).  Usage Frequency One more interesting way to visualize the codespace is to look at the distribution of usageâ€”in other words, how often each code point is actually used in real-world texts. Below is a heat map of planes 0â€“2 based on a large sample of text from Wikipedia and Twitter (all languages). Frequency increases from black (never seen) through red and yellow to white.  Heat map of code point usage frequency in Unicode planes 0â€“2 (click to zoom)  You can see that the vast majority of this text sample lies in the BMP, with only scattered usage of code points from planes 1â€“2. The biggest exception is emoji, which show up here as the several bright squares in the bottom row of plane 1.  Encodings Weâ€™ve seen that Unicode code points are abstractly identified by their index in the codespace, ranging from U+0000 to U+10FFFF. But how do code points get represented as bytes, in memory or in a file?  The most convenient, computer-friendliest (and programmer-friendliest) thing to do would be to just store the code point index as a 32-bit integer. This works, but it consumes 4 bytes per code point, which is sort of a lot. Using 32-bit ints for Unicode will cost you a bunch of extra storage, memory, and performance in bandwidth-bound scenarios, if you work with a lot of text.  Consequently, there are several more-compact encodings for Unicode. The 32-bit integer encoding is officially called UTF-32 (UTF = â€œUnicode Transformation Formatâ€), but itâ€™s rarely used for storage. At most, it comes up sometimes as a temporary internal representation, for examining or operating on the code points in a string.  Much more commonly, youâ€™ll see Unicode text encoded as either UTF-8 or UTF-16. These are both variable-length encodings, made up of 8-bit or 16-bit units, respectively. In these schemes, code points with smaller index values take up fewer bytes, which saves a lot of memory for typical texts. The trade-off is that processing UTF-8/16 texts is more programmatically involved, and likely slower.  UTF-8 In UTF-8, each code point is stored using 1 to 4 bytes, based on its index value.  UTF-8 uses a system of binary prefixes, in which the high bits of each byte mark whether itâ€™s a single byte, the beginning of a multi-byte sequence, or a continuation byte; the remaining bits, concatenated, give the code point index. This table shows how it works:  UTF-8 (binary)\tCode point (binary)\tRange 0xxxxxxx\txxxxxxx\tU+0000â€“U+007F 110xxxxx 10yyyyyy\txxxxxyyyyyy\tU+0080â€“U+07FF 1110xxxx 10yyyyyy 10zzzzzz\txxxxyyyyyyzzzzzz\tU+0800â€“U+FFFF 11110xxx 10yyyyyy 10zzzzzz 10wwwwww\txxxyyyyyyzzzzzzwwwwww\tU+10000â€“U+10FFFF A handy property of UTF-8 is that code points below 128 (ASCII characters) are encoded as single bytes, and all non-ASCII code points are encoded using sequences of bytes 128â€“255. This has a couple of nice consequences. First, any strings or files out there that are already in ASCII can also be interpreted as UTF-8 without any conversion. Second, lots of widely-used string programming idiomsâ€”such as null termination, or delimiters (newlines, tabs, commas, slashes, etc.)â€”will just work on UTF-8 strings. ASCII bytes never occur inside the encoding of non-ASCII code points, so searching byte-wise for a null terminator or a delimiter will do the right thing.  Thanks to this convenience, itâ€™s relatively simple to extend legacy ASCII programs and APIs to handle UTF-8 strings. UTF-8 is very widely used in the Unix/Linux and Web worlds, and many programmers argue UTF-8 should be the default encoding everywhere.  However, UTF-8 isnâ€™t a drop-in replacement for ASCII strings in all respects. For instance, code that iterates over the â€œcharactersâ€ in a string will need to decode UTF-8 and iterate over code points (or maybe grapheme clustersâ€”more about those later), not bytes. When you measure the â€œlengthâ€ of a string, youâ€™ll need to think about whether you want the length in bytes, the length in code points, the width of the text when rendered, or something else.  UTF-16 The other encoding that youâ€™re likely to encounter is UTF-16. It uses 16-bit words, with each code point stored as either 1 or 2 words.  Like UTF-8, we can express the UTF-16 encoding rules in the form of binary prefixes:  UTF-16 (binary)\tCode point (binary)\tRange xxxxxxxxxxxxxxxx\txxxxxxxxxxxxxxxx\tU+0000â€“U+FFFF 110110xxxxxxxxxx 110111yyyyyyyyyy\txxxxxxxxxxyyyyyyyyyy + 0x10000\tU+10000â€“U+10FFFF A more common way that people talk about UTF-16 encoding, though, is in terms of code points called â€œsurrogatesâ€. All the code points in the range U+D800â€“U+DFFFâ€”or in other words, the code points that match the binary prefixes 110110 and 110111 in the table aboveâ€”are reserved specifically for UTF-16 encoding, and donâ€™t represent any valid characters on their own. Theyâ€™re only meant to occur in the 2-word encoding pattern above, which is called a â€œsurrogate pairâ€. Surrogate code points are illegal in any other context! Theyâ€™re not allowed in UTF-8 or UTF-32 at all.  Historically, UTF-16 is a descendant of the original, pre-1996 versions of Unicode, in which there were only 65,536 code points. The original intention was that there would be no different â€œencodingsâ€; Unicode was supposed to be a straightforward 16-bit character set. Later, the codespace was expanded to make room for a long tail of less-common (but still important) Han characters, which the Unicode designers didnâ€™t originally plan for. Surrogates were then introduced, asâ€”to put it bluntlyâ€”a kludge, allowing 16-bit encodings to access the new code points.  Today, Javascript uses UTF-16 as its standard string representation: if you ask for the length of a string, or iterate over it, etc., the result will be in UTF-16 words, with any code points outside the BMP expressed as surrogate pairs. UTF-16 is also used by the Microsoft Win32 APIs; though Win32 supports either 8-bit or 16-bit strings, the 8-bit version unaccountably still doesnâ€™t support UTF-8â€”only legacy code-page encodings, like ANSI. This leaves UTF-16 as the only way to get proper Unicode support in Windows. (Update: in Win10 version 1903, they finally added UTF-8 support to the 8-bit APIs! ğŸ˜Š)  By the way, UTF-16â€™s words can be stored either little-endian or big-endian. Unicode has no opinion on that issue, though it does encourage the convention of putting U+FEFF zero width no-break space at the top of a UTF-16 file as a byte-order mark, to disambiguate the endianness. (If the file doesnâ€™t match the systemâ€™s endianness, the BOM will be decoded as U+FFFE, which isnâ€™t a valid code point.)  Combining Marks In the story so far, weâ€™ve been focusing on code points. But in Unicode, a â€œcharacterâ€ can be more complicated than just an individual code point!  Unicode includes a system for dynamically composing characters, by combining multiple code points together. This is used in various ways to gain flexibility without causing a huge combinatorial explosion in the number of code points.  In European languages, for example, this shows up in the application of diacritics to letters. Unicode supports a wide range of diacritics, including acute and grave accents, umlauts, cedillas, and many more. All these diacritics can be applied to any letter of any alphabetâ€”and in fact, multiple diacritics can be used on a single letter.  If Unicode tried to assign a distinct code point to every possible combination of letter and diacritics, things would rapidly get out of hand. Instead, the dynamic composition system enables you to construct the character you want, by starting with a base code point (the letter) and appending additional code points, called â€œcombining marksâ€, to specify the diacritics. When a text renderer sees a sequence like this in a string, it automatically stacks the diacritics over or under the base letter to create a composed character.  For example, the accented character â€œÃâ€ can be expressed as a string of two code points: U+0041 â€œAâ€ latin capital letter a plus U+0301 â€œâ—ŒÌâ€ combining acute accent. This string automatically gets rendered as a single character: â€œÃâ€.  Now, Unicode does also include many â€œprecomposedâ€ code points, each representing a letter with some combination of diacritics already applied, such as U+00C1 â€œÃâ€ latin capital letter a with acute or U+1EC7 â€œá»‡â€ latin small letter e with circumflex and dot below. I suspect these are mostly inherited from older encodings that were assimilated into Unicode, and kept around for compatibility. In practice, there are precomposed code points for most of the common letter-with-diacritic combinations in European-script languages, so they donâ€™t use dynamic composition that much in typical text.  Still, the system of combining marks does allow for an arbitrary number of diacritics to be stacked on any base character. The reductio-ad-absurdum of this is Zalgo text, which works by Í–ÍŸÍ…rÍaá¹‹Ì«Ì Ì–ÍˆÌ—dÍ–Ì»Ì¹Ã³mÌªÍ™Í•Ì—ÌÄ¼Í‡Ì°Í“Ì³Ì«Ã½Í“Ì¥ÌŸÍ Ì•sÌ«tÌ«Ì±Í•Ì—Ì°Ì¼Ì˜ÍœaÌ¼Ì©Í–Í‡Ì ÍˆÌ£ÍcÌ™ÍkÌ–Ì±Ì¹ÍÍ˜iÌ¢nÌ¨ÌºÌÍ‡Í‡ÌŸÍ™Ä£Ì«Ì®ÍÌ»ÌŸÍ… Ì•nÌ¼ÌºÍˆÍuÌ®Í™mÌºÌ­ÌŸÌ—ÍeÌÍ“Ì°Ì¤Í“Ì«rÌµoÌ–á¹·sÒ‰ÌªÍÌ­Ì¬ÌÌ¤ Ì®Í‰ÌÌÌ—ÌŸÍ dÌ´ÌŸÌœÌ±Í•ÍšiÍ‡Ì«Ì¼Ì¯Ì­ÌœÍ¡á¸Í™Ì»Ì¼cÌ²Ì²Ì¹rÌ¨Ì Ì¹Ì£Ì°Ì¦iÌ±tÌ¤Ì»Ì¤ÍÍ™Ì˜Ì•iÌµÌœÌ­Ì¤Ì±ÍcÌµs Í˜oÌ±Ì²ÍˆÌ™Í–Í‡Ì²Í¢nÍ˜ ÌœÍˆeÌ¬Ì²Ì Ì©acÍ•ÌºÌ Í‰hÌ·Ìª ÌºÌ£Í–Ì±á¸»Ì«Ì¬ÌÌ¹á¸™Ì™ÌºÍ™Ì­Í“Ì²tÌÌÍ‡Ì²Í‰ÍtÌ·Í”ÌªÍ‰Ì²Ì»Ì Í™eÌ¦Ì»ÍˆÍ‰Í‡rÍ‡Ì­Ì­Ì¬Í–,Ì–Ì ÌœÍ™Í“Ì£Ì­sÌ˜Ì˜ÍˆoÌ±Ì°Ì¤Ì²Í… Ì›Ì¬ÌœÌ™tÌ¼Ì¦Í•Ì±Ì¹Í•Ì¥hÌ³Ì²ÍˆÍÍ…aÌ¦tÌ»Ì² Ì»ÌŸÌ­Ì¦Ì–tÌ›Ì°Ì©hÌ Í•Ì³ÌÌ«Í•eÍˆÌ¤Ì˜Í–ÌÍ˜yÒ‰ÌÍ™ Ì·Í‰Í”Ì°Ì oÌÌ°vÍˆÍˆÌ³Ì˜ÍœerÌ¶fÌ°ÍˆÍ”á¸»Í•Ì˜Ì«ÌºÌ²oÌ²Ì­Í™Í Í…wÌ±Ì³Ìº ÍœtÌ¸hÍ‡Ì­Í•Ì³ÍeÌ–Ì¯ÌŸÌ  ÍÌÌœÍ”Ì©ÌªÍœÄ¼ÍÌªÌ²ÍšiÌÌ²Ì¹Ì™Ì©Ì¹nÌ¨Ì¦Ì©Ì–á¸™Ì¼Ì²Ì¼Í¢Í… Ì¬ÍsÌ¼ÍšÌ˜ÌÍpÍ™Ì˜Ì»aÌ™cÒ‰Í‰ÌœÌ¤ÍˆÌ¯Ì–iÌ¥Í¡nÌ¦Ì Ì±ÍŸgÌ¸Ì—Ì»Ì¦Ì­Ì®ÌŸÍ… Ì³ÌªÌ Í–Ì³Ì¯Ì•aÌ«ÍœnÍdÍ¡ Ì£Ì¦Ì™Í…cÌªÌ—rÌ´Í™Ì®Ì¦Ì¹Ì³eÍ‡ÍšÌÍ”Ì¹Ì«ÍŸaÌ™ÌºÌ™È›Í”ÍÌ˜Ì¹Í…eÌ¥Ì©Í aÍ–ÌªÌœÌ®Í™Ì¹nÌ¢Í‰Ì Í‡Í‰Í“Ì¦Ì¼ÌaÌ³Í–ÌªÌ¤Ì±pÌ–Í”Í”ÌŸÍ‡ÍÍ pÌ±ÍÌºÄ™Ì²ÍÍˆÌ°Ì²Ì¤Ì«aÌ¯ÍœrÌ¨Ì®Ì«Ì£Ì˜aÌ©Ì¯Í–nÌ¹Ì¦Ì°ÍÌ£ÌÌcÌ¨Ì¦Ì±Í”ÍÍÍ–eÌ¬Í“Í˜ Ì¤Ì°Ì©Í™Ì¤Ì¬Í™oÌµÌ¼Ì»Ì¬Ì»Í‡Ì®ÌªfÌ´ Ì¡Ì™Ì­Í“Í–ÌªÌ¤â€œÌ¸Í™Ì Ì¼cÌ³Ì—ÍœoÍÌ¼Í™Í”Ì®rÌÌ«ÌºÌÌ¥Ì¬ruÌºÌ»Ì¯Í‰Ì­Ì»Ì¯pÌ°Ì¥Í“Ì£Ì«Ì™Ì¤Í¢tÌ³ÍÌ³Ì–Í…iÌ¶ÍˆÌÍ™Ì¼Ì™Ì¹oÌ¡Í”nÌ™ÌºÌ¹Ì–Ì©ÍÍ…â€Ì¨Ì—Í–ÍšÌ©.Ì¯Í“  A few other places where dynamic character composition shows up in Unicode:  Vowel-pointing notation in Arabic and Hebrew. In these languages, words are normally spelled with some of their vowels left out. They then have diacritic notation to indicate the vowels (used in dictionaries, language-teaching materials, childrenâ€™s books, and such). These diacritics are expressed with combining marks.  A Hebrew example, with niqqud:\t×Ö¶×ª ×“Ö·×œÖ°×ªÖ´Ö¼×™ ×”Öµ×–Ö´×™×– ×”Öµ× Ö´×™×¢Ö·, ×§Ö¶×˜Ö¶×‘ ×œÖ´×©Ö°××›Ö·Ö¼×ªÖ´Ö¼×™ ×™Ö¸×©××•Ö¹×“ Normal writing (no niqqud):\t××ª ×“×œ×ª×™ ×”×–×™×– ×”× ×™×¢, ×§×˜×‘ ×œ×©×›×ª×™ ×™×©×•×“ Devanagari, the script used to write Hindi, Sanskrit, and many other South Asian languages, expresses certain vowels as combining marks attached to consonant letters. For example, â€œà¤¹â€ + â€œâ€‹à¤¿â€ = â€œà¤¹à¤¿â€ (â€œhâ€ + â€œiâ€ = â€œhiâ€). Korean characters stand for syllables, but they are composed of letters called jamo that stand for the vowels and consonants in the syllable. While there are code points for precomposed Korean syllables, itâ€™s also possible to dynamically compose them by concatenating their jamo. For example, â€œá„’â€ + â€œá…¡â€ + â€œá†«â€ = â€œí•œâ€ (â€œhâ€ + â€œaâ€ + â€œnâ€ = â€œhanâ€). Canonical Equivalence In Unicode, precomposed characters exist alongside the dynamic composition system. A consequence of this is that there are multiple ways to express â€œthe sameâ€ stringâ€”different sequences of code points that result in the same user-perceived characters. For example, as we saw earlier, we can express the character â€œÃâ€ either as the single code point U+00C1, or as the string of two code points U+0041 U+0301.  Another source of ambiguity is the ordering of multiple diacritics in a single character. Diacritic order matters visually when two diacritics apply to the same side of the base character, e.g. both above: â€œÇ¡â€ (dot, then macron) is different from â€œÄÌ‡â€ (macron, then dot). However, when diacritics apply to different sides of the character, e.g. one above and one below, then the order doesnâ€™t affect rendering. Moreover, a character with multiple diacritics might have one of the diacritics precomposed and others expressed as combining marks.  For example, the Vietnamese letter â€œá»‡â€ can be expressed in five different ways:  Fully precomposed: U+1EC7 â€œá»‡â€ Partially precomposed: U+1EB9 â€œáº¹â€ + U+0302 â€œâ—ŒÌ‚â€ Partially precomposed: U+00EA â€œÃªâ€ + U+0323 â€œâ—ŒÌ£â€ Fully decomposed: U+0065 â€œeâ€ + U+0323 â€œâ—ŒÌ£â€ + U+0302 â€œâ—ŒÌ‚â€ Fully decomposed: U+0065 â€œeâ€ + U+0302 â€œâ—ŒÌ‚â€ + U+0323 â€œâ—ŒÌ£â€ Unicode refers to set of strings like this as â€œcanonically equivalentâ€. Canonically equivalent strings are supposed to be treated as identical for purposes of searching, sorting, rendering, text selection, and so on. This has implications for how you implement operations on text. For example, if an app has a â€œfind in fileâ€ operation and the user searches for â€œá»‡â€, it should, by default, find occurrences of any of the five versions of â€œá»‡â€ above!  Normalization Forms To address the problem of â€œhow to handle canonically equivalent stringsâ€, Unicode defines several normalization forms: ways of converting strings into a canonical form so that they can be compared code-point-by-code-point (or byte-by-byte).  The â€œNFDâ€ normalization form fully decomposes every character down to its component base and combining marks, taking apart any precomposed code points in the string. It also sorts the combining marks in each character according to their rendered position, so e.g. diacritics that go below the character come before the ones that go above the character. (It doesnâ€™t reorder diacritics in the same rendered position, since their order matters visually, as previously mentioned.)  The â€œNFCâ€ form, conversely, puts things back together into precomposed code points as much as possible. If an unusual combination of diacritics is called for, there may not be any precomposed code point for it, in which case NFC still precomposes what it can and leaves any remaining combining marks in place (again ordered by rendered position, as in NFD).  There are also forms called NFKD and NFKC. The â€œKâ€ here refers to compatibility decompositions, which cover characters that are â€œsimilarâ€ in some sense but not visually identical. However, Iâ€™m not going to cover that here.  Grapheme Clusters As weâ€™ve seen, Unicode contains various cases where a thing that a user thinks of as a single â€œcharacterâ€ might actually be made up of multiple code points under the hood. Unicode formalizes this using the notion of a grapheme cluster: a string of one or more code points that constitute a single â€œuser-perceived characterâ€.  UAX #29 defines the rules for what, precisely, qualifies as a grapheme cluster. Itâ€™s approximately â€œa base code point followed by any number of combining marksâ€, but the actual definition is a bit more complicated; it accounts for things like Korean jamo, and emoji ZWJ sequences.  The main thing grapheme clusters are used for is text editing: theyâ€™re often the most sensible unit for cursor placement and text selection boundaries. Using grapheme clusters for these purposes ensures that you canâ€™t accidentally chop off some diacritics when you copy-and-paste text, that left/right arrow keys always move the cursor by one visible character, and so on.  Another place where grapheme clusters are useful is in enforcing a string length limitâ€”say, on a database field. While the true, underlying limit might be something like the byte length of the string in UTF-8, you wouldnâ€™t want to enforce that by just truncating bytes. At a minimum, youâ€™d want to â€œround downâ€ to the nearest code point boundary; but even better, round down to the nearest grapheme cluster boundary. Otherwise, you might be corrupting the last character by cutting off a diacritic, or interrupting a jamo sequence or ZWJ sequence.  And Moreâ€¦ Thereâ€™s much more that could be said about Unicode from a programmerâ€™s perspective! I havenâ€™t gotten into such fun topics as case mapping, collation, compatibility decompositions and confusables, Unicode-aware regexes, or bidirectional text. Nor have I said anything yet about implementation issuesâ€”how to efficiently store and look-up data about the sparsely-assigned code points, or how to optimize UTF-8 decoding, string comparison, or NFC normalization. Perhaps Iâ€™ll return to some of those things in future posts.  Unicode is a fascinating and complex system. It has a many-to-one mapping between bytes and code points, and on top of that a many-to-one (or, under some circumstances, many-to-many) mapping between code points and â€œcharactersâ€. It has oddball special cases in every corner. But no one ever claimed that representing all written languages was going to be easy, and itâ€™s clear that weâ€™re never going back to the bad old days of a patchwork of incompatible encodings.  Further reading:  The Unicode Standard UTF-8 Everywhere Manifesto Dark corners of Unicode by Eevee ICU (International Components for Unicode)â€”C/C++/Java libraries implementing many Unicode algorithms and related things Python 3 Unicode Howto Google Noto Fontsâ€”set of fonts intended to cover all assigned code points\n"
     ]
    }
   ],
   "source": [
    "vocab = {idx: bytes([idx]) for idx in range(256)} #mapping from token id to the  bytes object of the token\n",
    "for (p0, p1), idx in merges.items(): #go in order of all the merges (IMPORTANT: in Python 3.7 will go in order of in which items were inserted into the merges dict ), this order is important because EX. custom token 258 = token 3 + custom token 257, where custom token 257 = token 4 + token 6; so need to define componenets of token 257 to then define token 258\n",
    "    vocab[idx] = vocab[p0] + vocab[p1]   #populate the vocab list with concatenation of the bytes objects\n",
    "\n",
    "def decode(ids):\n",
    "  # given ids (list of integers), return Python string\n",
    "  tokens = b\"\".join(vocab[idx] for idx in ids) #b\"\" concatenates bytes\n",
    "  text = tokens.decode(\"utf-8\", errors=\"replace\") #raw bytes are decoded\n",
    "  return text\n",
    "\n",
    "print(decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: b'\\x00',\n",
       " 1: b'\\x01',\n",
       " 2: b'\\x02',\n",
       " 3: b'\\x03',\n",
       " 4: b'\\x04',\n",
       " 5: b'\\x05',\n",
       " 6: b'\\x06',\n",
       " 7: b'\\x07',\n",
       " 8: b'\\x08',\n",
       " 9: b'\\t',\n",
       " 10: b'\\n',\n",
       " 11: b'\\x0b',\n",
       " 12: b'\\x0c',\n",
       " 13: b'\\r',\n",
       " 14: b'\\x0e',\n",
       " 15: b'\\x0f',\n",
       " 16: b'\\x10',\n",
       " 17: b'\\x11',\n",
       " 18: b'\\x12',\n",
       " 19: b'\\x13',\n",
       " 20: b'\\x14',\n",
       " 21: b'\\x15',\n",
       " 22: b'\\x16',\n",
       " 23: b'\\x17',\n",
       " 24: b'\\x18',\n",
       " 25: b'\\x19',\n",
       " 26: b'\\x1a',\n",
       " 27: b'\\x1b',\n",
       " 28: b'\\x1c',\n",
       " 29: b'\\x1d',\n",
       " 30: b'\\x1e',\n",
       " 31: b'\\x1f',\n",
       " 32: b' ',\n",
       " 33: b'!',\n",
       " 34: b'\"',\n",
       " 35: b'#',\n",
       " 36: b'$',\n",
       " 37: b'%',\n",
       " 38: b'&',\n",
       " 39: b\"'\",\n",
       " 40: b'(',\n",
       " 41: b')',\n",
       " 42: b'*',\n",
       " 43: b'+',\n",
       " 44: b',',\n",
       " 45: b'-',\n",
       " 46: b'.',\n",
       " 47: b'/',\n",
       " 48: b'0',\n",
       " 49: b'1',\n",
       " 50: b'2',\n",
       " 51: b'3',\n",
       " 52: b'4',\n",
       " 53: b'5',\n",
       " 54: b'6',\n",
       " 55: b'7',\n",
       " 56: b'8',\n",
       " 57: b'9',\n",
       " 58: b':',\n",
       " 59: b';',\n",
       " 60: b'<',\n",
       " 61: b'=',\n",
       " 62: b'>',\n",
       " 63: b'?',\n",
       " 64: b'@',\n",
       " 65: b'A',\n",
       " 66: b'B',\n",
       " 67: b'C',\n",
       " 68: b'D',\n",
       " 69: b'E',\n",
       " 70: b'F',\n",
       " 71: b'G',\n",
       " 72: b'H',\n",
       " 73: b'I',\n",
       " 74: b'J',\n",
       " 75: b'K',\n",
       " 76: b'L',\n",
       " 77: b'M',\n",
       " 78: b'N',\n",
       " 79: b'O',\n",
       " 80: b'P',\n",
       " 81: b'Q',\n",
       " 82: b'R',\n",
       " 83: b'S',\n",
       " 84: b'T',\n",
       " 85: b'U',\n",
       " 86: b'V',\n",
       " 87: b'W',\n",
       " 88: b'X',\n",
       " 89: b'Y',\n",
       " 90: b'Z',\n",
       " 91: b'[',\n",
       " 92: b'\\\\',\n",
       " 93: b']',\n",
       " 94: b'^',\n",
       " 95: b'_',\n",
       " 96: b'`',\n",
       " 97: b'a',\n",
       " 98: b'b',\n",
       " 99: b'c',\n",
       " 100: b'd',\n",
       " 101: b'e',\n",
       " 102: b'f',\n",
       " 103: b'g',\n",
       " 104: b'h',\n",
       " 105: b'i',\n",
       " 106: b'j',\n",
       " 107: b'k',\n",
       " 108: b'l',\n",
       " 109: b'm',\n",
       " 110: b'n',\n",
       " 111: b'o',\n",
       " 112: b'p',\n",
       " 113: b'q',\n",
       " 114: b'r',\n",
       " 115: b's',\n",
       " 116: b't',\n",
       " 117: b'u',\n",
       " 118: b'v',\n",
       " 119: b'w',\n",
       " 120: b'x',\n",
       " 121: b'y',\n",
       " 122: b'z',\n",
       " 123: b'{',\n",
       " 124: b'|',\n",
       " 125: b'}',\n",
       " 126: b'~',\n",
       " 127: b'\\x7f',\n",
       " 128: b'\\x80',\n",
       " 129: b'\\x81',\n",
       " 130: b'\\x82',\n",
       " 131: b'\\x83',\n",
       " 132: b'\\x84',\n",
       " 133: b'\\x85',\n",
       " 134: b'\\x86',\n",
       " 135: b'\\x87',\n",
       " 136: b'\\x88',\n",
       " 137: b'\\x89',\n",
       " 138: b'\\x8a',\n",
       " 139: b'\\x8b',\n",
       " 140: b'\\x8c',\n",
       " 141: b'\\x8d',\n",
       " 142: b'\\x8e',\n",
       " 143: b'\\x8f',\n",
       " 144: b'\\x90',\n",
       " 145: b'\\x91',\n",
       " 146: b'\\x92',\n",
       " 147: b'\\x93',\n",
       " 148: b'\\x94',\n",
       " 149: b'\\x95',\n",
       " 150: b'\\x96',\n",
       " 151: b'\\x97',\n",
       " 152: b'\\x98',\n",
       " 153: b'\\x99',\n",
       " 154: b'\\x9a',\n",
       " 155: b'\\x9b',\n",
       " 156: b'\\x9c',\n",
       " 157: b'\\x9d',\n",
       " 158: b'\\x9e',\n",
       " 159: b'\\x9f',\n",
       " 160: b'\\xa0',\n",
       " 161: b'\\xa1',\n",
       " 162: b'\\xa2',\n",
       " 163: b'\\xa3',\n",
       " 164: b'\\xa4',\n",
       " 165: b'\\xa5',\n",
       " 166: b'\\xa6',\n",
       " 167: b'\\xa7',\n",
       " 168: b'\\xa8',\n",
       " 169: b'\\xa9',\n",
       " 170: b'\\xaa',\n",
       " 171: b'\\xab',\n",
       " 172: b'\\xac',\n",
       " 173: b'\\xad',\n",
       " 174: b'\\xae',\n",
       " 175: b'\\xaf',\n",
       " 176: b'\\xb0',\n",
       " 177: b'\\xb1',\n",
       " 178: b'\\xb2',\n",
       " 179: b'\\xb3',\n",
       " 180: b'\\xb4',\n",
       " 181: b'\\xb5',\n",
       " 182: b'\\xb6',\n",
       " 183: b'\\xb7',\n",
       " 184: b'\\xb8',\n",
       " 185: b'\\xb9',\n",
       " 186: b'\\xba',\n",
       " 187: b'\\xbb',\n",
       " 188: b'\\xbc',\n",
       " 189: b'\\xbd',\n",
       " 190: b'\\xbe',\n",
       " 191: b'\\xbf',\n",
       " 192: b'\\xc0',\n",
       " 193: b'\\xc1',\n",
       " 194: b'\\xc2',\n",
       " 195: b'\\xc3',\n",
       " 196: b'\\xc4',\n",
       " 197: b'\\xc5',\n",
       " 198: b'\\xc6',\n",
       " 199: b'\\xc7',\n",
       " 200: b'\\xc8',\n",
       " 201: b'\\xc9',\n",
       " 202: b'\\xca',\n",
       " 203: b'\\xcb',\n",
       " 204: b'\\xcc',\n",
       " 205: b'\\xcd',\n",
       " 206: b'\\xce',\n",
       " 207: b'\\xcf',\n",
       " 208: b'\\xd0',\n",
       " 209: b'\\xd1',\n",
       " 210: b'\\xd2',\n",
       " 211: b'\\xd3',\n",
       " 212: b'\\xd4',\n",
       " 213: b'\\xd5',\n",
       " 214: b'\\xd6',\n",
       " 215: b'\\xd7',\n",
       " 216: b'\\xd8',\n",
       " 217: b'\\xd9',\n",
       " 218: b'\\xda',\n",
       " 219: b'\\xdb',\n",
       " 220: b'\\xdc',\n",
       " 221: b'\\xdd',\n",
       " 222: b'\\xde',\n",
       " 223: b'\\xdf',\n",
       " 224: b'\\xe0',\n",
       " 225: b'\\xe1',\n",
       " 226: b'\\xe2',\n",
       " 227: b'\\xe3',\n",
       " 228: b'\\xe4',\n",
       " 229: b'\\xe5',\n",
       " 230: b'\\xe6',\n",
       " 231: b'\\xe7',\n",
       " 232: b'\\xe8',\n",
       " 233: b'\\xe9',\n",
       " 234: b'\\xea',\n",
       " 235: b'\\xeb',\n",
       " 236: b'\\xec',\n",
       " 237: b'\\xed',\n",
       " 238: b'\\xee',\n",
       " 239: b'\\xef',\n",
       " 240: b'\\xf0',\n",
       " 241: b'\\xf1',\n",
       " 242: b'\\xf2',\n",
       " 243: b'\\xf3',\n",
       " 244: b'\\xf4',\n",
       " 245: b'\\xf5',\n",
       " 246: b'\\xf6',\n",
       " 247: b'\\xf7',\n",
       " 248: b'\\xf8',\n",
       " 249: b'\\xf9',\n",
       " 250: b'\\xfa',\n",
       " 251: b'\\xfb',\n",
       " 252: b'\\xfc',\n",
       " 253: b'\\xfd',\n",
       " 254: b'\\xfe',\n",
       " 255: b'\\xff',\n",
       " 256: b'e ',\n",
       " 257: b'in',\n",
       " 258: b's ',\n",
       " 259: b'th',\n",
       " 260: b'er',\n",
       " 261: b'co',\n",
       " 262: b't ',\n",
       " 263: b'\\xe2\\x80',\n",
       " 264: b', ',\n",
       " 265: b'an',\n",
       " 266: b'or',\n",
       " 267: b'd ',\n",
       " 268: b'ar',\n",
       " 269: b'en',\n",
       " 270: b'ing',\n",
       " 271: b'cod',\n",
       " 272: b'y ',\n",
       " 273: b'. ',\n",
       " 274: b'al',\n",
       " 275: b'the '}"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_decode(ids) == decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[104, 101, 108, 108, 111, 32, 119, 266, 108, 100, 33]\n"
     ]
    }
   ],
   "source": [
    "def my_encode(text):\n",
    "    #given a string, return a list of integers (the tokens)\n",
    "    text = list(text.encode(\"utf-8\"))\n",
    "    changes = True\n",
    "    while changes == True:\n",
    "        changes = False\n",
    "        tokens = []\n",
    "        idx = 0\n",
    "        while idx < len(text):\n",
    "            if idx < len(text) - 1 and (text[idx],text[idx+1]) in merges.keys():\n",
    "                tokens.append(merges[(text[idx],text[idx+1])])\n",
    "                changes = True\n",
    "                idx +=2\n",
    "            else:\n",
    "                tokens.append(text[idx])\n",
    "                idx +=1\n",
    "        text = tokens\n",
    "    return text\n",
    "\n",
    "print(my_encode(\"hello world!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world!\n"
     ]
    }
   ],
   "source": [
    "print(decode(my_encode(\"hello world!\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(65, 32): 8,\n",
       " (32, 80): 7,\n",
       " (80, 114): 1,\n",
       " (114, 111): 57,\n",
       " (111, 103): 23,\n",
       " (103, 114): 25,\n",
       " (114, 97): 40,\n",
       " (97, 109): 47,\n",
       " (109, 109): 20,\n",
       " (109, 260): 10,\n",
       " (260, 263): 6,\n",
       " (263, 153): 64,\n",
       " (153, 258): 29,\n",
       " (258, 73): 3,\n",
       " (73, 110): 15,\n",
       " (110, 116): 18,\n",
       " (116, 114): 50,\n",
       " (111, 100): 18,\n",
       " (100, 117): 8,\n",
       " (117, 99): 19,\n",
       " (99, 116): 62,\n",
       " (116, 105): 134,\n",
       " (105, 111): 76,\n",
       " (111, 110): 143,\n",
       " (110, 32): 78,\n",
       " (32, 116): 66,\n",
       " (116, 111): 110,\n",
       " (111, 32): 137,\n",
       " (32, 85): 66,\n",
       " (85, 110): 60,\n",
       " (110, 105): 71,\n",
       " (105, 271): 59,\n",
       " (271, 256): 111,\n",
       " (256, 77): 3,\n",
       " (77, 268): 3,\n",
       " (268, 99): 4,\n",
       " (99, 104): 89,\n",
       " (104, 32): 45,\n",
       " (32, 51): 5,\n",
       " (51, 264): 2,\n",
       " (264, 50): 1,\n",
       " (50, 48): 1,\n",
       " (48, 49): 7,\n",
       " (49, 55): 2,\n",
       " (55, 32): 5,\n",
       " (32, 194): 2,\n",
       " (194, 183): 2,\n",
       " (183, 32): 2,\n",
       " (32, 67): 8,\n",
       " (67, 111): 17,\n",
       " (100, 270): 7,\n",
       " (270, 32): 107,\n",
       " (32, 50): 3,\n",
       " (50, 50): 1,\n",
       " (50, 32): 14,\n",
       " (111, 109): 43,\n",
       " (109, 269): 10,\n",
       " (269, 116): 36,\n",
       " (116, 258): 68,\n",
       " (258, 32): 1,\n",
       " (32, 239): 1,\n",
       " (239, 188): 1,\n",
       " (188, 181): 1,\n",
       " (181, 239): 1,\n",
       " (239, 189): 6,\n",
       " (189, 142): 1,\n",
       " (142, 239): 1,\n",
       " (189, 137): 1,\n",
       " (137, 239): 1,\n",
       " (189, 131): 1,\n",
       " (131, 239): 1,\n",
       " (189, 143): 1,\n",
       " (143, 239): 1,\n",
       " (189, 132): 1,\n",
       " (132, 239): 1,\n",
       " (189, 133): 1,\n",
       " (133, 33): 1,\n",
       " (33, 32): 10,\n",
       " (32, 240): 4,\n",
       " (240, 159): 16,\n",
       " (159, 133): 7,\n",
       " (133, 164): 1,\n",
       " (164, 240): 1,\n",
       " (133, 157): 1,\n",
       " (157, 240): 1,\n",
       " (133, 152): 1,\n",
       " (152, 240): 1,\n",
       " (133, 146): 1,\n",
       " (146, 240): 1,\n",
       " (133, 158): 1,\n",
       " (158, 240): 1,\n",
       " (133, 147): 1,\n",
       " (147, 240): 1,\n",
       " (133, 148): 1,\n",
       " (148, 263): 1,\n",
       " (263, 189): 1,\n",
       " (189, 32): 1,\n",
       " (159, 135): 7,\n",
       " (135, 186): 1,\n",
       " (186, 263): 1,\n",
       " (263, 140): 6,\n",
       " (140, 240): 6,\n",
       " (135, 179): 1,\n",
       " (179, 263): 1,\n",
       " (135, 174): 1,\n",
       " (174, 263): 1,\n",
       " (135, 168): 1,\n",
       " (168, 263): 1,\n",
       " (135, 180): 1,\n",
       " (180, 263): 1,\n",
       " (135, 169): 1,\n",
       " (169, 263): 1,\n",
       " (135, 170): 1,\n",
       " (170, 33): 1,\n",
       " (159, 152): 2,\n",
       " (152, 132): 1,\n",
       " (132, 32): 1,\n",
       " (32, 84): 17,\n",
       " (84, 104): 40,\n",
       " (104, 256): 21,\n",
       " (256, 118): 7,\n",
       " (118, 260): 47,\n",
       " (260, 272): 5,\n",
       " (272, 110): 6,\n",
       " (110, 97): 13,\n",
       " (109, 256): 29,\n",
       " (256, 115): 45,\n",
       " (115, 116): 104,\n",
       " (114, 105): 58,\n",
       " (105, 107): 12,\n",
       " (107, 101): 7,\n",
       " (101, 258): 74,\n",
       " (258, 102): 18,\n",
       " (102, 101): 10,\n",
       " (101, 268): 14,\n",
       " (268, 32): 5,\n",
       " (32, 265): 33,\n",
       " (265, 267): 70,\n",
       " (267, 97): 18,\n",
       " (97, 119): 3,\n",
       " (119, 256): 6,\n",
       " (256, 257): 8,\n",
       " (257, 116): 76,\n",
       " (32, 259): 121,\n",
       " (259, 256): 144,\n",
       " (256, 104): 5,\n",
       " (104, 101): 25,\n",
       " (268, 116): 7,\n",
       " (258, 111): 35,\n",
       " (111, 102): 116,\n",
       " (102, 32): 116,\n",
       " (32, 112): 35,\n",
       " (112, 114): 61,\n",
       " (260, 258): 25,\n",
       " (258, 119): 18,\n",
       " (119, 266): 21,\n",
       " (266, 108): 5,\n",
       " (108, 100): 9,\n",
       " (100, 119): 2,\n",
       " (119, 105): 40,\n",
       " (105, 100): 27,\n",
       " (100, 101): 46,\n",
       " (101, 273): 23,\n",
       " (273, 87): 6,\n",
       " (87, 256): 1,\n",
       " (256, 274): 7,\n",
       " (274, 108): 63,\n",
       " (108, 32): 51,\n",
       " (32, 107): 6,\n",
       " (107, 110): 5,\n",
       " (110, 111): 30,\n",
       " (111, 119): 57,\n",
       " (119, 32): 29,\n",
       " (32, 119): 50,\n",
       " (256, 111): 24,\n",
       " (111, 117): 81,\n",
       " (117, 103): 9,\n",
       " (103, 104): 22,\n",
       " (104, 262): 10,\n",
       " (262, 116): 12,\n",
       " (32, 263): 44,\n",
       " (263, 156): 71,\n",
       " (156, 115): 4,\n",
       " (115, 117): 40,\n",
       " (117, 112): 25,\n",
       " (112, 112): 26,\n",
       " (112, 266): 13,\n",
       " (266, 262): 6,\n",
       " (262, 85): 8,\n",
       " (271, 101): 32,\n",
       " (101, 263): 25,\n",
       " (263, 157): 71,\n",
       " (157, 32): 53,\n",
       " (32, 257): 43,\n",
       " (257, 32): 93,\n",
       " (32, 111): 80,\n",
       " (117, 114): 34,\n",
       " (114, 32): 13,\n",
       " (32, 115): 105,\n",
       " (115, 111): 41,\n",
       " (102, 116): 6,\n",
       " (116, 119): 7,\n",
       " (119, 268): 5,\n",
       " (268, 256): 38,\n",
       " (256, 40): 6,\n",
       " (40, 119): 1,\n",
       " (119, 104): 37,\n",
       " (104, 97): 24,\n",
       " (97, 116): 107,\n",
       " (116, 101): 88,\n",
       " (101, 118): 28,\n",
       " (260, 32): 93,\n",
       " (259, 97): 40,\n",
       " (97, 262): 47,\n",
       " (262, 109): 16,\n",
       " (109, 101): 20,\n",
       " (101, 265): 11,\n",
       " (265, 115): 3,\n",
       " (115, 263): 14,\n",
       " (263, 148): 25,\n",
       " (148, 108): 1,\n",
       " (108, 105): 43,\n",
       " (107, 256): 13,\n",
       " (256, 117): 14,\n",
       " (117, 115): 59,\n",
       " (115, 270): 19,\n",
       " (119, 99): 1,\n",
       " (104, 268): 43,\n",
       " (268, 95): 1,\n",
       " (95, 262): 1,\n",
       " (262, 102): 10,\n",
       " (102, 266): 58,\n",
       " (266, 32): 82,\n",
       " (32, 274): 7,\n",
       " (114, 270): 27,\n",
       " (270, 115): 10,\n",
       " (115, 264): 71,\n",
       " (264, 114): 6,\n",
       " (105, 103): 30,\n",
       " (104, 116): 4,\n",
       " (116, 63): 1,\n",
       " (63, 41): 1,\n",
       " (41, 273): 11,\n",
       " (273, 66): 8,\n",
       " (66, 117): 4,\n",
       " (117, 262): 32,\n",
       " (256, 99): 26,\n",
       " (99, 265): 20,\n",
       " (265, 32): 42,\n",
       " (32, 98): 57,\n",
       " (98, 256): 29,\n",
       " (256, 97): 30,\n",
       " (97, 98): 40,\n",
       " (98, 115): 4,\n",
       " (114, 117): 11,\n",
       " (115, 101): 99,\n",
       " (101, 264): 36,\n",
       " (264, 265): 26,\n",
       " (267, 100): 8,\n",
       " (100, 105): 59,\n",
       " (105, 118): 27,\n",
       " (118, 270): 1,\n",
       " (256, 259): 39,\n",
       " (259, 111): 13,\n",
       " (115, 265): 1,\n",
       " (265, 100): 14,\n",
       " (100, 45): 3,\n",
       " (45, 112): 8,\n",
       " (112, 97): 31,\n",
       " (97, 103): 31,\n",
       " (103, 256): 18,\n",
       " (256, 85): 18,\n",
       " (256, 83): 2,\n",
       " (83, 116): 3,\n",
       " (116, 265): 9,\n",
       " (100, 268): 7,\n",
       " (268, 267): 6,\n",
       " (267, 112): 7,\n",
       " (112, 108): 70,\n",
       " (108, 117): 19,\n",
       " (117, 258): 7,\n",
       " (258, 105): 13,\n",
       " (105, 116): 111,\n",
       " (258, 100): 6,\n",
       " (100, 111): 24,\n",
       " (111, 122): 1,\n",
       " (122, 269): 1,\n",
       " (269, 258): 1,\n",
       " (108, 101): 89,\n",
       " (101, 109): 42,\n",
       " (116, 268): 4,\n",
       " (268, 272): 7,\n",
       " (272, 265): 4,\n",
       " (265, 110): 3,\n",
       " (110, 101): 27,\n",
       " (101, 120): 75,\n",
       " (120, 101): 6,\n",
       " (101, 115): 117,\n",
       " (114, 101): 101,\n",
       " (101, 112): 19,\n",
       " (266, 116): 11,\n",
       " (116, 115): 34,\n",
       " (267, 110): 4,\n",
       " (111, 116): 11,\n",
       " (258, 99): 17,\n",
       " (256, 109): 17,\n",
       " (109, 266): 23,\n",
       " (266, 256): 19,\n",
       " (259, 265): 3,\n",
       " (32, 97): 86,\n",
       " (97, 32): 93,\n",
       " (32, 108): 46,\n",
       " (116, 116): 35,\n",
       " (116, 108): 7,\n",
       " (108, 256): 48,\n",
       " (105, 109): 26,\n",
       " (109, 105): 24,\n",
       " (100, 97): 9,\n",
       " (116, 270): 27,\n",
       " (270, 273): 5,\n",
       " (273, 73): 18,\n",
       " (73, 32): 13,\n",
       " (32, 100): 43,\n",
       " (110, 263): 17,\n",
       " (153, 262): 17,\n",
       " (262, 98): 14,\n",
       " (98, 108): 26,\n",
       " (108, 97): 31,\n",
       " (256, 112): 81,\n",
       " (105, 108): 46,\n",
       " (108, 108): 44,\n",
       " (32, 102): 38,\n",
       " (102, 257): 9,\n",
       " (257, 100): 11,\n",
       " (256, 119): 16,\n",
       " (104, 111): 20,\n",
       " (111, 108): 7,\n",
       " (259, 270): 13,\n",
       " (32, 109): 53,\n",
       " (109, 121): 1,\n",
       " (121, 115): 11,\n",
       " (116, 260): 99,\n",
       " (260, 105): 4,\n",
       " (264, 101): 10,\n",
       " (118, 269): 8,\n",
       " (269, 32): 28,\n",
       " (51, 48): 6,\n",
       " (48, 32): 7,\n",
       " (32, 121): 11,\n",
       " (121, 101): 3,\n",
       " (268, 258): 1,\n",
       " (258, 97): 46,\n",
       " (97, 102): 2,\n",
       " (258, 257): 34,\n",
       " (257, 99): 10,\n",
       " (99, 101): 37,\n",
       " (112, 116): 19,\n",
       " (110, 273): 8,\n",
       " (273, 32): 44,\n",
       " (32, 65): 15,\n",
       " (101, 119): 11,\n",
       " (109, 111): 24,\n",
       " (110, 259): 1,\n",
       " (259, 258): 1,\n",
       " (103, 111): 12,\n",
       " (111, 264): 3,\n",
       " (264, 73): 3,\n",
       " (32, 103): 14,\n",
       " (111, 262): 14,\n",
       " (262, 257): 14,\n",
       " (260, 101): 15,\n",
       " (101, 267): 105,\n",
       " (267, 257): 14,\n",
       " (256, 265): 7,\n",
       " (101, 99): 22,\n",
       " (99, 105): 14,\n",
       " (267, 116): 16,\n",
       " (115, 112): 31,\n",
       " (112, 269): 2,\n",
       " (269, 267): 2,\n",
       " (267, 115): 12,\n",
       " (256, 116): 19,\n",
       " (256, 108): 20,\n",
       " (268, 110): 1,\n",
       " (110, 270): 2,\n",
       " (98, 111): 28,\n",
       " (262, 105): 14,\n",
       " (105, 262): 35,\n",
       " (101, 116): 37,\n",
       " (116, 97): 35,\n",
       " (97, 105): 17,\n",
       " (108, 273): 2,\n",
       " (259, 105): 16,\n",
       " (105, 258): 63,\n",
       " (258, 268): 16,\n",
       " (105, 99): 87,\n",
       " (99, 108): 22,\n",
       " (73, 263): 5,\n",
       " (153, 108): 6,\n",
       " (103, 105): 2,\n",
       " (118, 256): 16,\n",
       " (32, 105): 46,\n",
       " (102, 114): 14,\n",
       " (109, 32): 30,\n",
       " (258, 112): 9,\n",
       " (112, 111): 110,\n",
       " (111, 257): 70,\n",
       " (257, 262): 17,\n",
       " (262, 111): 17,\n",
       " (32, 118): 13,\n",
       " (118, 105): 12,\n",
       " (105, 101): 20,\n",
       " (119, 273): 3,\n",
       " (32, 73): 6,\n",
       " (153, 109): 4,\n",
       " (111, 270): 5,\n",
       " (102, 111): 5,\n",
       " (111, 99): 10,\n",
       " (99, 117): 16,\n",
       " (268, 97): 42,\n",
       " (97, 99): 131,\n",
       " (101, 262): 9,\n",
       " (262, 265): 6,\n",
       " (267, 119): 4,\n",
       " (116, 263): 24,\n",
       " (257, 118): 2,\n",
       " (118, 111): 6,\n",
       " (108, 118): 2,\n",
       " (118, 101): 20,\n",
       " (266, 107): 8,\n",
       " (107, 270): 5,\n",
       " (105, 259): 30,\n",
       " (259, 32): 31,\n",
       " (270, 258): 17,\n",
       " (258, 265): 10,\n",
       " (267, 102): 11,\n",
       " (102, 105): 28,\n",
       " (120, 116): 16,\n",
       " (116, 273): 13,\n",
       " (273, 72): 5,\n",
       " (72, 111): 8,\n",
       " (119, 101): 20,\n",
       " (260, 264): 18,\n",
       " (264, 257): 7,\n",
       " (256, 73): 4,\n",
       " (32, 110): 19,\n",
       " (262, 103): 6,\n",
       " (116, 274): 6,\n",
       " (274, 107): 2,\n",
       " (107, 32): 20,\n",
       " (264, 116): 7,\n",
       " (120, 262): 13,\n",
       " (262, 108): 6,\n",
       " (97, 121): 11,\n",
       " (121, 111): 25,\n",
       " (117, 116): 20,\n",
       " (116, 47): 2,\n",
       " (47, 115): 1,\n",
       " (115, 104): 9,\n",
       " (97, 112): 38,\n",
       " (112, 270): 4,\n",
       " (270, 47): 1,\n",
       " (47, 114): 2,\n",
       " (114, 269): 12,\n",
       " (269, 100): 18,\n",
       " (100, 260): 25,\n",
       " (260, 270): 5,\n",
       " (270, 264): 13,\n",
       " (264, 266): 11,\n",
       " (108, 111): 30,\n",
       " (99, 274): 30,\n",
       " (274, 105): 12,\n",
       " (105, 122): 12,\n",
       " (122, 97): 6,\n",
       " (108, 263): 1,\n",
       " (148, 259): 2,\n",
       " (111, 115): 53,\n",
       " (115, 256): 27,\n",
       " (256, 268): 14,\n",
       " (112, 268): 7,\n",
       " (116, 256): 23,\n",
       " (256, 105): 11,\n",
       " (105, 115): 33,\n",
       " (115, 115): 34,\n",
       " (117, 101): 6,\n",
       " (264, 98): 14,\n",
       " (98, 101): 15,\n",
       " (101, 121): 5,\n",
       " (110, 267): 10,\n",
       " (267, 109): 6,\n",
       " (109, 272): 1,\n",
       " (272, 115): 13,\n",
       " (115, 261): 2,\n",
       " (261, 112): 4,\n",
       " (112, 256): 1,\n",
       " (40, 265): 3,\n",
       " (267, 107): 2,\n",
       " (119, 108): 2,\n",
       " (101, 100): 23,\n",
       " (100, 103): 2,\n",
       " (103, 101): 31,\n",
       " (101, 41): 5,\n",
       " (41, 32): 17,\n",
       " (32, 104): 24,\n",
       " (104, 260): 20,\n",
       " (32, 68): 5,\n",
       " (68, 105): 3,\n",
       " (260, 115): 24,\n",
       " (115, 105): 54,\n",
       " (116, 272): 15,\n",
       " (267, 73): 2,\n",
       " (110, 104): 2,\n",
       " (260, 269): 11,\n",
       " (269, 262): 19,\n",
       " (262, 67): 2,\n",
       " (109, 112): 70,\n",
       " (120, 105): 11,\n",
       " (272, 84): 1,\n",
       " (256, 67): 7,\n",
       " (99, 256): 26,\n",
       " (256, 65): 4,\n",
       " (65, 108): 4,\n",
       " (99, 97): 23,\n",
       " (32, 83): 4,\n",
       " (83, 99): 2,\n",
       " (99, 114): 40,\n",
       " (105, 112): 21,\n",
       " (258, 85): 11,\n",
       " (85, 115): 4,\n",
       " (115, 97): 18,\n",
       " (256, 70): 2,\n",
       " (70, 114): 3,\n",
       " (101, 113): 17,\n",
       " (113, 117): 28,\n",
       " (117, 269): 14,\n",
       " (269, 99): 18,\n",
       " (99, 272): 6,\n",
       " (272, 69): 2,\n",
       " (69, 110): 2,\n",
       " (110, 271): 2,\n",
       " (271, 270): 22,\n",
       " (85, 84): 40,\n",
       " (84, 70): 40,\n",
       " (70, 45): 39,\n",
       " (45, 56): 22,\n",
       " (56, 32): 21,\n",
       " (45, 49): 16,\n",
       " (49, 54): 25,\n",
       " (54, 32): 20,\n",
       " (109, 98): 29,\n",
       " (98, 257): 25,\n",
       " (257, 270): 17,\n",
       " (32, 77): 9,\n",
       " (268, 107): 14,\n",
       " (107, 258): 10,\n",
       " (258, 67): 1,\n",
       " (67, 265): 3,\n",
       " (265, 111): 6,\n",
       " (274, 32): 36,\n",
       " (32, 69): 6,\n",
       " (69, 113): 2,\n",
       " (117, 105): 8,\n",
       " (118, 274): 9,\n",
       " (274, 269): 5,\n",
       " (256, 78): 3,\n",
       " (78, 266): 4,\n",
       " (266, 109): 22,\n",
       " (109, 274): 17,\n",
       " (32, 70): 10,\n",
       " (70, 266): 11,\n",
       " (109, 258): 7,\n",
       " (258, 71): 1,\n",
       " (71, 114): 3,\n",
       " (112, 104): 11,\n",
       " (67, 108): 2,\n",
       " (258, 65): 2,\n",
       " (65, 110): 5,\n",
       " (267, 77): 2,\n",
       " (77, 266): 4,\n",
       " (266, 101): 16,\n",
       " (263, 166): 2,\n",
       " (166, 32): 2,\n",
       " (272, 65): 2,\n",
       " (65, 258): 3,\n",
       " (258, 115): 15,\n",
       " (111, 111): 13,\n",
       " (97, 258): 61,\n",
       " (258, 121): 4,\n",
       " (117, 32): 18,\n",
       " (268, 262): 4,\n",
       " (116, 117): 11,\n",
       " (117, 100): 9,\n",
       " (100, 272): 4,\n",
       " (272, 85): 2,\n",
       " (264, 105): 12,\n",
       " (101, 261): 19,\n",
       " (261, 109): 71,\n",
       " (262, 114): 10,\n",
       " (115, 269): 13,\n",
       " (108, 268): 7,\n",
       " (268, 103): 7,\n",
       " (256, 106): 2,\n",
       " (106, 117): 9,\n",
       " (117, 109): 16,\n",
       " (112, 32): 21,\n",
       " (32, 261): 40,\n",
       " (272, 111): 11,\n",
       " (111, 118): 15,\n",
       " (32, 99): 33,\n",
       " (258, 108): 7,\n",
       " (65, 83): 9,\n",
       " (83, 67): 9,\n",
       " (67, 73): 9,\n",
       " (73, 73): 9,\n",
       " (262, 121): 4,\n",
       " (109, 97): 39,\n",
       " (97, 272): 6,\n",
       " (272, 98): 3,\n",
       " (256, 102): 17,\n",
       " (102, 97): 6,\n",
       " (105, 268): 1,\n",
       " (259, 273): 1,\n",
       " (73, 116): 3,\n",
       " (258, 110): 6,\n",
       " (262, 106): 2,\n",
       " (115, 262): 27,\n",
       " (262, 259): 22,\n",
       " (256, 261): 21,\n",
       " (261, 110): 30,\n",
       " (97, 257): 13,\n",
       " (257, 258): 5,\n",
       " (109, 117): 14,\n",
       " (103, 260): 3,\n",
       " (110, 117): 9,\n",
       " (98, 260): 6,\n",
       " (264, 274): 4,\n",
       " (274, 259): 2,\n",
       " (273, 85): 9,\n",
       " (274, 115): 12,\n",
       " (101, 97): 32,\n",
       " (262, 100): 6,\n",
       " (101, 274): 3,\n",
       " (260, 110): 8,\n",
       " (110, 274): 6,\n",
       " (264, 102): 4,\n",
       " (112, 101): 20,\n",
       " (105, 274): 9,\n",
       " (97, 115): 46,\n",
       " (264, 109): 5,\n",
       " (97, 107): 9,\n",
       " (110, 256): 12,\n",
       " (262, 101): 4,\n",
       " (120, 112): 13,\n",
       " (99, 262): 6,\n",
       " (262, 97): 15,\n",
       " (260, 256): 22,\n",
       " (256, 263): 10,\n",
       " (156, 99): 7,\n",
       " (87, 101): 3,\n",
       " (101, 256): 7,\n",
       " (32, 87): 6,\n",
       " (87, 104): 7,\n",
       " (104, 269): 8,\n",
       " (110, 102): 2,\n",
       " (258, 261): 7,\n",
       " (116, 121): 10,\n",
       " (121, 264): 16,\n",
       " (108, 272): 55,\n",
       " (272, 97): 4,\n",
       " (32, 269): 15,\n",
       " (269, 103): 8,\n",
       " (103, 257): 7,\n",
       " (257, 101): 7,\n",
       " (101, 260): 1,\n",
       " (258, 104): 8,\n",
       " (257, 267): 3,\n",
       " (267, 111): 12,\n",
       " (101, 108): 43,\n",
       " (108, 102): 1,\n",
       " (115, 107): 3,\n",
       " (264, 263): 3,\n",
       " (156, 87): 1,\n",
       " (104, 272): 1,\n",
       " (272, 100): 8,\n",
       " (256, 110): 13,\n",
       " (101, 101): 10,\n",
       " (267, 274): 2,\n",
       " (115, 63): 1,\n",
       " (63, 32): 3,\n",
       " (73, 258): 2,\n",
       " (258, 259): 31,\n",
       " (258, 114): 4,\n",
       " (115, 268): 2,\n",
       " (268, 121): 7,\n",
       " (121, 63): 1,\n",
       " (117, 108): 34,\n",
       " (100, 110): 3,\n",
       " (105, 102): 20,\n",
       " (100, 63): 1,\n",
       " (63, 263): 1,\n",
       " (32, 32): 19,\n",
       " (32, 72): 8,\n",
       " (264, 85): 10,\n",
       " (258, 116): 18,\n",
       " (259, 102): 1,\n",
       " (102, 117): 8,\n",
       " (272, 114): 3,\n",
       " (256, 269): 8,\n",
       " (105, 114): 22,\n",
       " (114, 256): 9,\n",
       " (100, 263): 3,\n",
       " (119, 114): 8,\n",
       " (115, 121): 14,\n",
       " (109, 115): 5,\n",
       " (115, 273): 37,\n",
       " (273, 84): 25,\n",
       " (110, 115): 13,\n",
       " (115, 266): 6,\n",
       " (105, 117): 1,\n",
       " (109, 263): 3,\n",
       " (267, 103): 2,\n",
       " (111, 274): 1,\n",
       " (258, 263): 5,\n",
       " (156, 269): 2,\n",
       " (269, 97): 4,\n",
       " (108, 270): 2,\n",
       " (101, 111): 6,\n",
       " (111, 112): 20,\n",
       " (268, 111): 3,\n",
       " (117, 110): 27,\n",
       " (267, 259): 9,\n",
       " (108, 267): 8,\n",
       " (32, 117): 10,\n",
       " (112, 117): 9,\n",
       " (265, 272): 23,\n",
       " (272, 108): 4,\n",
       " (108, 265): 31,\n",
       " (265, 103): 21,\n",
       " (103, 117): 17,\n",
       " (117, 97): 16,\n",
       " (157, 273): 7,\n",
       " (273, 65): 8,\n",
       " (264, 259): 30,\n",
       " (256, 100): 25,\n",
       " (116, 269): 11,\n",
       " (269, 115): 4,\n",
       " (101, 33): 3,\n",
       " (84, 111): 4,\n",
       " (258, 49): 7,\n",
       " (49, 51): 3,\n",
       " (51, 53): 2,\n",
       " (53, 32): 4,\n",
       " (102, 102): 14,\n",
       " (102, 260): 9,\n",
       " (262, 115): 20,\n",
       " (115, 99): 15,\n",
       " (264, 261): 7,\n",
       " (261, 118): 4,\n",
       " (256, 49): 6,\n",
       " (49, 49): 19,\n",
       " (49, 48): 24,\n",
       " (48, 48): 32,\n",
       " (259, 260): 34,\n",
       " (110, 103): 5,\n",
       " (103, 32): 4,\n",
       " (32, 49): 20,\n",
       " (111, 259): 15,\n",
       " (267, 104): 3,\n",
       " (104, 105): 34,\n",
       " (116, 266): 14,\n",
       " (266, 105): 13,\n",
       " (274, 264): 2,\n",
       " (264, 119): 21,\n",
       " (97, 100): 16,\n",
       " (100, 100): 7,\n",
       " (100, 273): 4,\n",
       " (32, 71): 3,\n",
       " (71, 105): 1,\n",
       " (258, 269): 4,\n",
       " (269, 266): 1,\n",
       " (111, 106): 4,\n",
       " (106, 101): 1,\n",
       " (256, 101): 7,\n",
       " (98, 114): 9,\n",
       " (99, 99): 10,\n",
       " (272, 257): 7,\n",
       " (257, 104): 2,\n",
       " (258, 109): 8,\n",
       " (100, 256): 13,\n",
       " (104, 117): 2,\n",
       " (109, 265): 13,\n",
       " (73, 262): 8,\n",
       " (111, 101): 8,\n",
       " (115, 110): 8,\n",
       " (101, 45): 15,\n",
       " (45, 111): 4,\n",
       " (102, 258): 1,\n",
       " (110, 264): 12,\n",
       " (267, 105): 4,\n",
       " (258, 101): 14,\n",
       " (120, 99): 3,\n",
       " (110, 258): 9,\n",
       " (119, 110): 7,\n",
       " (32, 114): 18,\n",
       " (272, 116): 8,\n",
       " (114, 259): 2,\n",
       " (270, 108): 9,\n",
       " (98, 117): 14,\n",
       " (262, 274): 7,\n",
       " (108, 116): 10,\n",
       " (261, 101): 1,\n",
       " (262, 119): 13,\n",
       " (259, 257): 4,\n",
       " (148, 119): 2,\n",
       " (121, 273): 4,\n",
       " (77, 111): 1,\n",
       " (262, 112): 4,\n",
       " (109, 270): 2,\n",
       " (97, 118): 14,\n",
       " (105, 98): 17,\n",
       " (114, 268): 5,\n",
       " (268, 105): 13,\n",
       " (118, 97): 9,\n",
       " (104, 265): 7,\n",
       " (100, 108): 7,\n",
       " (256, 103): 4,\n",
       " (103, 266): 2,\n",
       " (266, 272): 4,\n",
       " (119, 45): 1,\n",
       " (45, 108): 2,\n",
       " (108, 258): 5,\n",
       " (265, 105): 2,\n",
       " (264, 121): 6,\n",
       " (117, 263): 5,\n",
       " (262, 99): 5,\n",
       " (99, 260): 2,\n",
       " (260, 116): 5,\n",
       " (32, 266): 13,\n",
       " (266, 100): 15,\n",
       " (272, 259): 7,\n",
       " (259, 101): 22,\n",
       " (109, 273): 4,\n",
       " (97, 267): 2,\n",
       " (267, 268): 3,\n",
       " (108, 264): 2,\n",
       " (261, 117): 7,\n",
       " (257, 107): 3,\n",
       " (256, 98): 19,\n",
       " (98, 105): 30,\n",
       " (259, 114): 4,\n",
       " (101, 105): 17,\n",
       " (273, 69): 2,\n",
       " (69, 109): 1,\n",
       " (121, 33): 1,\n",
       " (256, 76): 1,\n",
       " (76, 101): 2,\n",
       " (103, 269): 1,\n",
       " (269, 260): 1,\n",
       " (260, 274): 4,\n",
       " (105, 269): 6,\n",
       " (98, 97): 14,\n",
       " (99, 32): 10,\n",
       " (32, 101): 32,\n",
       " (148, 105): 2,\n",
       " (157, 264): 7,\n",
       " (260, 109): 4,\n",
       " (262, 113): 1,\n",
       " (256, 114): 18,\n",
       " (148, 268): 3,\n",
       " (267, 271): 12,\n",
       " (273, 67): 3,\n",
       " (100, 269): 5,\n",
       " (267, 98): 9,\n",
       " (98, 272): 14,\n",
       " (264, 99): 4,\n",
       " (109, 268): 12,\n",
       " (272, 119): 5,\n",
       " (120, 97): 11,\n",
       " (101, 102): 19,\n",
       " (105, 120): 6,\n",
       " (120, 32): 12,\n",
       " (156, 85): 2,\n",
       " (85, 43): 39,\n",
       " (43, 263): 1,\n",
       " (264, 115): 12,\n",
       " (43, 48): 24,\n",
       " (48, 52): 3,\n",
       " (52, 49): 3,\n",
       " (49, 32): 9,\n",
       " (156, 65): 2,\n",
       " (65, 263): 2,\n",
       " (116, 257): 7,\n",
       " (112, 105): 9,\n",
       " (48, 51): 10,\n",
       " (51, 66): 1,\n",
       " (66, 56): 1,\n",
       " (156, 206): 1,\n",
       " (206, 184): 1,\n",
       " (184, 263): 1,\n",
       " (101, 107): 2,\n",
       " (115, 109): 8,\n",
       " (97, 273): 1,\n",
       " (69, 97): 2,\n",
       " (32, 271): 32,\n",
       " (104, 266): 1,\n",
       " (262, 110): 3,\n",
       " (267, 113): 1,\n",
       " (112, 260): 12,\n",
       " (67, 104): 2,\n",
       " (68, 97): 1,\n",
       " (256, 271): 26,\n",
       " (49, 44): 1,\n",
       " (44, 49): 2,\n",
       " (49, 52): 2,\n",
       " (52, 44): 1,\n",
       " (49, 50): 6,\n",
       " (264, 111): 2,\n",
       " (110, 108): 8,\n",
       " (272, 49): 3,\n",
       " (50, 56): 4,\n",
       " (56, 44): 1,\n",
       " (44, 50): 1,\n",
       " (50, 51): 4,\n",
       " (51, 55): 2,\n",
       " (148, 97): 2,\n",
       " (262, 49): 2,\n",
       " (50, 37): 1,\n",
       " (37, 32): 1,\n",
       " (117, 274): 15,\n",
       " (103, 110): 8,\n",
       " (100, 264): 9,\n",
       " (108, 269): 8,\n",
       " (119, 259): 1,\n",
       " (259, 33): 1,\n",
       " (115, 260): 7,\n",
       " (260, 118): 3,\n",
       " (55, 44): 1,\n",
       " (44, 52): 1,\n",
       " (52, 54): 1,\n",
       " (54, 56): 1,\n",
       " (156, 112): 3,\n",
       " (32, 268): 5,\n",
       " (268, 101): 11,\n",
       " (268, 100): 1,\n",
       " (122, 101): 2,\n",
       " (265, 270): 1,\n",
       " (257, 256): 1,\n",
       " (114, 112): 3,\n",
       " (105, 267): 5,\n",
       " (116, 264): 18,\n",
       " (108, 112): 1,\n",
       " (112, 102): 1,\n",
       " (122, 256): 4,\n",
       " (66, 101): 2,\n",
       " (268, 114): 2,\n",
       " (114, 265): 6,\n",
       " (261, 104): 1,\n",
       " (101, 59): 2,\n",
       " (59, 32): 7,\n",
       " (115, 113): 4,\n",
       " (117, 268): 4,\n",
       " (54, 195): 1,\n",
       " (195, 151): 1,\n",
       " (151, 49): 1,\n",
       " (32, 61): 6,\n",
       " (61, 32): 6,\n",
       " (50, 53): 2,\n",
       " (53, 54): 1,\n",
       " (267, 101): 3,\n",
       " (265, 101): 12,\n",
       " (32, 54): 1,\n",
       " (54, 53): 5,\n",
       " (53, 44): 3,\n",
       " (44, 53): 3,\n",
       " (53, 51): 3,\n",
       " (51, 54): 3,\n",
       " (258, 274): 8,\n",
       " (274, 116): 1,\n",
       " (101, 259): 7,\n",
       " (260, 273): 11,\n",
       " (77, 97): 2,\n",
       " (40, 99): 3,\n",
       " (99, 107): 9,\n",
       " (32, 122): 4,\n",
       " (122, 111): 4,\n",
       " (109, 41): 3,\n",
       " (258, 117): 7,\n",
       " (66, 108): 1,\n",
       " (117, 256): 2,\n",
       " (264, 103): 2,\n",
       " (101, 269): 7,\n",
       " (45, 117): 4,\n",
       " (114, 114): 9,\n",
       " (103, 97): 10,\n",
       " (258, 40): 4,\n",
       " (40, 109): 2,\n",
       " (260, 41): 3,\n",
       " (268, 115): 2,\n",
       " (108, 121): 14,\n",
       " (262, 261): 6,\n",
       " ...}"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(101, 32): 256,\n",
       " (105, 110): 257,\n",
       " (115, 32): 258,\n",
       " (116, 104): 259,\n",
       " (101, 114): 260,\n",
       " (99, 111): 261,\n",
       " (116, 32): 262,\n",
       " (226, 128): 263,\n",
       " (44, 32): 264,\n",
       " (97, 110): 265,\n",
       " (111, 114): 266,\n",
       " (100, 32): 267,\n",
       " (97, 114): 268,\n",
       " (101, 110): 269,\n",
       " (257, 103): 270,\n",
       " (261, 100): 271,\n",
       " (121, 32): 272,\n",
       " (46, 32): 273,\n",
       " (97, 108): 274,\n",
       " (259, 256): 275}"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[104, 101, 108, 108, 111, 32, 119, 266, 108, 100, 33]\n"
     ]
    }
   ],
   "source": [
    "def encode(text):\n",
    "  # given a string, return list of integers (the tokens)\n",
    "  tokens = list(text.encode(\"utf-8\"))\n",
    "  while len(tokens) >= 2: #fixes bug with min, when one token in sequence\n",
    "    stats = get_stats(tokens) #Will be getting all the possible byte pairs, dont care about number of times they appear\n",
    "    #We moreso care about finding the byte pair which was added earliest to the merges dict; since remember later custom byte pairs can be a combination of earlier custom tokens/bytes; so need to derive the token representations of early byte pairs to progressively convert the sequence\n",
    "    pair = min(stats, key=lambda p: merges.get(p, float(\"inf\"))) #Similiar to Max, Min will iterate over the keys in the dict STATS. In order to sort the keys, need to define a function, \n",
    "    #in this case using merges.get, which will return the int index we used for byte pairs. Byte pairs which were added earlier have a lower int index. \"inf\" is used to assign values for pairs in STATS that do not appear in merges\n",
    "    if pair not in merges:\n",
    "      break # nothing else can be merged\n",
    "    #if we find a pair which can be merged/inside merges, then use the prior merge function\n",
    "    idx = merges[pair]\n",
    "    tokens = merge(tokens, pair, idx)\n",
    "  return tokens\n",
    "\n",
    "print(encode(\"hello world!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print(decode(encode(\"hello world\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "text2 = decode(encode(text))\n",
    "print(text2 == text)\n",
    "#Checking after encoding and decoding, get the same thing back w.r.t to tokenizer training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#Also testing out validation data (data not used to build/seen by tokenizer)\n",
    "valtext = \"Many common characters, including numerals, punctuation, and other symbols, are unified within the standard and are not treated as specific to any given writing system. Unicode encodes thousands of emoji, with the continued development thereof conducted by the Consortium as a part of the standard.[4] Moreover, the widespread adoption of Unicode was in large part responsible for the initial popularization of emoji outside of Japan. Unicode is ultimately capable of encoding more than 1.1 million characters.\"\n",
    "valtext2 = decode(encode(valtext))\n",
    "print(valtext2 == valtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The \"parameters\" of the Byte Pair Encoding tokenizer are really the dictionary of merges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is the basics of the Byte Pair Encoding Tokenizer, however current approaches to tokenization for LLMs have expanded upon this with additional complexity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the GPT series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One issues the GPT team recognized, is that common words like DOG, might appear frequently with punctutation such as DOG!, DOG., DOG? and naively the BP algo could merge these, ending up with a lot of DOG tokens with slightly different punctuation. So feels like you are clustering things that shouldnt be clustered (i.e semantics and punctuation); which is suboptimal. So the team wanted to enforce that some characters should never be merged together "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forced splits using regex patterns (GPT series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ' world', ' how', ' are', ' you']\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "gpt2pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "print(re.findall(gpt2pat, \"Hello world how are you\")) #finds all the occurances of the \"gpt2pat\" pattern in an arbitrary string (going from left to right) and places in list\n",
    "#Looking at \"Hello\" (really starting at  \"H\"), we can see it is not 's, 't etc... but it is \" ?\\p{L}+\"\"; where \" ?\\p{L}+\" refers to optional space + any letters and HELLO is made up of letters\n",
    "#But the match ends, since the white space between HELLO WORLD is not a letter. Now a new attempt starts to match \" world\", where once again \" ?\\p{N}+\" will work where \" WORLD\" has a starting space = optional space + a bunch of letters\n",
    "#So we get a set of elements..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once these list of elements/texts are created from an input text, all these elements are processed independently by the tokenizer into a token sequence and all the results of this processing are concatenated\n",
    "\n",
    "Thus will only find merges with each elements of the list individually, I.E only consider merges in \"Hello\". After you have done all the possible merging for these elements individually, the results of all that will be joined by concatenation\n",
    "\n",
    "So, for instance will never merge the e in are with a space in \" you\" as these are now seperate elements in the list\n",
    "\n",
    "So this REGEX pattern, is one approach used to enforce that certain merges will not happen\n",
    "\n",
    "At high level, the above pattern is used to mitigate merging across letters, across numbers, across punctuation etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', \"'ve\", ' world', '123', ' how', \"'s\", ' HOW', \"'\", 'S', ' are', '           ', ' you', '!!?', '  ']\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "gpt2pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "#\" ?\\p{N}+\" similiar to the p{L} but instead numbers\n",
    "#'ve looks for exactly 've\n",
    "#Can see tokenization inconsistent in upper case HOW'S\n",
    "#  ?[^\\s\\p{L}\\p{N}]+ -> optional space followed by somthing that is not a space,letter,number and can have 1 or more of that; roughly speaking looking to match punctuation\n",
    "# \\s+(?!\\S) -> Mathching any number of white space \\s+ up to but no including the last white space char ((?!\\S) -> called negative look ahead assertion). Now when we have alot of space in front of \"are        you\" it preserves the space before you -> \" you\" \n",
    "#The above is useful because in a sentence without this large amount of space, \" you\" is the commen token and not \"you\" with its prior space stripped; and so it ensures we still get the common \" you\" token in scenarios where there is extra space\n",
    "print(re.findall(gpt2pat, \"Hello've world123 how's HOW'S are            you!!?  \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', 'for', ' i', ' in', ' range', '(', '1', ',', ' 101', '):', '\\n   ', ' if', ' i', ' %', ' 3', ' ==', ' 0', ' and', ' i', ' %', ' 5', ' ==', ' 0', ':', '\\n       ', ' print', '(\"', 'FizzBuzz', '\")', '\\n   ', ' elif', ' i', ' %', ' 3', ' ==', ' 0', ':', '\\n       ', ' print', '(\"', 'Fizz', '\")', '\\n   ', ' elif', ' i', ' %', ' 5', ' ==', ' 0', ':', '\\n       ', ' print', '(\"', 'Buzz', '\")', '\\n   ', ' else', ':', '\\n       ', ' print', '(', 'i', ')', '\\n']\n"
     ]
    }
   ],
   "source": [
    "example = \"\"\"\n",
    "for i in range(1, 101):\n",
    "    if i % 3 == 0 and i % 5 == 0:\n",
    "        print(\"FizzBuzz\")\n",
    "    elif i % 3 == 0:\n",
    "        print(\"Fizz\")\n",
    "    elif i % 5 == 0:\n",
    "        print(\"Buzz\")\n",
    "    else:\n",
    "        print(i)\n",
    "\"\"\"\n",
    "print(re.findall(gpt2pat, example))\n",
    "\n",
    "#There are some additional rules employed by OPENAI during tokenization, where its not as simple splitting text into chunks and then running the BPE algo over the chunks\n",
    "#For instance there are a elements such as \"      \\n\" where there are alot of consective spaces, but OPENAI does'nt just merge these spaces, and instead each space is still\n",
    "#represented by an independent token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tiktoken is a tokenization library from OpenAI, allows for tokenization inference (training is hidden by OpenAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[220, 220, 220, 220, 220, 23748, 62, 6894, 10185]\n",
      "[415, 24748, 1917, 12340]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# GPT-2 (does not merge spaces)\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "print(enc.encode(\"      hello_world!!!\"))\n",
    "\n",
    "#GPT-4 (merge spaces); uses different REGEX expression to chunk up text\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "print(enc.encode(\"      hello world!!!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-07-24 15:17:44--  https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/vocab.bpe\n",
      "Resolving openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)... 20.150.77.132\n",
      "Connecting to openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)|20.150.77.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 456318 (446K) [application/octet-stream]\n",
      "Saving to: â€˜vocab.bpeâ€™\n",
      "\n",
      "vocab.bpe           100%[===================>] 445.62K  1.88MB/s    in 0.2s    \n",
      "\n",
      "2024-07-24 15:17:45 (1.88 MB/s) - â€˜vocab.bpeâ€™ saved [456318/456318]\n",
      "\n",
      "--2024-07-24 15:17:45--  https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/encoder.json\n",
      "Resolving openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)... 20.150.77.132\n",
      "Connecting to openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)|20.150.77.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1042301 (1018K) [application/json]\n",
      "Saving to: â€˜encoder.jsonâ€™\n",
      "\n",
      "encoder.json        100%[===================>]   1018K  2.41MB/s    in 0.4s    \n",
      "\n",
      "2024-07-24 15:17:46 (2.41 MB/s) - â€˜encoder.jsonâ€™ saved [1042301/1042301]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/vocab.bpe\n",
    "!wget https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/encoder.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "\n",
    "with open('video_9_dependencies/encoder.json', 'r') as f:\n",
    "    encoder = json.load(f) # <--- ~equivalent to our \"vocab\" (i.e mapping of ints/ids to byte representations)\n",
    "\n",
    "with open('video_9_dependencies/vocab.bpe', 'r', encoding=\"utf-8\") as f:\n",
    "    bpe_data = f.read()\n",
    "bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split('\\n')[1:-1]]\n",
    "# ^---- ~equivalent to our \"merges (i.e byte pairs, in int representations and their mapping to new int token)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPECIAL TOKENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoder) #256 raw byte tokens. 50,000 merges. 1 special token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder[\"<|endoftext|>\"]\n",
    "#This is the special last token\n",
    "#This is used to delimit documents in the training set\n",
    "#This used to signal to the model, that the document has ended, and what follows is unrelated to the document previously \n",
    "#That said, the LM needs to learn this from the data (i.e learn to wipe its memory from before and what came before is not useful for predicting what comes next)\n",
    "\n",
    "#Tokenizer code has special case instructions to come in and swap in these Special Tokens; outside of BPE\n",
    "\n",
    "#It is common to train a base language model, and that to faciliate its use for applications such as an assistant\n",
    "#add additional Special Tokens to better formet the input. This does require doing \"surgery\" on a trained model, where\n",
    "#an additional embedding vector needs to be added to the table alongside an additional dimension to the model output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving on from TikToken to instead SentencePiece (which can be used for both training and inference) and is used by both Llama and Mistral series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/bhavverma/Documents/karpathy_notes_and_solutions/video_9_dependencies/toy.txt\", \"w\", encoding = \"utf-8\") as f:\n",
    "  f.write(\"SentencePiece is an unsupervised text tokenizer and detokenizer mainly for Neural Network-based text generation systems where the vocabulary size is predetermined prior to the neural model training. SentencePiece implements subword units (e.g., byte-pair-encoding (BPE) [Sennrich et al.]) and unigram language model [Kudo.]) with the extension of direct training from raw sentences. SentencePiece allows us to make a purely end-to-end system that does not depend on language-specific pre/postprocessing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /Users/bhavverma/Documents/karpathy_notes_and_solutions/video_7_dependencies/toy.txt\n",
      "  input_format: text\n",
      "  model_prefix: tok400\n",
      "  model_type: BPE\n",
      "  vocab_size: 400\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.99995\n",
      "  input_sentence_size: 200000000\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 8\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 1\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 1\n",
      "  required_chars: \n",
      "  byte_fallback: 1\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  â‡ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: identity\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 0\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: /Users/bhavverma/Documents/karpathy_notes_and_solutions/video_7_dependencies/toy.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 1 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x00>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x01>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x02>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x03>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x04>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x05>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x06>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x07>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x08>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x09>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x0A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x0B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x0C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x0D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x0E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x0F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x10>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x11>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x12>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x13>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x14>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x15>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x16>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x17>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x18>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x19>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x1A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x1B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x1C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x1D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x1E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x1F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x20>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x21>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x22>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x23>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x24>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x25>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x26>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x27>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x28>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x29>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x2A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x2B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x2C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x2D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x2E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x2F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x30>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x31>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x32>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x33>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x34>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x35>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x36>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x37>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x38>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x39>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x3A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x3B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x3C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x3D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x3E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x3F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x40>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x41>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x42>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x43>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x44>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x45>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x46>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x47>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x48>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x49>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x4A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x4B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x4C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x4D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x4E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x4F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x50>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x51>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x52>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x53>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x54>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x55>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x56>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x57>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x58>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x59>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x5A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x5B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x5C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x5D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x5E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x5F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x60>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x61>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x62>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x63>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x64>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x65>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x66>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x67>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x68>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x69>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x6A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x6B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x6C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x6D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x6E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x6F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x70>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x71>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x72>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x73>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "meta_piece: <0x74>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x75>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x76>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x77>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x78>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x79>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x7A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x7B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x7C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x7D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x7E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x7F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x80>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x81>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x82>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x83>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x84>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x85>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x86>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x87>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x88>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x89>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x8A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x8B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x8C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x8D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x8E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x8F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x90>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x91>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x92>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x93>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x94>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x95>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x96>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x97>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x98>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x99>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x9A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x9B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x9C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x9D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x9E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x9F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA0>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA1>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA2>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA3>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA4>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA5>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA6>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA7>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA8>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA9>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xAA>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xAB>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xAC>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xAD>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xAE>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xAF>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB0>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB1>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB2>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB3>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB4>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB5>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB6>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB7>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB8>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB9>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xBA>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xBB>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xBC>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xBD>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xBE>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xBF>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC0>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC1>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC2>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC3>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC4>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC5>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC6>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC7>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC8>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC9>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xCA>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xCB>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xCC>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xCD>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xCE>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xCF>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD0>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD1>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD2>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD3>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD4>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD5>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD6>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD7>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD8>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD9>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xDA>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xDB>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xDC>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xDD>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xDE>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xDF>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE0>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE1>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE2>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE3>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE4>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE5>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE6>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE7>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE8>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE9>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xEA>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xEB>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xEC>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xED>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xEE>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xEF>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF0>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF1>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF2>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF3>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF4>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF5>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF6>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF7>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF8>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF9>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xFA>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xFB>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xFC>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xFD>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xFE>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xFF>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=504\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=39\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 1 sentences.\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 1\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 58\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=18 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=20 all=283 active=244 piece=ed\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=40 all=305 active=266 piece=.]\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=60 all=324 active=285 piece=ken\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=80 all=334 active=295 piece=â–model\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=100 all=338 active=299 piece=lo\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1 min_freq=0\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: tok400.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: tok400.vocab\n"
     ]
    }
   ],
   "source": [
    "# train a sentencepiece model on it\n",
    "# the settings here are (best effort) those used for training Llama 2\n",
    "import os\n",
    "\n",
    "options = dict(\n",
    "  # input spec\n",
    "  input=\"/Users/bhavverma/Documents/karpathy_notes_and_solutions/video_9_dependencies/toy.txt\",\n",
    "  input_format=\"text\",\n",
    "  # output spec\n",
    "  model_prefix=\"video_9_dependencies/tok400\", # output filename prefix\n",
    "  # algorithm spec\n",
    "  # BPE alg\n",
    "  model_type=\"bpe\",\n",
    "  vocab_size=400,\n",
    "  #normalization (preprocessing of the input)\n",
    "  #Prior to LLM, alot of work was put into removing double white whitespace, making lower case; i.e simplify data\n",
    "  #However Karpathy recommends not to do this with LLMS, you want to not touch your data, keep raw data as much as possible in raw form\n",
    "  #So good idea to turn off normalization\n",
    "  normalization_rule_name=\"identity\", # ew, turn off normalization\n",
    "  remove_extra_whitespaces=False,\n",
    "  input_sentence_size=200000000, # max number of training sentences\n",
    "  max_sentence_length=4192, # max number of bytes per sentence\n",
    "  seed_sentencepiece_size=1000000,\n",
    "  shuffle_input_sentence=True,\n",
    "  # rare word treatment\n",
    "  character_coverage=0.99995,\n",
    "  byte_fallback=True,\n",
    "  # merge rules\n",
    "  split_digits=True,\n",
    "  split_by_unicode_script=True,\n",
    "  split_by_whitespace=True,\n",
    "  split_by_number=True,\n",
    "  max_sentencepiece_length=16,\n",
    "  add_dummy_prefix=True,\n",
    "  allow_whitespace_only_pieces=True,\n",
    "  # special tokens\n",
    "  unk_id=0, # the UNK token MUST exist\n",
    "  bos_id=1, # the others are optional, set to -1 to turn off\n",
    "  eos_id=2,\n",
    "  pad_id=-1, #-1 means choose to not use\n",
    "  # systems\n",
    "  num_threads=os.cpu_count(), # use ~all system resources\n",
    ")\n",
    "\n",
    "spm.SentencePieceTrainer.train(**options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<unk>', 0],\n",
       " ['<s>', 1],\n",
       " ['</s>', 2],\n",
       " ['<0x00>', 3],\n",
       " ['<0x01>', 4],\n",
       " ['<0x02>', 5],\n",
       " ['<0x03>', 6],\n",
       " ['<0x04>', 7],\n",
       " ['<0x05>', 8],\n",
       " ['<0x06>', 9],\n",
       " ['<0x07>', 10],\n",
       " ['<0x08>', 11],\n",
       " ['<0x09>', 12],\n",
       " ['<0x0A>', 13],\n",
       " ['<0x0B>', 14],\n",
       " ['<0x0C>', 15],\n",
       " ['<0x0D>', 16],\n",
       " ['<0x0E>', 17],\n",
       " ['<0x0F>', 18],\n",
       " ['<0x10>', 19],\n",
       " ['<0x11>', 20],\n",
       " ['<0x12>', 21],\n",
       " ['<0x13>', 22],\n",
       " ['<0x14>', 23],\n",
       " ['<0x15>', 24],\n",
       " ['<0x16>', 25],\n",
       " ['<0x17>', 26],\n",
       " ['<0x18>', 27],\n",
       " ['<0x19>', 28],\n",
       " ['<0x1A>', 29],\n",
       " ['<0x1B>', 30],\n",
       " ['<0x1C>', 31],\n",
       " ['<0x1D>', 32],\n",
       " ['<0x1E>', 33],\n",
       " ['<0x1F>', 34],\n",
       " ['<0x20>', 35],\n",
       " ['<0x21>', 36],\n",
       " ['<0x22>', 37],\n",
       " ['<0x23>', 38],\n",
       " ['<0x24>', 39],\n",
       " ['<0x25>', 40],\n",
       " ['<0x26>', 41],\n",
       " ['<0x27>', 42],\n",
       " ['<0x28>', 43],\n",
       " ['<0x29>', 44],\n",
       " ['<0x2A>', 45],\n",
       " ['<0x2B>', 46],\n",
       " ['<0x2C>', 47],\n",
       " ['<0x2D>', 48],\n",
       " ['<0x2E>', 49],\n",
       " ['<0x2F>', 50],\n",
       " ['<0x30>', 51],\n",
       " ['<0x31>', 52],\n",
       " ['<0x32>', 53],\n",
       " ['<0x33>', 54],\n",
       " ['<0x34>', 55],\n",
       " ['<0x35>', 56],\n",
       " ['<0x36>', 57],\n",
       " ['<0x37>', 58],\n",
       " ['<0x38>', 59],\n",
       " ['<0x39>', 60],\n",
       " ['<0x3A>', 61],\n",
       " ['<0x3B>', 62],\n",
       " ['<0x3C>', 63],\n",
       " ['<0x3D>', 64],\n",
       " ['<0x3E>', 65],\n",
       " ['<0x3F>', 66],\n",
       " ['<0x40>', 67],\n",
       " ['<0x41>', 68],\n",
       " ['<0x42>', 69],\n",
       " ['<0x43>', 70],\n",
       " ['<0x44>', 71],\n",
       " ['<0x45>', 72],\n",
       " ['<0x46>', 73],\n",
       " ['<0x47>', 74],\n",
       " ['<0x48>', 75],\n",
       " ['<0x49>', 76],\n",
       " ['<0x4A>', 77],\n",
       " ['<0x4B>', 78],\n",
       " ['<0x4C>', 79],\n",
       " ['<0x4D>', 80],\n",
       " ['<0x4E>', 81],\n",
       " ['<0x4F>', 82],\n",
       " ['<0x50>', 83],\n",
       " ['<0x51>', 84],\n",
       " ['<0x52>', 85],\n",
       " ['<0x53>', 86],\n",
       " ['<0x54>', 87],\n",
       " ['<0x55>', 88],\n",
       " ['<0x56>', 89],\n",
       " ['<0x57>', 90],\n",
       " ['<0x58>', 91],\n",
       " ['<0x59>', 92],\n",
       " ['<0x5A>', 93],\n",
       " ['<0x5B>', 94],\n",
       " ['<0x5C>', 95],\n",
       " ['<0x5D>', 96],\n",
       " ['<0x5E>', 97],\n",
       " ['<0x5F>', 98],\n",
       " ['<0x60>', 99],\n",
       " ['<0x61>', 100],\n",
       " ['<0x62>', 101],\n",
       " ['<0x63>', 102],\n",
       " ['<0x64>', 103],\n",
       " ['<0x65>', 104],\n",
       " ['<0x66>', 105],\n",
       " ['<0x67>', 106],\n",
       " ['<0x68>', 107],\n",
       " ['<0x69>', 108],\n",
       " ['<0x6A>', 109],\n",
       " ['<0x6B>', 110],\n",
       " ['<0x6C>', 111],\n",
       " ['<0x6D>', 112],\n",
       " ['<0x6E>', 113],\n",
       " ['<0x6F>', 114],\n",
       " ['<0x70>', 115],\n",
       " ['<0x71>', 116],\n",
       " ['<0x72>', 117],\n",
       " ['<0x73>', 118],\n",
       " ['<0x74>', 119],\n",
       " ['<0x75>', 120],\n",
       " ['<0x76>', 121],\n",
       " ['<0x77>', 122],\n",
       " ['<0x78>', 123],\n",
       " ['<0x79>', 124],\n",
       " ['<0x7A>', 125],\n",
       " ['<0x7B>', 126],\n",
       " ['<0x7C>', 127],\n",
       " ['<0x7D>', 128],\n",
       " ['<0x7E>', 129],\n",
       " ['<0x7F>', 130],\n",
       " ['<0x80>', 131],\n",
       " ['<0x81>', 132],\n",
       " ['<0x82>', 133],\n",
       " ['<0x83>', 134],\n",
       " ['<0x84>', 135],\n",
       " ['<0x85>', 136],\n",
       " ['<0x86>', 137],\n",
       " ['<0x87>', 138],\n",
       " ['<0x88>', 139],\n",
       " ['<0x89>', 140],\n",
       " ['<0x8A>', 141],\n",
       " ['<0x8B>', 142],\n",
       " ['<0x8C>', 143],\n",
       " ['<0x8D>', 144],\n",
       " ['<0x8E>', 145],\n",
       " ['<0x8F>', 146],\n",
       " ['<0x90>', 147],\n",
       " ['<0x91>', 148],\n",
       " ['<0x92>', 149],\n",
       " ['<0x93>', 150],\n",
       " ['<0x94>', 151],\n",
       " ['<0x95>', 152],\n",
       " ['<0x96>', 153],\n",
       " ['<0x97>', 154],\n",
       " ['<0x98>', 155],\n",
       " ['<0x99>', 156],\n",
       " ['<0x9A>', 157],\n",
       " ['<0x9B>', 158],\n",
       " ['<0x9C>', 159],\n",
       " ['<0x9D>', 160],\n",
       " ['<0x9E>', 161],\n",
       " ['<0x9F>', 162],\n",
       " ['<0xA0>', 163],\n",
       " ['<0xA1>', 164],\n",
       " ['<0xA2>', 165],\n",
       " ['<0xA3>', 166],\n",
       " ['<0xA4>', 167],\n",
       " ['<0xA5>', 168],\n",
       " ['<0xA6>', 169],\n",
       " ['<0xA7>', 170],\n",
       " ['<0xA8>', 171],\n",
       " ['<0xA9>', 172],\n",
       " ['<0xAA>', 173],\n",
       " ['<0xAB>', 174],\n",
       " ['<0xAC>', 175],\n",
       " ['<0xAD>', 176],\n",
       " ['<0xAE>', 177],\n",
       " ['<0xAF>', 178],\n",
       " ['<0xB0>', 179],\n",
       " ['<0xB1>', 180],\n",
       " ['<0xB2>', 181],\n",
       " ['<0xB3>', 182],\n",
       " ['<0xB4>', 183],\n",
       " ['<0xB5>', 184],\n",
       " ['<0xB6>', 185],\n",
       " ['<0xB7>', 186],\n",
       " ['<0xB8>', 187],\n",
       " ['<0xB9>', 188],\n",
       " ['<0xBA>', 189],\n",
       " ['<0xBB>', 190],\n",
       " ['<0xBC>', 191],\n",
       " ['<0xBD>', 192],\n",
       " ['<0xBE>', 193],\n",
       " ['<0xBF>', 194],\n",
       " ['<0xC0>', 195],\n",
       " ['<0xC1>', 196],\n",
       " ['<0xC2>', 197],\n",
       " ['<0xC3>', 198],\n",
       " ['<0xC4>', 199],\n",
       " ['<0xC5>', 200],\n",
       " ['<0xC6>', 201],\n",
       " ['<0xC7>', 202],\n",
       " ['<0xC8>', 203],\n",
       " ['<0xC9>', 204],\n",
       " ['<0xCA>', 205],\n",
       " ['<0xCB>', 206],\n",
       " ['<0xCC>', 207],\n",
       " ['<0xCD>', 208],\n",
       " ['<0xCE>', 209],\n",
       " ['<0xCF>', 210],\n",
       " ['<0xD0>', 211],\n",
       " ['<0xD1>', 212],\n",
       " ['<0xD2>', 213],\n",
       " ['<0xD3>', 214],\n",
       " ['<0xD4>', 215],\n",
       " ['<0xD5>', 216],\n",
       " ['<0xD6>', 217],\n",
       " ['<0xD7>', 218],\n",
       " ['<0xD8>', 219],\n",
       " ['<0xD9>', 220],\n",
       " ['<0xDA>', 221],\n",
       " ['<0xDB>', 222],\n",
       " ['<0xDC>', 223],\n",
       " ['<0xDD>', 224],\n",
       " ['<0xDE>', 225],\n",
       " ['<0xDF>', 226],\n",
       " ['<0xE0>', 227],\n",
       " ['<0xE1>', 228],\n",
       " ['<0xE2>', 229],\n",
       " ['<0xE3>', 230],\n",
       " ['<0xE4>', 231],\n",
       " ['<0xE5>', 232],\n",
       " ['<0xE6>', 233],\n",
       " ['<0xE7>', 234],\n",
       " ['<0xE8>', 235],\n",
       " ['<0xE9>', 236],\n",
       " ['<0xEA>', 237],\n",
       " ['<0xEB>', 238],\n",
       " ['<0xEC>', 239],\n",
       " ['<0xED>', 240],\n",
       " ['<0xEE>', 241],\n",
       " ['<0xEF>', 242],\n",
       " ['<0xF0>', 243],\n",
       " ['<0xF1>', 244],\n",
       " ['<0xF2>', 245],\n",
       " ['<0xF3>', 246],\n",
       " ['<0xF4>', 247],\n",
       " ['<0xF5>', 248],\n",
       " ['<0xF6>', 249],\n",
       " ['<0xF7>', 250],\n",
       " ['<0xF8>', 251],\n",
       " ['<0xF9>', 252],\n",
       " ['<0xFA>', 253],\n",
       " ['<0xFB>', 254],\n",
       " ['<0xFC>', 255],\n",
       " ['<0xFD>', 256],\n",
       " ['<0xFE>', 257],\n",
       " ['<0xFF>', 258],\n",
       " ['en', 259],\n",
       " ['â–t', 260],\n",
       " ['ce', 261],\n",
       " ['in', 262],\n",
       " ['ra', 263],\n",
       " ['â–a', 264],\n",
       " ['de', 265],\n",
       " ['er', 266],\n",
       " ['â–s', 267],\n",
       " ['ent', 268],\n",
       " ['or', 269],\n",
       " ['pr', 270],\n",
       " ['â–m', 271],\n",
       " ['â–u', 272],\n",
       " ['ing', 273],\n",
       " ['â–th', 274],\n",
       " ['ence', 275],\n",
       " ['entence', 276],\n",
       " ['Pi', 277],\n",
       " ['ed', 278],\n",
       " ['em', 279],\n",
       " ['ex', 280],\n",
       " ['is', 281],\n",
       " ['iz', 282],\n",
       " ['la', 283],\n",
       " ['on', 284],\n",
       " ['st', 285],\n",
       " ['â–S', 286],\n",
       " ['Pie', 287],\n",
       " ['end', 288],\n",
       " ['ext', 289],\n",
       " ['â–an', 290],\n",
       " ['â–pr', 291],\n",
       " ['â–to', 292],\n",
       " ['â–un', 293],\n",
       " ['â–the', 294],\n",
       " ['Piece', 295],\n",
       " ['â–Sentence', 296],\n",
       " ['â–SentencePiece', 297],\n",
       " ['.]', 298],\n",
       " ['Ne', 299],\n",
       " ['ag', 300],\n",
       " ['do', 301],\n",
       " ['ec', 302],\n",
       " ['gu', 303],\n",
       " ['ic', 304],\n",
       " ['ir', 305],\n",
       " ['it', 306],\n",
       " ['ly', 307],\n",
       " ['to', 308],\n",
       " ['â–(', 309],\n",
       " ['â–[', 310],\n",
       " ['â–f', 311],\n",
       " ['â–n', 312],\n",
       " ['â–w', 313],\n",
       " ['.])', 314],\n",
       " ['age', 315],\n",
       " ['del', 316],\n",
       " ['ion', 317],\n",
       " ['ken', 318],\n",
       " ['lan', 319],\n",
       " ['ral', 320],\n",
       " ['wor', 321],\n",
       " ['yst', 322],\n",
       " ['â–Ne', 323],\n",
       " ['â–al', 324],\n",
       " ['â–de', 325],\n",
       " ['â–is', 326],\n",
       " ['â–ma', 327],\n",
       " ['â–mo', 328],\n",
       " ['izer', 329],\n",
       " ['rain', 330],\n",
       " ['ural', 331],\n",
       " ['â–and', 332],\n",
       " ['â–lan', 333],\n",
       " ['â–pre', 334],\n",
       " ['guage', 335],\n",
       " ['ystem', 336],\n",
       " ['â–text', 337],\n",
       " ['â–model', 338],\n",
       " ['â–train', 339],\n",
       " ['kenizer', 340],\n",
       " ['â–system', 341],\n",
       " ['â–language', 342],\n",
       " ['â–training', 343],\n",
       " ['.,', 344],\n",
       " ['BP', 345],\n",
       " ['Ku', 346],\n",
       " ['ab', 347],\n",
       " ['as', 348],\n",
       " ['at', 349],\n",
       " ['by', 350],\n",
       " ['co', 351],\n",
       " ['es', 352],\n",
       " ['et', 353],\n",
       " ['if', 354],\n",
       " ['ig', 355],\n",
       " ['im', 356],\n",
       " ['ke', 357],\n",
       " ['lo', 358],\n",
       " ['nr', 359],\n",
       " ['oc', 360],\n",
       " ['e', 361],\n",
       " ['â–', 362],\n",
       " ['n', 363],\n",
       " ['t', 364],\n",
       " ['i', 365],\n",
       " ['r', 366],\n",
       " ['a', 367],\n",
       " ['o', 368],\n",
       " ['s', 369],\n",
       " ['d', 370],\n",
       " ['c', 371],\n",
       " ['l', 372],\n",
       " ['u', 373],\n",
       " ['g', 374],\n",
       " ['m', 375],\n",
       " ['p', 376],\n",
       " ['.', 377],\n",
       " ['h', 378],\n",
       " ['-', 379],\n",
       " ['w', 380],\n",
       " ['y', 381],\n",
       " ['P', 382],\n",
       " ['S', 383],\n",
       " ['b', 384],\n",
       " ['f', 385],\n",
       " ['k', 386],\n",
       " [')', 387],\n",
       " ['x', 388],\n",
       " ['z', 389],\n",
       " ['(', 390],\n",
       " ['N', 391],\n",
       " ['[', 392],\n",
       " [']', 393],\n",
       " ['v', 394],\n",
       " [',', 395],\n",
       " ['/', 396],\n",
       " ['B', 397],\n",
       " ['E', 398],\n",
       " ['K', 399]]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(\"video_9_dependencies/tok400.model\")\n",
    "vocab = [[sp.id_to_piece(idx), idx] for idx in range(sp.get_piece_size())]\n",
    "vocab\n",
    "\n",
    "#Will have 400 tokens, combination of special tokens, 256 byte tokens, merges, and the individual tokens/code point tokens (in order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[362, 378, 361, 372, 358, 362, 239, 152, 139, 238, 136, 152, 240, 152, 155, 239, 135, 187, 239, 157, 151]\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "ids = sp.encode(\"hello ì•ˆë…•í•˜ì„¸ìš”\")\n",
    "print(ids)\n",
    "print(len(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we look below, the Korean characters were not apart of the training set, so sentence piece is encountering code points it has not seen during training time and those code points do not have a token associated with them, so suddenly these are unknown tokens but because we set byte_fallback = True during training, instead sentencepiece fallsback to bytes and so encodes the Korean Chars to UTF-8 and then it uses the Byte Tokens to represent those Bytes. If we dont Byte Fallback the above output is instead soemthing like...\n",
    "\n",
    "[362, 378, 361, 372, 358, 362, 0]\n",
    "\n",
    "[â–', 'h', 'e', 'l', 'lo', 'â–', unk]\n",
    "\n",
    "If used with fallback in LLM, doesnt seem so good. Thus, using byte_fallback is a good idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['â–', 'h', 'e', 'l', 'lo', 'â–', '<0xEC>', '<0x95>', '<0x88>', '<0xEB>', '<0x85>', '<0x95>', '<0xED>', '<0x95>', '<0x98>', '<0xEC>', '<0x84>', '<0xB8>', '<0xEC>', '<0x9A>', '<0x94>']\n"
     ]
    }
   ],
   "source": [
    "print([sp.id_to_piece(idx) for idx in ids])\n",
    "#Tokenizer add_dummy_prefix=True such that a space \"_\" token is appended to the start of the sentence\n",
    "#This is so, for instance, world and Hello world would be treated the same. In Tiktoken \"world\" and \" world\" are\n",
    "#different tokens and the model needs to learn from data their similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Llama 2 tokenizer proto If you'd like to export the raw protocol buffer for the tokenizer.model released by meta, this is a helpful issue. And this is the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer_spec {\n",
    "  name: \"identity\"\n",
    "  precompiled_charsmap: \"\"\n",
    "  add_dummy_prefix: true\n",
    "  remove_extra_whitespaces: false\n",
    "  normalization_rule_tsv: \"\"\n",
    "}\n",
    "\n",
    "trainer_spec {\n",
    "  input: \"/large_experiments/theorem/datasets/MERGED/all.test1.merged\"\n",
    "  model_prefix: \"spm_model_32k_200M_charcov099995_allowWSO__v2\"\n",
    "  model_type: BPE\n",
    "  vocab_size: 32000\n",
    "  self_test_sample_size: 0\n",
    "  input_format: \"text\"\n",
    "  character_coverage: 0.99995\n",
    "  input_sentence_size: 200000000\n",
    "  seed_sentencepiece_size: 1000000\n",
    "  shrinking_factor: 0.75\n",
    "  num_threads: 80\n",
    "  num_sub_iterations: 2\n",
    "  max_sentence_length: 4192\n",
    "  shuffle_input_sentence: true\n",
    "  max_sentencepiece_length: 16\n",
    "  split_by_unicode_script: true\n",
    "  split_by_whitespace: true\n",
    "  split_by_number: true\n",
    "  treat_whitespace_as_suffix: false\n",
    "  split_digits: true\n",
    "  allow_whitespace_only_pieces: true\n",
    "  vocabulary_output_piece_score: true\n",
    "  hard_vocab_limit: true\n",
    "  use_all_vocab: false\n",
    "  byte_fallback: true\n",
    "  required_chars: \"\"\n",
    "  unk_id: 0\n",
    "  bos_id: 1\n",
    "  eos_id: 2\n",
    "  pad_id: -1\n",
    "  unk_surface: \" \\342\\201\\207 \"\n",
    "  unk_piece: \"<unk>\"\n",
    "  bos_piece: \"<s>\"\n",
    "  eos_piece: \"</s>\"\n",
    "  pad_piece: \"<pad>\"\n",
    "  train_extremely_large_corpus: false\n",
    "  enable_differential_privacy: false\n",
    "  differential_privacy_noise_level: 0.0\n",
    "  differential_privacy_clipping_threshold: 0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vocab_size\n",
    "- Q: what should be vocab_size\n",
    "- Q: how can I increase vocab_size\n",
    "- A: let's see. Reminder https://github.com/karpathy/ng-video-lecture/blob/master/gpt.py from before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember in the GPT from the last video, vocab_size in the model is needed in the token embedding table + output linear layer used to generate the LOGITS\n",
    "\n",
    "\n",
    "Why can't vocab size be infinite\n",
    "- embedding table will grow + linear layer will grow; requires more computation\n",
    "- will require more parameters, and could be concerned that will be undertraining these parameters. Ex. have 1 million tokens in vocab, then everyone of these tokens will come up more rarely in the training data. Will see fewer and fewer examples for each individual token, and will be worried that the vectors associated with each token will be undertrained as a result (as it doesnt come up too often and will not participate in forward/backward pass)\n",
    "- Also even though it is a nice propert that as vocab size increases, sequence length decreases, allowing the model to attend more and more text in its limited context length, but a potential problem is that too large of chunks are being squished into single tokens and so the model \"does not have enough time to think per some number of characters in the text\"\n",
    "\n",
    "Modern LLMS use high ten thousands to hundred thousands for vocab size\n",
    "\n",
    "\n",
    "As highlighted earlier, when finetuning GPT for chatGPT, additional tokens are added to maintain the metadata and stucture of conversation objects between user and assistant. To do so requires adding random init row to embedding table and extend the weights in output Linear Layer. What would be done is freeze the base model, introduce the new parameters, and only train these new parameters to introduce new tokens to architecture "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One interesting idea w.r.t to the design space of incorporating new tokens in the vocab:\n",
    "- Ex. Long Prompts (I.e You are an assistant ...) can use a lot of context length, so the authors introduce new tokens called \"GIST\" tokens to replace them via distillation. \n",
    "- One technique in parameter efficient fine-tuning, where most of the model is fixed, but in this instance no training of weights, no LORA (i.e no training of new non-embedding parameters), the only parameters trained in this instance are token embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Community has converged to the idea, in multi-modality, to tokenize alternative input domains (i.e images) and pretend it is just text tokens and use the Transformer architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going through common problems in LLMS, w.r.t to Tokenization and possible reasones\n",
    "- P: LLM cant spell,LLM cant reverse string A: Some tokens can represent a large amount of characters, i.e token 98518 \".DefaultCellStyle\"; likely too much crammed in single token; will be unable to spell DefaultCellStyle or reverse it. One solution Karapthy did was to split up \"DefaultCellStyle\" into individual chars then reverse/spell (as it now has multiple tokens in the sequence, each token representing one char)\n",
    "- P: Why is LLM poor at non-english A: LLM training + tokenizer training has limited amount of non-english data. Non-english sentences will expeirance substantial blow up w.r.t the amount of tokens needed to represent the sequence\n",
    "- P: Why is LLM bad at arithmetic A: Integers with multiple digits are arbitrarely tokenized\n",
    "- P: Why is GPT2 bad at programming A: Partly modelling and training data problem, but also tokenized in such a way that there is a large amount of white space in indents, where each white space is represented by a single token, bloating the sequence and reducing the context length the model can attend across\n",
    "- P: GPT abrupts when sees \"ENDOFTEXT\" A: Special token not accounted for as potential input by user; still treated as special token\n",
    "- P: Why does LLM have \"trailing whitespace\" issue in OpenAI playground A: If input is \"write tagline for song:\" it might output \" Its Bananas\" and it is important to recognize the token for 'Its' in GPT includes the white space prior \" Its\". However, if we use as input \"write tagline for song: \", we now have a trailing white space, which is represented by its own unique token, and now this is suddenely out of distribution for the model because the space should be apart of the next token but because we have explicitly put it and because the model has seen very little data of whitespace by itself bad things can happen\n",
    "- P: Why did \"SolidGoldMagikarp\" create problems A: When training the Tokenizer, likely on Reddit data\", the username appeared so often that it was given its own Token. However, when training the LLM, the token was never used, and so the embedding vector remained random init/untrained. When invoked during test time, when fed into Transformer, creates undefined behavior as model is out of distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_stack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

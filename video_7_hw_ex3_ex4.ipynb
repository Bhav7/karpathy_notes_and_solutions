{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " EX3: Find a dataset that is very large, so large that you can't see a gap between train and val loss. Pretrain the transformer on this data, then initialize with that model and finetune it on tiny shakespeare with a smaller number of steps and lower learning rate. Can you obtain a lower validation loss by the use of pretraining?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Global Variables \"\"\"\n",
    "block_size = 32\n",
    "batch_size = 16\n",
    "embed_dim = 64\n",
    "num_heads = 4\n",
    "num_blocks = 4\n",
    "lr = 1e-3\n",
    "epochs = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a baseline with just the tiny shakespeare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create Dataset \"\"\"\n",
    "with open('video_7_dependencies/input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] \n",
    "decode = lambda l: ''.join([itos[i] for i in l]) \n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create Model \"\"\"\n",
    "\n",
    "class MaskedMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, embed_dim, block_size):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.proj_dim = int(embed_dim/num_heads)\n",
    "        self.w_q = nn.Linear(embed_dim, num_heads*self.proj_dim, bias = False)\n",
    "        self.w_k = nn.Linear(embed_dim, num_heads*self.proj_dim, bias = False)\n",
    "        self.w_v = nn.Linear(embed_dim, num_heads*self.proj_dim, bias = False)\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size).to(device)))\n",
    "        self.dp = nn.Dropout(0.0)\n",
    "        self.out_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        B, T, C = input.shape #C == embed_dim\n",
    "        query = self.w_q(input).view(B, T, self.num_heads, self.proj_dim) #B, T, H, proj_dim\n",
    "        key = self.w_k(input).view(B, T, self.num_heads, self.proj_dim) #B, T, H proj_dim\n",
    "        wei = (query.permute(0,2,1,3) @ key.permute(0,2,3,1))*(C**-0.5) #B, H, T,T\n",
    "        wei = wei.masked_fill(self.tril[:T,:T] == 0, float(\"-inf\")) #Note: subset the mask self.tril[:T,:T] in case input sequence is less then block_size; the mask needs to be broadcastable with wei\n",
    "        wei = F.softmax(wei, dim = -1)\n",
    "\n",
    "        value = self.w_v(input).view(B, T, self.num_heads, self.proj_dim) #B, T, H, proj_dim\n",
    "        \n",
    "        out = wei @ value.permute(0,2,1,3) #B, H, T, proj_dim\n",
    "        out = out.permute(0,2,1,3).contiguous().view(B,T,C) #B, T, C\n",
    "        out = self.out_linear(out)  #B, T, C\n",
    "        return self.dp(out)\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.ffn = nn.Sequential(nn.Linear(embed_dim, embed_dim*4), nn.ReLU(), nn.Linear(embed_dim*4, embed_dim))\n",
    "        self.dp = nn.Dropout(0.0)\n",
    "\n",
    "    def forward(self, input):\n",
    "        out = self.ffn(input)\n",
    "        return self.dp(out)\n",
    "    \n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, num_heads, embed_dim, block_size):\n",
    "        super().__init__()\n",
    "        self.attention = MaskedMultiHeadAttention(num_heads, embed_dim, block_size)\n",
    "        self.ffn = FeedForward(embed_dim)\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.attention(self.ln1(input)) + input\n",
    "        out = self.ffn(self.ln2(x)) + x\n",
    "        return out\n",
    "    \n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, block_size, num_heads, num_blocks):\n",
    "        super().__init__()\n",
    "        self.content_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.position_embedding = nn.Embedding(block_size, embed_dim)\n",
    "        self.blocks = nn.ModuleList([Block(num_heads, embed_dim, block_size) for _ in range(num_blocks)])\n",
    "        self.output = nn.Linear(embed_dim, vocab_size)\n",
    "        self.dp = nn.Dropout(0.0)\n",
    "\n",
    "    def forward(self, input):\n",
    "        B, T = input.shape\n",
    "        con_embed = self.content_embedding(input) #B,T,embed_dim\n",
    "        pos_embed = self.position_embedding(torch.arange(T, device = device)) #1,T,embed_dim\n",
    "        x = con_embed + pos_embed #B,T,embed_dim\n",
    "        x = self.dp(x)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x) #B,T,embed_dim\n",
    "        out = self.output(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: When transferring models to device, any instances in which tensors were hard coded (i.e torch.ones or torch.arange), need to be explicitly moved to device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(vocab_size, embed_dim, block_size, num_heads, num_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (content_embedding): Embedding(65, 64)\n",
       "  (position_embedding): Embedding(32, 64)\n",
       "  (blocks): ModuleList(\n",
       "    (0-3): 4 x Block(\n",
       "      (attention): MaskedMultiHeadAttention(\n",
       "        (w_q): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (w_k): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (w_v): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (dp): Dropout(p=0.0, inplace=False)\n",
       "        (out_linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        )\n",
       "        (dp): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (output): Linear(in_features=64, out_features=65, bias=True)\n",
       "  (dp): Dropout(p=0.0, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(eval_iters):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            X,Y = X.to(device), Y.to(device)\n",
    "            logits  = model(X)\n",
    "            loss = F.cross_entropy(logits.permute(0, 2, 1), Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': tensor(4.6563), 'val': tensor(4.6507)}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate_loss(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Train Loss 4.3266777992248535, Val Loss 4.323242664337158\n",
      "Iteration 100, Train Loss 2.6200339794158936, Val Loss 2.6297202110290527\n",
      "Iteration 200, Train Loss 2.503884792327881, Val Loss 2.512178659439087\n",
      "Iteration 300, Train Loss 2.426879644393921, Val Loss 2.432345390319824\n",
      "Iteration 400, Train Loss 2.3615517616271973, Val Loss 2.3792293071746826\n",
      "Iteration 500, Train Loss 2.304750919342041, Val Loss 2.301269292831421\n",
      "Iteration 600, Train Loss 2.266355276107788, Val Loss 2.2765989303588867\n",
      "Iteration 700, Train Loss 2.215177536010742, Val Loss 2.2469358444213867\n",
      "Iteration 800, Train Loss 2.1655590534210205, Val Loss 2.2056145668029785\n",
      "Iteration 900, Train Loss 2.1372628211975098, Val Loss 2.173919200897217\n",
      "Iteration 1000, Train Loss 2.1057562828063965, Val Loss 2.143547296524048\n",
      "Iteration 1100, Train Loss 2.0912489891052246, Val Loss 2.1403238773345947\n",
      "Iteration 1200, Train Loss 2.054708242416382, Val Loss 2.092014789581299\n",
      "Iteration 1300, Train Loss 2.0232534408569336, Val Loss 2.0843966007232666\n",
      "Iteration 1400, Train Loss 2.004840612411499, Val Loss 2.077805757522583\n",
      "Iteration 1500, Train Loss 1.9745306968688965, Val Loss 2.058232307434082\n",
      "Iteration 1600, Train Loss 1.951646327972412, Val Loss 2.027053117752075\n",
      "Iteration 1700, Train Loss 1.945162296295166, Val Loss 2.0304512977600098\n",
      "Iteration 1800, Train Loss 1.927634596824646, Val Loss 2.0286142826080322\n",
      "Iteration 1900, Train Loss 1.9118690490722656, Val Loss 2.0068776607513428\n",
      "Iteration 2000, Train Loss 1.8962074518203735, Val Loss 1.9915744066238403\n",
      "Iteration 2100, Train Loss 1.8966320753097534, Val Loss 2.001162528991699\n",
      "Iteration 2200, Train Loss 1.8568578958511353, Val Loss 1.9585641622543335\n",
      "Iteration 2300, Train Loss 1.8482599258422852, Val Loss 1.9622279405593872\n",
      "Iteration 2400, Train Loss 1.8354301452636719, Val Loss 1.9538809061050415\n",
      "Iteration 2500, Train Loss 1.8210290670394897, Val Loss 1.9462682008743286\n",
      "Iteration 2600, Train Loss 1.816612958908081, Val Loss 1.9464324712753296\n",
      "Iteration 2700, Train Loss 1.801916480064392, Val Loss 1.938746690750122\n",
      "Iteration 2800, Train Loss 1.803281307220459, Val Loss 1.9353080987930298\n",
      "Iteration 2900, Train Loss 1.7861491441726685, Val Loss 1.930242896080017\n",
      "Iteration 3000, Train Loss 1.7935312986373901, Val Loss 1.9296929836273193\n",
      "Iteration 3100, Train Loss 1.7697409391403198, Val Loss 1.910300612449646\n",
      "Iteration 3200, Train Loss 1.7652194499969482, Val Loss 1.89664626121521\n",
      "Iteration 3300, Train Loss 1.7594245672225952, Val Loss 1.9093531370162964\n",
      "Iteration 3400, Train Loss 1.744694709777832, Val Loss 1.8898210525512695\n",
      "Iteration 3500, Train Loss 1.7337008714675903, Val Loss 1.8941911458969116\n",
      "Iteration 3600, Train Loss 1.7389644384384155, Val Loss 1.894300103187561\n",
      "Iteration 3700, Train Loss 1.7221519947052002, Val Loss 1.8844799995422363\n",
      "Iteration 3800, Train Loss 1.7289297580718994, Val Loss 1.8842031955718994\n",
      "Iteration 3900, Train Loss 1.7211201190948486, Val Loss 1.8630437850952148\n",
      "Iteration 4000, Train Loss 1.7166550159454346, Val Loss 1.8683912754058838\n",
      "Iteration 4100, Train Loss 1.7033108472824097, Val Loss 1.8797082901000977\n",
      "Iteration 4200, Train Loss 1.7066154479980469, Val Loss 1.871291160583496\n",
      "Iteration 4300, Train Loss 1.7097488641738892, Val Loss 1.878293752670288\n",
      "Iteration 4400, Train Loss 1.6996043920516968, Val Loss 1.8712875843048096\n",
      "Iteration 4500, Train Loss 1.6838829517364502, Val Loss 1.8623467683792114\n",
      "Iteration 4600, Train Loss 1.6751495599746704, Val Loss 1.848088026046753\n",
      "Iteration 4700, Train Loss 1.676404595375061, Val Loss 1.8407021760940552\n",
      "Iteration 4800, Train Loss 1.6792783737182617, Val Loss 1.849888563156128\n",
      "Iteration 4900, Train Loss 1.6741849184036255, Val Loss 1.8423229455947876\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "    x,y = get_batch('train')\n",
    "    x,y = x.to(device), y.to(device)\n",
    "    logits = model(x)\n",
    "    loss = F.cross_entropy(logits.permute(0, 2, 1), y)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (i % 100) == 0:\n",
    "        estimated_loss = estimate_loss(200)\n",
    "        print(f\"Iteration {i}, Train Loss {estimated_loss['train']}, Val Loss {estimated_loss['val']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Let's use the \"AG News Classification Dataset\" for pretraining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Create Pretrain Data, also since Vocab is differed, need to redefine the OG data\"\n",
    "import pandas as pd\n",
    "news = pd.read_csv(\"video_7_dependencies/news.csv\")\n",
    "pretrain_text = \" \".join([description for description in news.Description.values])\n",
    "vocab = sorted(list(set(list(set(pretrain_text)) + list(set(text)))))\n",
    "stoi = {token: idx for idx, token in enumerate(vocab)}\n",
    "itos = {idx: token for token, idx in stoi.items()}\n",
    "encode = lambda input_text: [stoi[i] for i in input_text]\n",
    "pretrain_data = torch.tensor(encode(pretrain_text)).to(torch.long)\n",
    "\n",
    "def get_pretrain_batch(split):\n",
    "    if split == \"train\":\n",
    "        idxs = torch.randint(0, int(len(pretrain_data)*0.9) - block_size, (batch_size,))\n",
    "    else:\n",
    "        idxs = torch.randint(int(len(pretrain_data)*0.9), len(pretrain_data) - block_size, (batch_size,))\n",
    "    X = []\n",
    "    Y = []\n",
    "    for idx in idxs:\n",
    "        X.append(pretrain_data[idx:idx+block_size].tolist())\n",
    "        Y.append(pretrain_data[idx+1:idx+block_size+1].tolist())\n",
    "    X = torch.tensor(X).to(torch.long)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(len(vocab), embed_dim, block_size, num_heads, num_blocks)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_pretrain_loss(eval_iters):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_pretrain_batch(split)\n",
    "            X,Y = X.to(device), Y.to(device)\n",
    "            logits  = model(X)\n",
    "            loss = F.cross_entropy(logits.permute(0, 2, 1), Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretrain for 5000 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Train Loss 4.5866522789001465, Val Loss 4.584536552429199\n",
      "Iteration 100, Train Loss 2.776937961578369, Val Loss 2.797126531600952\n",
      "Iteration 200, Train Loss 2.679657220840454, Val Loss 2.677330732345581\n",
      "Iteration 300, Train Loss 2.611804246902466, Val Loss 2.618142604827881\n",
      "Iteration 400, Train Loss 2.5495996475219727, Val Loss 2.5565340518951416\n",
      "Iteration 500, Train Loss 2.4924263954162598, Val Loss 2.4919979572296143\n",
      "Iteration 600, Train Loss 2.4207353591918945, Val Loss 2.435755729675293\n",
      "Iteration 700, Train Loss 2.3772237300872803, Val Loss 2.3657279014587402\n",
      "Iteration 800, Train Loss 2.3333449363708496, Val Loss 2.3267691135406494\n",
      "Iteration 900, Train Loss 2.288182258605957, Val Loss 2.2962470054626465\n",
      "Iteration 1000, Train Loss 2.259556531906128, Val Loss 2.2608225345611572\n",
      "Iteration 1100, Train Loss 2.222048282623291, Val Loss 2.230478286743164\n",
      "Iteration 1200, Train Loss 2.204315185546875, Val Loss 2.194192409515381\n",
      "Iteration 1300, Train Loss 2.1645803451538086, Val Loss 2.1730682849884033\n",
      "Iteration 1400, Train Loss 2.143474578857422, Val Loss 2.1536543369293213\n",
      "Iteration 1500, Train Loss 2.1365623474121094, Val Loss 2.125577926635742\n",
      "Iteration 1600, Train Loss 2.1102378368377686, Val Loss 2.113590717315674\n",
      "Iteration 1700, Train Loss 2.0731394290924072, Val Loss 2.0805938243865967\n",
      "Iteration 1800, Train Loss 2.061152935028076, Val Loss 2.0775487422943115\n",
      "Iteration 1900, Train Loss 2.044076681137085, Val Loss 2.0557358264923096\n",
      "Iteration 2000, Train Loss 2.028555393218994, Val Loss 2.033324956893921\n",
      "Iteration 2100, Train Loss 2.0195024013519287, Val Loss 2.0205283164978027\n",
      "Iteration 2200, Train Loss 2.001086950302124, Val Loss 1.9976129531860352\n",
      "Iteration 2300, Train Loss 1.9942407608032227, Val Loss 1.9786138534545898\n",
      "Iteration 2400, Train Loss 1.9786138534545898, Val Loss 1.9734283685684204\n",
      "Iteration 2500, Train Loss 1.9733065366744995, Val Loss 1.9725805521011353\n",
      "Iteration 2600, Train Loss 1.9697239398956299, Val Loss 1.949241280555725\n",
      "Iteration 2700, Train Loss 1.9506016969680786, Val Loss 1.9431513547897339\n",
      "Iteration 2800, Train Loss 1.945154070854187, Val Loss 1.9407879114151\n",
      "Iteration 2900, Train Loss 1.9396859407424927, Val Loss 1.928284764289856\n",
      "Iteration 3000, Train Loss 1.927122950553894, Val Loss 1.9201180934906006\n",
      "Iteration 3100, Train Loss 1.9101872444152832, Val Loss 1.896882176399231\n",
      "Iteration 3200, Train Loss 1.8828387260437012, Val Loss 1.8810621500015259\n",
      "Iteration 3300, Train Loss 1.8891545534133911, Val Loss 1.882156491279602\n",
      "Iteration 3400, Train Loss 1.8728524446487427, Val Loss 1.8725378513336182\n",
      "Iteration 3500, Train Loss 1.872752070426941, Val Loss 1.8861764669418335\n",
      "Iteration 3600, Train Loss 1.866506814956665, Val Loss 1.8566062450408936\n",
      "Iteration 3700, Train Loss 1.8703263998031616, Val Loss 1.8507801294326782\n",
      "Iteration 3800, Train Loss 1.8430653810501099, Val Loss 1.8375848531723022\n",
      "Iteration 3900, Train Loss 1.8487601280212402, Val Loss 1.8487190008163452\n",
      "Iteration 4000, Train Loss 1.8402999639511108, Val Loss 1.8370496034622192\n",
      "Iteration 4100, Train Loss 1.8339349031448364, Val Loss 1.8275448083877563\n",
      "Iteration 4200, Train Loss 1.8269821405410767, Val Loss 1.8354811668395996\n",
      "Iteration 4300, Train Loss 1.8164671659469604, Val Loss 1.8323429822921753\n",
      "Iteration 4400, Train Loss 1.803646206855774, Val Loss 1.8073036670684814\n",
      "Iteration 4500, Train Loss 1.8133872747421265, Val Loss 1.8096824884414673\n",
      "Iteration 4600, Train Loss 1.7952978610992432, Val Loss 1.7959572076797485\n",
      "Iteration 4700, Train Loss 1.792148470878601, Val Loss 1.79097318649292\n",
      "Iteration 4800, Train Loss 1.7957402467727661, Val Loss 1.800074577331543\n",
      "Iteration 4900, Train Loss 1.7977056503295898, Val Loss 1.779392123222351\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "    x,y = get_pretrain_batch('train')\n",
    "    x,y = x.to(device), y.to(device)\n",
    "    logits = model(x)\n",
    "    loss = F.cross_entropy(logits.permute(0, 2, 1), y)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (i % 100) == 0:\n",
    "        estimated_losses = estimate_pretrain_loss(200)\n",
    "        print(f\"Iteration {i}, Train Loss {estimated_losses['train']}, Val Loss {estimated_losses['val']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train it for 1000 epochs on the Shakespeare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr = lr/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(eval_iters):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            X,Y = X.to(device), Y.to(device)\n",
    "            logits  = model(X)\n",
    "            loss = F.cross_entropy(logits.permute(0, 2, 1), Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Train Loss 2.999953508377075, Val Loss 3.138110876083374\n",
      "Iteration 100, Train Loss 2.227320671081543, Val Loss 2.2652769088745117\n",
      "Iteration 200, Train Loss 2.0891001224517822, Val Loss 2.124817371368408\n",
      "Iteration 300, Train Loss 2.0378313064575195, Val Loss 2.060847759246826\n",
      "Iteration 400, Train Loss 1.9866249561309814, Val Loss 2.033018112182617\n",
      "Iteration 500, Train Loss 1.9645189046859741, Val Loss 2.0126116275787354\n",
      "Iteration 600, Train Loss 1.939063549041748, Val Loss 1.9961519241333008\n",
      "Iteration 700, Train Loss 1.9237667322158813, Val Loss 1.9802175760269165\n",
      "Iteration 800, Train Loss 1.9072626829147339, Val Loss 1.9655234813690186\n",
      "Iteration 900, Train Loss 1.8930515050888062, Val Loss 1.9440083503723145\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    x,y = get_batch('train')\n",
    "    x,y = x.to(device), y.to(device)\n",
    "    logits = model(x)\n",
    "    loss = F.cross_entropy(logits.permute(0, 2, 1), y)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (i % 100) == 0:\n",
    "        estimated_loss = estimate_loss(200)\n",
    "        print(f\"Iteration {i}, Train Loss {estimated_loss['train']}, Val Loss {estimated_loss['val']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After pretraining, takes much less iterations to converge to \"low loss\", took around twice as many without pretraining. The final loss was not lower, but this could be simply due not allowing for enough iterations either during pretraining or finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EX4: Read some transformer papers and implement one additional feature or change that people seem to use. Does it improve the performance of your GPT?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a recent family lightweight LLM's built by Google, called Gemma, https://arxiv.org/abs/2403.08295, the authors used multiple alterations, but three important ones was the use of\n",
    "- Multi-Query Attention (Use Same Keys and Values across Heads)\n",
    "- GeGLU Activations (Use GELU non-linearity + linear layer)\n",
    "- RMSNorm (Unlike LayerNorm, which subtracts mean and divides by STD for each element in a output vector Y, RMSNorm divides each element root mean squared of vector Y; reduces computational overhead of LayerNorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create Model \"\"\"\n",
    "\n",
    "class GeGLU_FeedForward(nn.Module):\n",
    "    \"\"\"FFN with GeGLU\"\"\"\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.gelu_ffn = nn.Sequential(nn.Linear(embed_dim, int(8/3)*embed_dim, bias = False), nn.GELU()) #OG paper has inner-layer projection of embed_dim*4, but the GeGELU paper reduces the projection by 2/3rd\n",
    "        self.glu_ffn = nn.Linear(embed_dim, int(8/3)*embed_dim, bias = False)\n",
    "        self.out_ffn = nn.Linear(int(8/3)*embed_dim, embed_dim,  bias = False)\n",
    "        self.dp = nn.Dropout(0.0)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x1 = self.gelu_ffn(input)\n",
    "        x2 = self.glu_ffn(input)\n",
    "        x = x1 * x2\n",
    "        out = self.out_ffn(x)\n",
    "        return self.dp(out)\n",
    "    \n",
    "\n",
    "class MaskedQueryHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, embed_dim, block_size):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.proj_dim = int(embed_dim/num_heads)\n",
    "        self.w_q = nn.Linear(embed_dim, num_heads*self.proj_dim, bias = False)\n",
    "        self.w_k = nn.Linear(embed_dim, self.proj_dim, bias = False)\n",
    "        self.w_v = nn.Linear(embed_dim, self.proj_dim, bias = False)\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size).to(device)))\n",
    "        self.dp = nn.Dropout(0.0)\n",
    "        self.out_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        B, T, C = input.shape #C == embed_dim\n",
    "        query = self.w_q(input).view(B, T, self.num_heads, self.proj_dim) #B, T, H, proj_dim\n",
    "        key = self.w_k(input).view(B, T, 1, self.proj_dim) #B, H, 1, proj_dim\n",
    "        wei = (query.permute(0,2,1,3) @ key.permute(0,2,3,1))*(C**-0.5) #B, H, T,T\n",
    "        wei = wei.masked_fill(self.tril[:T,:T] == 0, float(\"-inf\")) \n",
    "        wei = F.softmax(wei, dim = -1)\n",
    "\n",
    "        value = self.w_v(input).view(B, T, 1, self.proj_dim) #B, H, 1, proj_dim\n",
    "        \n",
    "        out = wei @ value.permute(0,2,1,3) #B, H, T, proj_dim\n",
    "        out = out.permute(0,2,1,3).contiguous().view(B,T,C) #B, T, C\n",
    "        out = self.out_linear(out)  #B, T, C\n",
    "        return self.dp(out)\n",
    "    \n",
    "\n",
    "class RMS_Norm(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.gain = torch.nn.Parameter(torch.ones(embed_dim))\n",
    "\n",
    "    def forward(self, input):\n",
    "        rms = (torch.mean(input**2, dim = -1, keepdim = True))**(-1/2) #B,T,1\n",
    "        out = input * rms #B,T,C\n",
    "        out = out * self.gain #B,T,C * C\n",
    "        return out\n",
    "    \n",
    "class Gemma_Block(nn.Module):\n",
    "    def __init__(self, num_heads, embed_dim, block_size):\n",
    "        super().__init__()\n",
    "        self.attention = MaskedQueryHeadAttention(num_heads, embed_dim, block_size)\n",
    "        self.ffn = GeGLU_FeedForward(embed_dim)\n",
    "        self.rn1 = RMS_Norm(embed_dim)\n",
    "        self.rn2 = RMS_Norm(embed_dim)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.attention(self.rn1(input)) + input\n",
    "        out = self.ffn(self.rn2(x)) + x\n",
    "        return out\n",
    "    \n",
    "class Modified_Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, block_size, num_heads, num_blocks):\n",
    "        super().__init__()\n",
    "        self.content_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.position_embedding = nn.Embedding(block_size, embed_dim)\n",
    "        self.blocks = nn.ModuleList([Gemma_Block(num_heads, embed_dim, block_size) for _ in range(num_blocks)])\n",
    "        self.output = nn.Linear(embed_dim, vocab_size)\n",
    "        self.dp = nn.Dropout(0.0)\n",
    "\n",
    "    def forward(self, input):\n",
    "        B, T = input.shape\n",
    "        con_embed = self.content_embedding(input) #B,T,embed_dim\n",
    "        pos_embed = self.position_embedding(torch.arange(T, device = device)) #1,T,embed_dim\n",
    "        x = con_embed + pos_embed #B,T,embed_dim\n",
    "        x = self.dp(x)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x) #B,T,embed_dim\n",
    "        out = self.output(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Modified_Transformer(len(vocab), embed_dim, block_size, num_heads, num_blocks)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(eval_iters):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            X,Y = X.to(device), Y.to(device)\n",
    "            logits  = model(X)\n",
    "            loss = F.cross_entropy(logits.permute(0, 2, 1), Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Train Loss 4.6836838722229, Val Loss 4.679835319519043\n",
      "Iteration 100, Train Loss 2.628729820251465, Val Loss 2.656503915786743\n",
      "Iteration 200, Train Loss 2.5119457244873047, Val Loss 2.5252857208251953\n",
      "Iteration 300, Train Loss 2.4421651363372803, Val Loss 2.4577879905700684\n",
      "Iteration 400, Train Loss 2.3663156032562256, Val Loss 2.3756864070892334\n",
      "Iteration 500, Train Loss 2.2952089309692383, Val Loss 2.320103645324707\n",
      "Iteration 600, Train Loss 2.2566213607788086, Val Loss 2.2725915908813477\n",
      "Iteration 700, Train Loss 2.211203098297119, Val Loss 2.2292425632476807\n",
      "Iteration 800, Train Loss 2.168210506439209, Val Loss 2.209113359451294\n",
      "Iteration 900, Train Loss 2.1260242462158203, Val Loss 2.155752182006836\n",
      "Iteration 1000, Train Loss 2.0863001346588135, Val Loss 2.1410882472991943\n",
      "Iteration 1100, Train Loss 2.0450620651245117, Val Loss 2.105604887008667\n",
      "Iteration 1200, Train Loss 2.0286126136779785, Val Loss 2.09820556640625\n",
      "Iteration 1300, Train Loss 1.9987938404083252, Val Loss 2.075296401977539\n",
      "Iteration 1400, Train Loss 1.9893118143081665, Val Loss 2.0529563426971436\n",
      "Iteration 1500, Train Loss 1.952102541923523, Val Loss 2.0353927612304688\n",
      "Iteration 1600, Train Loss 1.9293572902679443, Val Loss 2.0295228958129883\n",
      "Iteration 1700, Train Loss 1.916600227355957, Val Loss 2.0305449962615967\n",
      "Iteration 1800, Train Loss 1.8829134702682495, Val Loss 1.988792061805725\n",
      "Iteration 1900, Train Loss 1.8872168064117432, Val Loss 1.9826141595840454\n",
      "Iteration 2000, Train Loss 1.8659309148788452, Val Loss 1.977908730506897\n",
      "Iteration 2100, Train Loss 1.8467227220535278, Val Loss 1.952175498008728\n",
      "Iteration 2200, Train Loss 1.8294166326522827, Val Loss 1.9536995887756348\n",
      "Iteration 2300, Train Loss 1.8056637048721313, Val Loss 1.9327712059020996\n",
      "Iteration 2400, Train Loss 1.803898811340332, Val Loss 1.94584321975708\n",
      "Iteration 2500, Train Loss 1.805299997329712, Val Loss 1.9325419664382935\n",
      "Iteration 2600, Train Loss 1.7844603061676025, Val Loss 1.9260058403015137\n",
      "Iteration 2700, Train Loss 1.7841384410858154, Val Loss 1.9190667867660522\n",
      "Iteration 2800, Train Loss 1.7767105102539062, Val Loss 1.9170483350753784\n",
      "Iteration 2900, Train Loss 1.7648780345916748, Val Loss 1.9050387144088745\n",
      "Iteration 3000, Train Loss 1.7554272413253784, Val Loss 1.8981529474258423\n",
      "Iteration 3100, Train Loss 1.7534103393554688, Val Loss 1.9065568447113037\n",
      "Iteration 3200, Train Loss 1.7432217597961426, Val Loss 1.8843698501586914\n",
      "Iteration 3300, Train Loss 1.740536093711853, Val Loss 1.8823518753051758\n",
      "Iteration 3400, Train Loss 1.7180465459823608, Val Loss 1.882421851158142\n",
      "Iteration 3500, Train Loss 1.7121002674102783, Val Loss 1.882332444190979\n",
      "Iteration 3600, Train Loss 1.7166882753372192, Val Loss 1.8723154067993164\n",
      "Iteration 3700, Train Loss 1.6986004114151, Val Loss 1.865088939666748\n",
      "Iteration 3800, Train Loss 1.712802767753601, Val Loss 1.8552517890930176\n",
      "Iteration 3900, Train Loss 1.698279857635498, Val Loss 1.859214425086975\n",
      "Iteration 4000, Train Loss 1.690972924232483, Val Loss 1.8566217422485352\n",
      "Iteration 4100, Train Loss 1.693812608718872, Val Loss 1.8628511428833008\n",
      "Iteration 4200, Train Loss 1.6781560182571411, Val Loss 1.8410412073135376\n",
      "Iteration 4300, Train Loss 1.6782547235488892, Val Loss 1.8387564420700073\n",
      "Iteration 4400, Train Loss 1.6663938760757446, Val Loss 1.841577172279358\n",
      "Iteration 4500, Train Loss 1.6659743785858154, Val Loss 1.8220164775848389\n",
      "Iteration 4600, Train Loss 1.6652544736862183, Val Loss 1.8266701698303223\n",
      "Iteration 4700, Train Loss 1.6573659181594849, Val Loss 1.8384705781936646\n",
      "Iteration 4800, Train Loss 1.6544287204742432, Val Loss 1.821659803390503\n",
      "Iteration 4900, Train Loss 1.657965064048767, Val Loss 1.8446197509765625\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "    x,y = get_batch('train')\n",
    "    x,y = x.to(device), y.to(device)\n",
    "    logits = model(x)\n",
    "    loss = F.cross_entropy(logits.permute(0, 2, 1), y)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (i % 100) == 0:\n",
    "        estimated_loss = estimate_loss(200)\n",
    "        print(f\"Iteration {i}, Train Loss {estimated_loss['train']}, Val Loss {estimated_loss['val']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a much improved 1.82 Val Loss in Iteration 4800, which is much lower then any Loss generated in the prior applications of the Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_stack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

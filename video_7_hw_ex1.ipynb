{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EX1: The n-dimensional tensor mastery challenge: Combine the `Head` and `MultiHeadAttention` into one class that processes all the heads in parallel, treating the heads as another batch dimension (answer is in nanoGPT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Global Variables \"\"\"\n",
    "block_size = 32\n",
    "batch_size = 16\n",
    "embed_dim = 32\n",
    "num_heads = 4\n",
    "num_blocks = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create Dataset \"\"\"\n",
    "text = open(\"video_7_dependencies/input.txt\").read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(list(set(\"\".join(text))))\n",
    "print(len(vocab))\n",
    "stoi = {token: idx for idx, token in enumerate(vocab)}\n",
    "itos = {idx: token for token, idx in stoi.items()}\n",
    "dataset = [stoi[t] for t in \"\".join(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(batch_size, mode = \"train\"):\n",
    "    if mode == \"train\":\n",
    "        idxs = torch.randint(0, int(len(dataset)*0.9) - block_size, size = (batch_size, ))\n",
    "    else:\n",
    "        idxs = torch.randint(int(len(dataset)*0.9), len(dataset) - block_size, size = (batch_size, ))\n",
    "    X = []\n",
    "    Y = []\n",
    "    for idx in idxs:\n",
    "        X.append(dataset[idx:idx+block_size])\n",
    "        Y.append(dataset[idx + 1: idx + block_size + 1])\n",
    "    X = torch.tensor(X).to(torch.long)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, I will create a \"Transformer-like\" architecture which uses two implementations of multi-head attention (iterations vs vectorized approach) and determine whether the outputs are the same. To do so, I fix the weights to the same set of values for the two implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create Model \"\"\"\n",
    "\n",
    "class MaskedSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, proj_dim, block_size):\n",
    "        super().__init__()\n",
    "        self.w_q = nn.Linear(embed_dim, proj_dim, bias = False)\n",
    "        self.w_k = nn.Linear(embed_dim, proj_dim, bias = False)\n",
    "        self.w_v = nn.Linear(embed_dim, proj_dim, bias = False)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.w_q.weight = nn.Parameter(torch.arange(0, embed_dim*proj_dim).view(proj_dim, embed_dim).to(torch.float32))\n",
    "            self.w_k.weight = nn.Parameter(torch.arange(0, embed_dim*proj_dim).view(proj_dim, embed_dim).to(torch.float32))\n",
    "            self.w_v.weight = nn.Parameter(torch.arange(0, embed_dim*proj_dim).view(proj_dim, embed_dim).to(torch.float32))\n",
    "            \n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
    "        \n",
    "    def forward(self, input):\n",
    "        B, T, C = input.shape\n",
    "        query = self.w_q(input) #B,T,proj_dim\n",
    "        key = self.w_k(input) #B,T,proj_dim\n",
    "        wei = (query @ key.permute(0,2,1))*(C**-0.5) #B,T,T\n",
    "        wei = wei.masked_fill(self.tril[:T,:T] == 0, float(\"-inf\")) #Note: subset the mask self.tril[:T,:T] in case input sequence is less then block_size; the mask needs to be broadcastable with wei\n",
    "        wei = F.softmax(wei, dim = -1)\n",
    "\n",
    "        value = self.w_v(input) #B,T,proj_dim\n",
    "        out = wei @ value\n",
    "        \n",
    "        return out\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, embed_dim, block_size):\n",
    "        super().__init__()\n",
    "        self.multiple_attention = nn.ModuleList([MaskedSelfAttention(embed_dim, int(embed_dim/num_heads), block_size) for i in range(num_heads)])\n",
    "    def forward(self, input):\n",
    "        out = torch.cat([attn(input) for attn in self.multiple_attention], dim = -1) #B,T, embed_dim\n",
    "        return out\n",
    "    \n",
    "\n",
    "class MaskedMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, embed_dim, block_size):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.proj_dim = int(embed_dim/num_heads)\n",
    "        self.w_q = nn.Linear(embed_dim, num_heads*self.proj_dim, bias = False)\n",
    "        self.w_k = nn.Linear(embed_dim, num_heads*self.proj_dim, bias = False)\n",
    "        self.w_v = nn.Linear(embed_dim, num_heads*self.proj_dim, bias = False)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.w_q.weight = nn.Parameter(torch.arange(0, embed_dim*self.proj_dim).repeat(num_heads).view(num_heads*self.proj_dim, embed_dim).to(torch.float32))\n",
    "            self.w_k.weight = nn.Parameter(torch.arange(0, embed_dim*self.proj_dim).repeat(num_heads).view(num_heads*self.proj_dim, embed_dim).to(torch.float32))\n",
    "            self.w_v.weight = nn.Parameter(torch.arange(0, embed_dim*self.proj_dim).repeat(num_heads).view(num_heads*self.proj_dim, embed_dim).to(torch.float32))\n",
    "            \n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
    "        \n",
    "    def forward(self, input):\n",
    "        B, T, C = input.shape #C == embed_dim\n",
    "        query = self.w_q(input).view(B, T, self.num_heads, self.proj_dim) #B, T, H, proj_dim\n",
    "        key = self.w_k(input).view(B, T, self.num_heads, self.proj_dim) #B, T, H, proj_dim\n",
    "        wei = (query.permute(0,2,1,3) @ key.permute(0,2,3,1))*(C**-0.5) #B, H, T, T\n",
    "        wei = wei.masked_fill(self.tril[:T,:T] == 0, float(\"-inf\")) \n",
    "        wei = F.softmax(wei, dim = -1)\n",
    "\n",
    "\n",
    "        value = self.w_v(input).view(B, T, self.num_heads, self.proj_dim) #B, T, H, proj_dim\n",
    "        out = wei @ value.permute(0,2,1,3) #B, H, T, proj_dim\n",
    "        out = out.permute(0,2,1,3).contiguous().view(B,T,C) #B, T, C\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, block_size, num_heads):\n",
    "        super().__init__()\n",
    "        self.content_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.position_embedding = nn.Embedding(block_size, embed_dim)\n",
    "        self.iterable_attention = MultiHeadAttention(num_heads, embed_dim, block_size)\n",
    "        self.tensor_attention = MaskedMultiHeadAttention(num_heads, embed_dim, block_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        B, T = input.shape\n",
    "        con_embed = self.content_embedding(input) #B,T,embed_dim\n",
    "        pos_embed = self.position_embedding(torch.arange(T)) #1,T,embed_dim\n",
    "        x = con_embed + pos_embed #B,T,embed_dim\n",
    "        x1 = self.iterable_attention(x)\n",
    "        x2 = self.tensor_attention(x)\n",
    "        return [x1, x2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(len(vocab), embed_dim, block_size, num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = create_batches(batch_size)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 32, 32])\n",
      "tensor(-16841928., grad_fn=<SumBackward0>)\n",
      "torch.Size([16, 32, 32])\n",
      "tensor(-16841928., grad_fn=<SumBackward0>)\n",
      "tensor([[[True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         ...,\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True]],\n",
      "\n",
      "        [[True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         ...,\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True]],\n",
      "\n",
      "        [[True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         ...,\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         ...,\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True]],\n",
      "\n",
      "        [[True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         ...,\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True]],\n",
      "\n",
      "        [[True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         ...,\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True]]])\n"
     ]
    }
   ],
   "source": [
    "iterable_approach, tensor_approach = model(x)\n",
    "print(iterable_approach.shape)\n",
    "print(iterable_approach.sum())\n",
    "\n",
    "print(tensor_approach.shape)\n",
    "print(tensor_approach.sum())\n",
    "\n",
    "print(iterable_approach == tensor_approach)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEED TO BE CAREFUL WITH VIEW: was creating a bug where going from B,T,num_heads x n_proj -> B,num_heads,T,n_proj was not generating an equivalent result to the iterable variant of MHSA. Instead first veiwing it to\n",
    "B,T,num_heads,n_proj and then permuting it resolved the issues\n",
    "\n",
    "Nice thread which higlights problems with using view for dimension swapping: https://discuss.pytorch.org/t/for-beginners-do-not-use-view-or-reshape-to-swap-dimensions-of-tensors/75524\n",
    "\n",
    "\n",
    "Same issues when going from B,num_heads,T,n_proj -> B,T,num_heads x n_proj when generating the output of MHSA, need to first permute to  B,T,num_heads,n_proj then convert to contigous tensor (needed to \"view\" a permuted tensor from more to less dims; note this creates a copy of the non-continguous tensor as a contigious tensor), then view to B,T,num_heads x n_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_stack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

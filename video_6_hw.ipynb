{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Musing 1: Create Conv1D Layer and test its equivalency to torch implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 10])\n"
     ]
    }
   ],
   "source": [
    "#Creating example input\n",
    "in_channels = 3\n",
    "sample_input = torch.randn((32, in_channels, 10))\n",
    "print(sample_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters used to compare torch and custom implementation\n",
    "out_channels = 6\n",
    "kernel_size = 3\n",
    "stride = 4\n",
    "padding = 1\n",
    "dilation = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 2])\n",
      "tensor(4.0717, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#For the purposes of this implementation, ignoring: groups > 2 and padding_mode != zeros\n",
    "torch_implementation = nn.Conv1d(in_channels, out_channels, kernel_size, stride = stride, padding = padding, dilation = dilation, bias = True)\n",
    "print(torch_implementation(sample_input).shape)\n",
    "print(torch_implementation(sample_input).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "print(torch_implementation.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([-0.0237,  0.0176,  0.0527, -0.0591, -0.0552,  0.3033],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(torch_implementation.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Conv1d:\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, dilation):\n",
    "        self.filters = torch.randn((out_channels, in_channels, kernel_size))\n",
    "        self.biases = torch.randn((out_channels, 1))\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size #size of filter\n",
    "        self.stride = stride #amount filter slides\n",
    "        self.padding = padding \n",
    "        self.dilation = dilation #space between individual weights in filter\n",
    "\n",
    "    def __call__(self, input):\n",
    "        #Output Length formula from PyTorch documentation\n",
    "        output_length = int((((input.shape[-1]) + (2*self.padding) - self.dilation * (self.kernel_size - 1) - 1)/self.stride) + 1)\n",
    "        output = torch.zeros((input.shape[0], self.out_channels, output_length)) #N, OC, L\n",
    "        if self.padding != 0:\n",
    "            #Will add the desired amount of zeros to both sides (didnt implement one-side padding)\n",
    "            input = torch.cat([torch.zeros((input.shape[0], self.in_channels, self.padding)), input, torch.zeros((input.shape[0], self.in_channels, self.padding))], dim = 2)\n",
    "        \n",
    "        if self.dilation != 1:\n",
    "            #Dilation = N, adds N zeros between all weights, but first and last weight do not get appending or trailing zero\n",
    "            dilation_kernel_size = self.kernel_size + (self.dilation-1)*(self.kernel_size-1)\n",
    "            dilation_filters = torch.zeros(self.out_channels, self.in_channels, dilation_kernel_size)\n",
    "            dilation_filters[:, :, torch.arange(0, dilation_kernel_size, self.dilation)] = self.filters\n",
    "\n",
    "        for out_channel in range(self.out_channels):\n",
    "            for in_channel in range(self.in_channels):\n",
    "                #out_idx corresponds to output matrix, and in_idx corresponds to input matrix\n",
    "                for out_idx, in_idx in enumerate(range(0, input.shape[-1], self.stride)):\n",
    "                    if out_idx == output_length: #Terminates instances in which filter only partially covers input\n",
    "                        break\n",
    "                    if self.dilation !=1:\n",
    "                        output[:, out_channel, out_idx] += (input[:, in_channel, in_idx:in_idx+(dilation_kernel_size)]*dilation_filters[out_channel, in_channel, :]).sum(1)\n",
    "                    else:\n",
    "                        # += is needed since for each output channel, need to do element wise mul and sum using a unique filter for each input channel\n",
    "                        #                                                 N, 1, kernel_size                            1, 1, kernel_size\n",
    "                        output[:, out_channel, out_idx] += (input[:, in_channel, in_idx:in_idx+self.kernel_size]*self.filters[out_channel, in_channel, :]).sum(1)\n",
    "        output += self.biases\n",
    "        return output\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.filters, self.biases]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "my_implementation = My_Conv1d(in_channels, out_channels, kernel_size, stride = stride, padding = padding, dilation = dilation)\n",
    "print(my_implementation(sample_input).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7867)\n"
     ]
    }
   ],
   "source": [
    "print(my_implementation(sample_input).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(False)\n"
     ]
    }
   ],
   "source": [
    "print(my_implementation(sample_input).sum() == torch_implementation(sample_input).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sv/0hsl00jn4glf08wd477vxd900000gn/T/ipykernel_63200/128250507.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  my_implementation.filters = torch.tensor(list(torch_implementation.parameters())[0])\n",
      "/var/folders/sv/0hsl00jn4glf08wd477vxd900000gn/T/ipykernel_63200/128250507.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  my_implementation.biases = torch.tensor(list(torch_implementation.parameters())[1]).view(-1,1)\n"
     ]
    }
   ],
   "source": [
    "#To check if operations are equivlanet assign the params used in the torch implementation to custom implementation\n",
    "#then check if sums of outputs match\n",
    "\n",
    "my_implementation.filters = torch.tensor(list(torch_implementation.parameters())[0])\n",
    "my_implementation.biases = torch.tensor(list(torch_implementation.parameters())[1]).view(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "print(my_implementation(sample_input).sum() == torch_implementation(sample_input).sum())\n",
    "\n",
    "#Thus my_implementation is equivlant to PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interesting idea is that, in the context of autoregressive character-level models, convolutions allow for more efficient training of linear layers relative to traditional MLPs, as do not need to break single sentences into multiple training examples (instead can train all at once. For more information:  https://www.kilians.net/post/convolution-in-autoregressive-neural-networks/\n",
    "\n",
    "\n",
    "Musing 2: Exploit Normal Conv Layers to train 8-Context autoregressive language model and show it is equivalent to MLP generated in the lecture. Also try out dilated Conv layers which progressively fuse the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Dataset\n",
    "words = open(\"video_2_dependencies/names.txt\").read().splitlines()\n",
    "chars = [\".\"] + sorted(list(set(\"\".join(words)))) + [\";\"]\n",
    "#Notice, I added \";\" as a potential char, it will be used as a STOP token, further explanation in create_datasets line\n",
    "stoi = {char:idx for idx, char in enumerate(chars)}\n",
    "itos = {idx: char for char, idx in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033\n"
     ]
    }
   ],
   "source": [
    "print(len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the dataset for conv layers such that it can be trained more efficiently, the batched input sentences must be of equivalent size. However, this is not the case, as sentences differ in size. To remedy this, the function below indentifies the sentence of longest length and pads the remaining sentences to be\n",
    "of equal length with STOP \";\" token. I opted to add this new token instead of using \".\" token, such that model learns the presence of \".\" means to predict an actual letter and \";\" means to predict no more letters and instead just continue to predict more \";\" stop tokens. If I used only \".\" for both start and stop, it risks the model predicting letters after the previous output predicted stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(words, block_size = 8, max_len = 0):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for word in words:\n",
    "        context = [0]*block_size\n",
    "        for ch in word:\n",
    "            context.append(stoi[ch])\n",
    "        X.append(context)\n",
    "        Y.append(0)\n",
    "        if len(context) > max_len:\n",
    "            max_len = len(context)\n",
    "\n",
    "    for idx, x in tqdm(enumerate(X)):\n",
    "        if len(x) == max_len:\n",
    "            print(idx)\n",
    "        X[idx] = x + [27]*(max_len - len(x))\n",
    "        Y[idx] = X[idx][block_size:] + [27]\n",
    "    \n",
    "    print(\"Creating Tensors\")\n",
    "    X = torch.tensor(X).view(len(words), max_len)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25626it [00:00, 969286.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17026\n",
      "23789\n",
      "Creating Tensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3203it [00:00, 1072260.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Tensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3204it [00:00, 1109065.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Tensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n1 = int(len(words)*0.8)\n",
    "n2 = int(len(words)*0.9)\n",
    "\n",
    "X_tr, Y_tr = create_dataset(words[:n1])\n",
    "X_val, Y_val = create_dataset(words[n1:n2], 8, X_tr.shape[-1])\n",
    "X_te, Y_te = create_dataset(words[n2:], 8, X_tr.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25626, 23]) torch.Size([3203, 23]) torch.Size([3204, 23])\n"
     ]
    }
   ],
   "source": [
    "print(X_tr.shape, X_val.shape, X_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  ..., 27, 27, 27],\n",
       "        [ 0,  0,  0,  ..., 27, 27, 27],\n",
       "        [ 0,  0,  0,  ..., 27, 27, 27],\n",
       "        ...,\n",
       "        [ 0,  0,  0,  ..., 27, 27, 27],\n",
       "        [ 0,  0,  0,  ..., 27, 27, 27],\n",
       "        [ 0,  0,  0,  ..., 27, 27, 27]])"
      ]
     },
     "execution_count": 531,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0,  0,  0,  0,  0,  0,  0,  0, 13, 21,  8,  1, 13, 13,  1,  4, 13, 21,\n",
       "         19, 20,  1,  6,  1]),\n",
       " tensor([13, 21,  8,  1, 13, 13,  1,  4, 13, 21, 19, 20,  1,  6,  1, 27]))"
      ]
     },
     "execution_count": 532,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr[17026], Y_tr[17026]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25626, 16]) torch.Size([3203, 16]) torch.Size([3204, 16])\n"
     ]
    }
   ],
   "source": [
    "print(Y_tr.shape, Y_val.shape, Y_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([25, 21,  8,  5, 14,  7, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27])"
      ]
     },
     "execution_count": 534,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_tr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, 10)\n",
    "        #Will have output of N, vocab_size, 16 (16 determined by using max_len = 23 and blcok_size = 8; euqal to Y target size)\n",
    "        #Treat as a multi-class multi-output problem\n",
    "        #Will have vocab_size output channel, where we can softmax over to determine highest prob next char and compute loss\n",
    "        #Since output will have 16 multi-class outputs simultaneously, it is a multi-output problem\n",
    "\n",
    "        #Need stride = 10, since each letters embedding is 10-d, and want filters to slide over indiv. chars only\n",
    "        self.convolution = nn.Conv1d(1, vocab_size, 10*block_size, 10)\n",
    "\n",
    "    def __call__(self, input):\n",
    "        x = self.embedding(input)\n",
    "        #Will create N, 1, 80 (block_size * embedding -> 8*10) input\n",
    "        x = x.view(input.shape[0], 1, -1)\n",
    "        output = self.convolution(x)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = CNN(len(chars), 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 308/20000 [00:00<00:12, 1546.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.4435, grad_fn=<NllLoss2DBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 1275/20000 [00:00<00:11, 1596.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1061, grad_fn=<NllLoss2DBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 2230/20000 [00:01<00:11, 1573.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1053, grad_fn=<NllLoss2DBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 3177/20000 [00:02<00:10, 1564.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0793, grad_fn=<NllLoss2DBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 4283/20000 [00:02<00:10, 1530.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0954, grad_fn=<NllLoss2DBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 5213/20000 [00:03<00:09, 1535.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0500, grad_fn=<NllLoss2DBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 6301/20000 [00:04<00:08, 1540.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9798, grad_fn=<NllLoss2DBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 7238/20000 [00:04<00:08, 1539.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9851, grad_fn=<NllLoss2DBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 8169/20000 [00:05<00:07, 1543.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0715, grad_fn=<NllLoss2DBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 9246/20000 [00:05<00:07, 1508.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0896, grad_fn=<NllLoss2DBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 10166/20000 [00:06<00:06, 1517.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0234, grad_fn=<NllLoss2DBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 11244/20000 [00:07<00:05, 1519.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9675, grad_fn=<NllLoss2DBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 12165/20000 [00:07<00:05, 1528.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0069, grad_fn=<NllLoss2DBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 13233/20000 [00:08<00:04, 1446.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0089, grad_fn=<NllLoss2DBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 14152/20000 [00:09<00:04, 1409.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0958, grad_fn=<NllLoss2DBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 15217/20000 [00:09<00:03, 1470.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0172, grad_fn=<NllLoss2DBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████▏ | 16285/20000 [00:10<00:02, 1521.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9229, grad_fn=<NllLoss2DBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 17192/20000 [00:11<00:02, 1397.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0363, grad_fn=<NllLoss2DBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 18260/20000 [00:12<00:01, 1509.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0427, grad_fn=<NllLoss2DBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 19168/20000 [00:12<00:00, 1505.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9861, grad_fn=<NllLoss2DBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:13<00:00, 1518.98it/s]\n"
     ]
    }
   ],
   "source": [
    "iterations = 20000\n",
    "optimizer = torch.optim.SGD(cnn_model.parameters(), lr = 0.1)\n",
    "for i in tqdm(range(iterations)):\n",
    "    batch_idx = torch.randint(0, len(X_tr), (32,))\n",
    "    logits = cnn_model(X_tr[batch_idx])\n",
    "    loss = F.cross_entropy(logits, Y_tr[batch_idx])\n",
    "    if i == 15000:\n",
    "        optimizer = torch.optim.SGD(cnn_model.parameters(), lr = 0.01)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    if i % 1000 == 0:\n",
    "        print(loss)\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0138)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    train_logits = cnn_model(X_tr)\n",
    "    train_loss = F.cross_entropy(train_logits, Y_tr)\n",
    "    print(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0075)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    val_logits = cnn_model(X_val)\n",
    "    val_loss = F.cross_entropy(val_logits, Y_val)\n",
    "    print(val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice the loss are much smaller then the lexture loss, this is because through padding the inputs, some portions of the sentence are \"a ; ; ; ; ; ;\" in which the model probably easily learned to predict \";\". inflating perceived performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting datasets used in Conv to function in normal MLP\n",
    "\n",
    "def create_mlp_datasets(dataset_X, dataset_Y, block_size = 8):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for instance_x, instance_y in zip(dataset_X, dataset_Y):\n",
    "        instance_x = instance_x.view(-1).tolist()\n",
    "        for chr_idx in range(0, len(instance_x) - block_size + 1):\n",
    "            X.append(instance_x[chr_idx:chr_idx+block_size])\n",
    "            Y.append(instance_y[chr_idx])\n",
    "    return torch.tensor(X), torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_X_tr, mlp_Y_tr = create_mlp_datasets(X_tr, Y_tr, block_size = 8)\n",
    "mlp_X_val, mlp_Y_val = create_mlp_datasets(X_val, Y_val, block_size = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([410016, 8])\n"
     ]
    }
   ],
   "source": [
    "print(mlp_X_tr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0, 25],\n",
       "        [ 0,  0,  0,  0,  0,  0, 25, 21],\n",
       "        [ 0,  0,  0,  0,  0, 25, 21,  8],\n",
       "        [ 0,  0,  0,  0, 25, 21,  8,  5],\n",
       "        [ 0,  0,  0, 25, 21,  8,  5, 14],\n",
       "        [ 0,  0, 25, 21,  8,  5, 14,  7],\n",
       "        [ 0, 25, 21,  8,  5, 14,  7, 27],\n",
       "        [25, 21,  8,  5, 14,  7, 27, 27],\n",
       "        [21,  8,  5, 14,  7, 27, 27, 27]])"
      ]
     },
     "execution_count": 578,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_X_tr[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([25, 21,  8,  ..., 27, 27, 27])"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_Y_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, 10)\n",
    "        self.linear = nn.Linear(block_size*10, vocab_size)\n",
    "    \n",
    "    def __call__(self, input):\n",
    "        x = self.embedding(input)\n",
    "        x = x.view(input.shape[0], -1)\n",
    "        output = self.linear(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(len(chars), 8 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing how trained parameters used in Conv can be used in simple MLP and achieve the same performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.embedding.weight.data = list(cnn_model.embedding.parameters())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.linear.weight.data = list(cnn_model.convolution.parameters())[0].reshape(len(chars), 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.linear.bias.data = list(cnn_model.convolution.parameters())[1].reshape(28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0138)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    train_logits = model(mlp_X_tr)\n",
    "    train_loss = F.cross_entropy(train_logits, mlp_Y_tr)\n",
    "    print(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0075)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    train_logits = model(mlp_X_val)\n",
    "    train_loss = F.cross_entropy(train_logits, mlp_Y_val)\n",
    "    print(train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the exact same train and val performance, proving in autoregressive setting, can make CONV == MLP\n",
    "\n",
    "Now lets used a dilated CNN to progressively fuse the inputs (similiar to lecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dilated_CNN(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, 10)\n",
    "        self.convolution = nn.Sequential(\n",
    "        nn.Conv1d(1, 100, 10*2, stride = 10), nn.Tanh(),  #Embeddings are 10-dim per char, so filter of 20 looks at 2 chars, we use stride of 10 to slide over each char (which is needed for the parallel multi-output training for each time step)\n",
    "        nn.Conv1d(100, 100, 2, stride = 1, dilation = 2), nn.Tanh(), #Each bigram char has length 1 and channel dim 100, a filter of 2 would consider a 4-gram. We use a dilation factor of 2, such that two 2-grams composed of 4 different characters are considered. If no dilation, would consider a bigram like abcd: \"ab bc\" but since dilation = 2 ensures the filter weight skips 1 time step, uses \"ab cd\" instead. We need stride to = 1 for a similiar reason as above\n",
    "        nn.Conv1d(100, vocab_size, 2, stride = 1, dilation = 4), #Each fourgram has length 1 and channel dim of 100, a filter of 2 would consider a 8-gram. Use dilation = 4, such that two fourgrams composed of 8 different chars/positions are considered\n",
    "        )\n",
    "\n",
    "    def __call__(self, input):\n",
    "        x = self.embedding(input)\n",
    "        x = x.view(input.shape[0], 1, -1)\n",
    "        output = self.convolution(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {},
   "outputs": [],
   "source": [
    "dilated_cnn_model = dilated_CNN(len(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 28, 16])"
      ]
     },
     "execution_count": 726,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dilated_cnn_model(X_tr[:10]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 12/20000 [00:00<05:49, 57.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.2740, grad_fn=<NllLoss2DBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1010/20000 [00:20<05:38, 56.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0191, grad_fn=<NllLoss2DBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2011/20000 [00:38<05:51, 51.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9787, grad_fn=<NllLoss2DBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3012/20000 [00:57<05:10, 54.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0505, grad_fn=<NllLoss2DBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4008/20000 [01:16<04:47, 55.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9523, grad_fn=<NllLoss2DBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5011/20000 [01:34<04:38, 53.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9466, grad_fn=<NllLoss2DBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6011/20000 [01:53<04:12, 55.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0167, grad_fn=<NllLoss2DBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7010/20000 [02:11<04:18, 50.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0019, grad_fn=<NllLoss2DBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8010/20000 [02:30<03:27, 57.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9970, grad_fn=<NllLoss2DBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9008/20000 [02:47<03:10, 57.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0279, grad_fn=<NllLoss2DBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10007/20000 [03:05<02:52, 57.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9725, grad_fn=<NllLoss2DBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11012/20000 [03:23<02:32, 58.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9704, grad_fn=<NllLoss2DBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12016/20000 [03:40<01:33, 85.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9152, grad_fn=<NllLoss2DBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13013/20000 [03:52<01:30, 77.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0233, grad_fn=<NllLoss2DBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14010/20000 [04:05<01:17, 77.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0073, grad_fn=<NllLoss2DBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15016/20000 [04:18<01:04, 77.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9321, grad_fn=<NllLoss2DBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16017/20000 [04:31<00:48, 82.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9201, grad_fn=<NllLoss2DBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17009/20000 [04:43<00:36, 82.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8832, grad_fn=<NllLoss2DBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18017/20000 [04:55<00:24, 81.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8821, grad_fn=<NllLoss2DBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19012/20000 [05:08<00:12, 79.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9268, grad_fn=<NllLoss2DBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [05:21<00:00, 62.25it/s]\n"
     ]
    }
   ],
   "source": [
    "iterations = 20000\n",
    "optimizer = torch.optim.SGD(dilated_cnn_model.parameters(), lr = 0.1)\n",
    "for i in tqdm(range(iterations)):\n",
    "    batch_idx = torch.randint(0, len(X_tr), (32,))\n",
    "    logits = dilated_cnn_model(X_tr[batch_idx])\n",
    "    loss = F.cross_entropy(logits, Y_tr[batch_idx])\n",
    "    if i == 15000:\n",
    "        optimizer = torch.optim.SGD(dilated_cnn_model.parameters(), lr = 0.01)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    if i % 1000 == 0:\n",
    "        print(loss)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9333)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    train_logits = dilated_cnn_model(X_tr)\n",
    "    train_loss = F.cross_entropy(train_logits, Y_tr)\n",
    "    print(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9351)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    val_logits = dilated_cnn_model(X_val)\n",
    "    val_loss = F.cross_entropy(val_logits, Y_val)\n",
    "    print(val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We used much more parameters and non-linear functions, so its not an apt comparison to the simple Conv Network, but anyhow, I am getting nice performance gains through this dilated network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_stack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

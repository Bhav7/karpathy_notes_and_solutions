{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EX2: Train the GPT on your own dataset of choice! What other data could be fun to blabber on about? (A fun advanced suggestion if you like: train a GPT to do addition of two numbers, i.e. a+b=c. You may find it helpful to predict the digits of c in reverse order, as the typical addition algorithm (that you're hoping it learns) would proceed right to left too. You may want to modify the data loader to simply serve random problems and skip the generation of train.bin, val.bin. You may want to mask out the loss at the input positions of a+b that just specify the problem using y=-1 in the targets (see CrossEntropyLoss ignore_index). Does your Transformer learn to add? Once you have this, swole doge project: build a calculator clone in GPT, for all of +-*/. Not an easy problem. You may need Chain of Thought traces.)\n",
    "\n",
    "Note: I only focused on teaching a Transformer how to a) add (w.o COT) and another Transformer how to b) multiply (w/ and w.o COT traces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Paper, Teaching Arithmetic to Small Transformers(https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://arxiv.org/abs/2307.03381&ved=2ahUKEwjtsdeK7MKHAxX_4skDHXZZPI8QFnoECAEQAQ&usg=AOvVaw2vTw_O8bIV2L-6v0GvzuZT) was\n",
    "very important to solving the below excercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part a: Addition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing which can be noticed below is that I have seperated out the dataset into \n",
    " - addition of single digit numbers\n",
    " - addition of double digit and single digit numbers\n",
    " - addition of double digit numbers\n",
    " - addition of three digit and double digit or single digit numbers\n",
    " - addition of three digit numbers\n",
    "\n",
    "\n",
    " I did so, such that when randomly sampling training instances (I.e a total of 10000, I can ensure that my training set has a good coverage of the potentially different set of problems)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create ADD Math Dataset \"\"\"\n",
    "math_single_dataset = []\n",
    "nums = torch.arange(0, 10)\n",
    "for num1 in nums:\n",
    "    for num2 in nums:\n",
    "        out = num1 + num2\n",
    "        math_single_dataset.append(f\"{str(num1.item()).zfill(3)}+{str(num2.item()).zfill(3)}={str(out.item()).zfill(4)[::-1]}.\")\n",
    "\n",
    "math_double_dataset_mixed = []\n",
    "nums = torch.arange(0, 99)\n",
    "for num1 in nums:\n",
    "    for num2 in nums[:10]:\n",
    "        out = num1 + num2\n",
    "        math_double_dataset_mixed.append(f\"{str(num1.item()).zfill(3)}+{str(num2.item()).zfill(3)}={str(out.item()).zfill(4)[::-1]}.\")\n",
    "\n",
    "math_double_dataset_exclusive = []\n",
    "nums = torch.arange(10, 99)\n",
    "for num1 in nums:\n",
    "    for num2 in nums:\n",
    "        out = num1 + num2\n",
    "        math_double_dataset_exclusive.append(f\"{str(num1.item()).zfill(3)}+{str(num2.item()).zfill(3)}={str(out.item()).zfill(4)[::-1]}.\")\n",
    "\n",
    "math_triple_dataset_mixed = []\n",
    "nums = torch.arange(0, 999)\n",
    "for num1 in nums:\n",
    "    for num2 in nums[:99]:\n",
    "        out = num1 + num2\n",
    "        math_triple_dataset_mixed.append(f\"{str(num1.item()).zfill(3)}+{str(num2.item()).zfill(3)}={str(out.item()).zfill(4)[::-1]}.\")\n",
    "\n",
    "math_triple_dataset_exclusive = []\n",
    "nums = torch.arange(100, 999)\n",
    "for num1 in nums:\n",
    "    for num2 in nums:\n",
    "        out = num1 + num2\n",
    "        math_triple_dataset_exclusive.append(f\"{str(num1.item()).zfill(3)}+{str(num2.item()).zfill(3)}={str(out.item()).zfill(4)[::-1]}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 990 7921 98901 808201\n"
     ]
    }
   ],
   "source": [
    "print(len(math_single_dataset),len(math_double_dataset_mixed),len(math_double_dataset_exclusive),len(math_triple_dataset_mixed),len(math_triple_dataset_exclusive))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(math_single_dataset)\n",
    "random.shuffle(math_double_dataset_mixed)\n",
    "random.shuffle(math_triple_dataset_mixed)\n",
    "random.shuffle(math_double_dataset_exclusive)\n",
    "random.shuffle(math_triple_dataset_exclusive)\n",
    "\n",
    "math_dataset = math_single_dataset + math_double_dataset_mixed[:300] + math_double_dataset_mixed[:600] + math_triple_dataset_mixed[:2000] +  math_triple_dataset_exclusive[:7000]\n",
    "random.shuffle(math_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "math_vocab = [\".\", \"+\", \"=\", \"P\"] + [str(num) for num in torch.arange(0, 10).tolist()]\n",
    "math_stoi = {char:idx for idx, char in enumerate(math_vocab)}\n",
    "math_itos = {idx:char for char, idx in math_stoi.items()}\n",
    "math_encode = lambda math_eq: [math_stoi[char] for char in math_eq]\n",
    "math_decode = lambda math_idx: \"\".join([math_itos[idx] for idx in math_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "math_dataset = [math_encode(eq) for eq in math_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12, 11, 11, 1, 10, 9, 8, 2, 5, 7, 9, 5, 0]\n",
      "877+654=1351.\n"
     ]
    }
   ],
   "source": [
    "print(math_dataset[0])\n",
    "print(math_decode(math_dataset[0]))\n",
    "\n",
    "#Remember output digits are reversed, more inline with how humans tackle addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.': 0,\n",
       " '+': 1,\n",
       " '=': 2,\n",
       " 'P': 3,\n",
       " '0': 4,\n",
       " '1': 5,\n",
       " '2': 6,\n",
       " '3': 7,\n",
       " '4': 8,\n",
       " '5': 9,\n",
       " '6': 10,\n",
       " '7': 11,\n",
       " '8': 12,\n",
       " '9': 13}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math_stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Global Variables \"\"\"\n",
    "block_size = 32\n",
    "batch_size = 16\n",
    "embed_dim = 64\n",
    "num_heads = 4\n",
    "num_blocks = 4\n",
    "lr = 1e-3\n",
    "epochs = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    if split == \"train\":\n",
    "        idxs = torch.randint(0, int(len(math_dataset)*0.9) - block_size, (batch_size,))\n",
    "    else:\n",
    "        idxs = torch.randint(int(len(math_dataset)*0.9), len(math_dataset) - block_size, (batch_size,))\n",
    "    X = []\n",
    "    Y = []\n",
    "    for idx in idxs:\n",
    "        x = math_dataset[idx] + [math_stoi[\"P\"]]*block_size #Padding to allow for batching addition examples\n",
    "        X.append(x[:block_size])\n",
    "        ignore1_idx = torch.argwhere(torch.tensor(x[:block_size]) == 2)[0][0].item() \n",
    "        ignore2_idx = torch.argwhere(torch.tensor(x[:block_size]) == 3)[0][0].item()\n",
    "        Y.append([-1]*ignore1_idx + x[ignore1_idx+1:ignore2_idx] + [-1]*(len(x[:block_size]) - ignore2_idx + 1)) #Adding target = -1 for any thing up to or before = and anything after the stop token . (so for the padding \"P\")\n",
    "    X = torch.tensor(X).to(torch.long)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create Model \"\"\"\n",
    "\n",
    "class MaskedMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, embed_dim, block_size):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.proj_dim = int(embed_dim/num_heads)\n",
    "        self.w_q = nn.Linear(embed_dim, num_heads*self.proj_dim, bias = False)\n",
    "        self.w_k = nn.Linear(embed_dim, num_heads*self.proj_dim, bias = False)\n",
    "        self.w_v = nn.Linear(embed_dim, num_heads*self.proj_dim, bias = False)\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size).to(device)))\n",
    "        self.dp = nn.Dropout(0.0)\n",
    "        self.out_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        B, T, C = input.shape #C == embed_dim\n",
    "        query = self.w_q(input).view(B, T, self.num_heads, self.proj_dim) #B, T, H, proj_dim\n",
    "        key = self.w_k(input).view(B, T, self.num_heads, self.proj_dim) #B, T, H proj_dim\n",
    "        wei = (query.permute(0,2,1,3) @ key.permute(0,2,3,1))*(C**-0.5) #B, H, T,T\n",
    "        wei = wei.masked_fill(self.tril[:T,:T] == 0, float(\"-inf\")) #Note: subset the mask self.tril[:T,:T] in case input sequence is less then block_size; the mask needs to be broadcastable with wei\n",
    "        wei = F.softmax(wei, dim = -1)\n",
    "\n",
    "        value = self.w_v(input).view(B, T, self.num_heads, self.proj_dim) #B, T, H, proj_dim\n",
    "        \n",
    "        out = wei @ value.permute(0,2,1,3) #B, H, T, proj_dim\n",
    "        out = out.permute(0,2,1,3).contiguous().view(B,T,C) #B, T, C\n",
    "        out = self.out_linear(out)  #B, T, C\n",
    "        return self.dp(out)\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.ffn = nn.Sequential(nn.Linear(embed_dim, embed_dim*4), nn.ReLU(), nn.Linear(embed_dim*4, embed_dim))\n",
    "        self.dp = nn.Dropout(0.0)\n",
    "\n",
    "    def forward(self, input):\n",
    "        out = self.ffn(input)\n",
    "        return self.dp(out)\n",
    "    \n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, num_heads, embed_dim, block_size):\n",
    "        super().__init__()\n",
    "        self.attention = MaskedMultiHeadAttention(num_heads, embed_dim, block_size)\n",
    "        self.ffn = FeedForward(embed_dim)\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.attention(self.ln1(input)) + input\n",
    "        out = self.ffn(self.ln2(x)) + x\n",
    "        return out\n",
    "    \n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, block_size, num_heads, num_blocks):\n",
    "        super().__init__()\n",
    "        self.content_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.position_embedding = nn.Embedding(block_size, embed_dim)\n",
    "        self.blocks = nn.ModuleList([Block(num_heads, embed_dim, block_size) for _ in range(num_blocks)])\n",
    "        self.output = nn.Linear(embed_dim, vocab_size)\n",
    "        self.dp = nn.Dropout(0.0)\n",
    "\n",
    "    def forward(self, input):\n",
    "        B, T = input.shape\n",
    "        con_embed = self.content_embedding(input) #B,T,embed_dim\n",
    "        pos_embed = self.position_embedding(torch.arange(T, device = device)) #1,T,embed_dim\n",
    "        x = con_embed + pos_embed #B,T,embed_dim\n",
    "        x = self.dp(x)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x) #B,T,embed_dim\n",
    "        out = self.output(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(len(math_vocab), embed_dim, block_size, num_heads, num_blocks)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(eval_iters):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            X,Y = X.to(device), Y.to(device)\n",
    "            logits  = model(X)\n",
    "            loss = F.cross_entropy(logits.permute(0, 2, 1), Y, ignore_index = -1)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Train Loss 2.662686824798584, Val Loss 2.6516246795654297\n",
      "Iteration 100, Train Loss 1.387786865234375, Val Loss 1.3856680393218994\n",
      "Iteration 200, Train Loss 1.361119270324707, Val Loss 1.3627911806106567\n",
      "Iteration 300, Train Loss 1.3566230535507202, Val Loss 1.3616020679473877\n",
      "Iteration 400, Train Loss 1.3193213939666748, Val Loss 1.3247090578079224\n",
      "Iteration 500, Train Loss 1.0440106391906738, Val Loss 1.010298252105713\n",
      "Iteration 600, Train Loss 0.5419133901596069, Val Loss 0.5412693619728088\n",
      "Iteration 700, Train Loss 0.2846592664718628, Val Loss 0.28586316108703613\n",
      "Iteration 800, Train Loss 0.1886684000492096, Val Loss 0.1880323737859726\n",
      "Iteration 900, Train Loss 0.09563055634498596, Val Loss 0.0948578342795372\n",
      "Iteration 1000, Train Loss 0.012534172274172306, Val Loss 0.012815193273127079\n",
      "Iteration 1100, Train Loss 0.06405463069677353, Val Loss 0.05420443415641785\n",
      "Iteration 1200, Train Loss 0.07592733204364777, Val Loss 0.07644709944725037\n",
      "Iteration 1300, Train Loss 0.021755538880825043, Val Loss 0.021168794482946396\n",
      "Iteration 1400, Train Loss 0.026681818068027496, Val Loss 0.023205313831567764\n",
      "Iteration 1500, Train Loss 0.0047545526176691055, Val Loss 0.005406750366091728\n",
      "Iteration 1600, Train Loss 0.0013855851721018553, Val Loss 0.0015498653519898653\n",
      "Iteration 1700, Train Loss 0.001022720243781805, Val Loss 0.000968962733168155\n",
      "Iteration 1800, Train Loss 0.0007710711797699332, Val Loss 0.0008244702476076782\n",
      "Iteration 1900, Train Loss 0.0006675763288512826, Val Loss 0.0007394618005491793\n",
      "Iteration 2000, Train Loss 0.0005853496259078383, Val Loss 0.0005978964036330581\n",
      "Iteration 2100, Train Loss 0.0004626567242667079, Val Loss 0.0004987733555026352\n",
      "Iteration 2200, Train Loss 0.0003972973790951073, Val Loss 0.0004234565712977201\n",
      "Iteration 2300, Train Loss 0.00034079051692970097, Val Loss 0.0003570562694221735\n",
      "Iteration 2400, Train Loss 0.0003520541649777442, Val Loss 0.0004134623450227082\n",
      "Iteration 2500, Train Loss 0.0002794349566102028, Val Loss 0.00031796813709661365\n",
      "Iteration 2600, Train Loss 0.0002538320259191096, Val Loss 0.00028788013150915504\n",
      "Iteration 2700, Train Loss 0.00022013344278093427, Val Loss 0.0002482897543814033\n",
      "Iteration 2800, Train Loss 0.00020879434305243194, Val Loss 0.00022653277846984565\n",
      "Iteration 2900, Train Loss 0.00019764003809541464, Val Loss 0.00021197725436650217\n",
      "Iteration 3000, Train Loss 0.00017757002206053585, Val Loss 0.00020662091264966875\n",
      "Iteration 3100, Train Loss 0.00016565227997489274, Val Loss 0.0001799389283405617\n",
      "Iteration 3200, Train Loss 0.00015108301886357367, Val Loss 0.00017063891573343426\n",
      "Iteration 3300, Train Loss 0.0001293378445552662, Val Loss 0.0001484620152041316\n",
      "Iteration 3400, Train Loss 0.00013470285921357572, Val Loss 0.00015458361303899437\n",
      "Iteration 3500, Train Loss 0.00012186026287963614, Val Loss 0.0001460736821172759\n",
      "Iteration 3600, Train Loss 0.00011144641030114144, Val Loss 0.0001244798331754282\n",
      "Iteration 3700, Train Loss 0.0001003834477160126, Val Loss 0.00011528395407367498\n",
      "Iteration 3800, Train Loss 9.045566548593342e-05, Val Loss 0.00011199430446140468\n",
      "Iteration 3900, Train Loss 8.63810273585841e-05, Val Loss 0.00010535572801018134\n",
      "Iteration 4000, Train Loss 8.68444112711586e-05, Val Loss 0.00010218092938885093\n",
      "Iteration 4100, Train Loss 7.579103112220764e-05, Val Loss 9.252496238332242e-05\n",
      "Iteration 4200, Train Loss 7.211006595753133e-05, Val Loss 8.233600965468213e-05\n",
      "Iteration 4300, Train Loss 6.678484351141378e-05, Val Loss 8.165296458173543e-05\n",
      "Iteration 4400, Train Loss 6.19616184849292e-05, Val Loss 8.463473932351917e-05\n",
      "Iteration 4500, Train Loss 5.670117388945073e-05, Val Loss 7.402832852676511e-05\n",
      "Iteration 4600, Train Loss 5.592849993263371e-05, Val Loss 6.617227336391807e-05\n",
      "Iteration 4700, Train Loss 5.021995093557052e-05, Val Loss 6.0214544646441936e-05\n",
      "Iteration 4800, Train Loss 4.844469003728591e-05, Val Loss 6.007512638461776e-05\n",
      "Iteration 4900, Train Loss 4.465488746063784e-05, Val Loss 5.459695967147127e-05\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "    x,y = get_batch('train')\n",
    "    x,y = x.to(device), y.to(device)\n",
    "    logits = model(x)\n",
    "    loss = F.cross_entropy(logits.permute(0, 2, 1), y, ignore_index = -1)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (i % 100) == 0:\n",
    "        estimated_losses = estimate_loss(200)\n",
    "        print(f\"Iteration {i}, Train Loss {estimated_losses['train']}, Val Loss {estimated_losses['val']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Function to use Transformer as an addition calculator\"\n",
    "\n",
    "@torch.no_grad()\n",
    "def addition_calculator(num1,num2):\n",
    "    pred_idx = 2\n",
    "    encoded_input = math_encode(f\"{str(num1).zfill(3)}+{str(num2).zfill(3)}=\")\n",
    "    decoded_output = []\n",
    "    while pred_idx != 0: #terminate at stop token\n",
    "        transformer_input = torch.tensor(encoded_input).view(1,-1)\n",
    "        model.eval()\n",
    "        output = model(transformer_input.to(device))\n",
    "        model.train()\n",
    "        pred_idx = torch.argmax(output[0,-1,:]).item()\n",
    "        encoded_input += [pred_idx]\n",
    "        decoded_output += [pred_idx]\n",
    "    return int(math_decode(decoded_output[::-1][1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addition_calculator(245, 63) == 245 + 63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:43<00:00,  9.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"Testing Performance on 1000 trained instances\"\n",
    "seen_examples = math_double_dataset_mixed[:600] + math_triple_dataset_mixed[:2000] +  math_triple_dataset_exclusive[:7000]\n",
    "random.seed(42)\n",
    "random.shuffle(seen_examples)\n",
    "\n",
    "amount_correct = 0\n",
    "for ex in tqdm(seen_examples[:1000]):\n",
    "    num1, num2 = ex.split(\"=\")[0].split(\"+\")\n",
    "    transformer_output = int(addition_calculator(num1, num2))\n",
    "    actual_output = int(num1) + int(num2)\n",
    "    if actual_output == transformer_output:\n",
    "        amount_correct += 1\n",
    "\n",
    "print(amount_correct/1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:27<00:00, 11.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"Testing Performance on 1000 Test instances\"\n",
    "unseen_examples = math_double_dataset_mixed[600:] + math_triple_dataset_mixed[2000:6000] +  math_triple_dataset_exclusive[7000:11000]\n",
    "random.seed(42)\n",
    "random.shuffle(unseen_examples)\n",
    "\n",
    "\n",
    "amount_correct = 0\n",
    "for ex in tqdm(unseen_examples[:1000]):\n",
    "    num1, num2 = ex.split(\"=\")[0].split(\"+\")\n",
    "    transformer_output = int(addition_calculator(num1, num2))\n",
    "    actual_output = int(num1) + int(num2)\n",
    "    if actual_output == transformer_output:\n",
    "        amount_correct += 1\n",
    "\n",
    "print(amount_correct/1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interesting bug is that in order to get correct output need number with more digits first in the expression; due to how it was trained\n",
    "\n",
    "\n",
    "Aside from that we can see that the Transformer has 100% accuracy in Addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "308"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "245 + 63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "308"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addition_calculator(245,63)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addition_calculator(63,245) #BUG which was higlighted above, due to hwo it was trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part b: Multiply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets redo exactly as above but with multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create MUL Math Dataset \"\"\"\n",
    "math_single_dataset = []\n",
    "nums = torch.arange(0, 10)\n",
    "for num1 in nums:\n",
    "    for num2 in nums:\n",
    "        out = num1 * num2\n",
    "        math_single_dataset.append(f\"{str(num1.item()).zfill(3)}*{str(num2.item()).zfill(3)}={str(out.item()).zfill(6)[::-1]}.\")\n",
    "\n",
    "math_double_dataset_mixed = []\n",
    "nums = torch.arange(0, 99)\n",
    "for num1 in nums:\n",
    "    for num2 in nums[:10]:\n",
    "        out = num1 * num2\n",
    "        math_double_dataset_mixed.append(f\"{str(num1.item()).zfill(3)}*{str(num2.item()).zfill(3)}={str(out.item()).zfill(6)[::-1]}.\")\n",
    "\n",
    "math_double_dataset_exclusive = []\n",
    "nums = torch.arange(10, 99)\n",
    "for num1 in nums:\n",
    "    for num2 in nums:\n",
    "        out = num1 * num2\n",
    "        math_double_dataset_exclusive.append(f\"{str(num1.item()).zfill(3)}*{str(num2.item()).zfill(3)}={str(out.item()).zfill(6)[::-1]}.\")\n",
    "\n",
    "math_triple_dataset_mixed = []\n",
    "nums = torch.arange(0, 999)\n",
    "for num1 in nums:\n",
    "    for num2 in nums[:99]:\n",
    "        out = num1 * num2\n",
    "        math_triple_dataset_mixed.append(f\"{str(num1.item()).zfill(3)}*{str(num2.item()).zfill(3)}={str(out.item()).zfill(6)[::-1]}.\")\n",
    "\n",
    "math_triple_dataset_exclusive = []\n",
    "nums = torch.arange(100, 999)\n",
    "for num1 in nums:\n",
    "    for num2 in nums:\n",
    "        out = num1 * num2\n",
    "        math_triple_dataset_exclusive.append(f\"{str(num1.item()).zfill(3)}*{str(num2.item()).zfill(3)}={str(out.item()).zfill(6)[::-1]}.\")\n",
    "\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(math_single_dataset)\n",
    "random.shuffle(math_double_dataset_mixed)\n",
    "random.shuffle(math_triple_dataset_mixed)\n",
    "random.shuffle(math_double_dataset_exclusive)\n",
    "random.shuffle(math_triple_dataset_exclusive)\n",
    "\n",
    "math_dataset = math_single_dataset + math_double_dataset_mixed[:300] + math_double_dataset_mixed[:600] + math_triple_dataset_mixed[:2000] +  math_triple_dataset_exclusive[:7000]\n",
    "random.shuffle(math_dataset)\n",
    "\n",
    "math_vocab = [\".\", \"*\", \"=\", \"P\"] + [str(num) for num in torch.arange(0, 10).tolist()]\n",
    "math_stoi = {char:idx for idx, char in enumerate(math_vocab)}\n",
    "math_itos = {idx:char for char, idx in math_stoi.items()}\n",
    "math_encode = lambda math_eq: [math_stoi[char] for char in math_eq]\n",
    "math_decode = lambda math_idx: \"\".join([math_itos[idx] for idx in math_idx])\n",
    "\n",
    "math_dataset = [math_encode(eq) for eq in math_dataset]\n",
    "\n",
    "def get_batch(split):\n",
    "    if split == \"train\":\n",
    "        idxs = torch.randint(0, int(len(math_dataset)*0.9) - block_size, (batch_size,))\n",
    "    else:\n",
    "        idxs = torch.randint(int(len(math_dataset)*0.9), len(math_dataset) - block_size, (batch_size,))\n",
    "    X = []\n",
    "    Y = []\n",
    "    for idx in idxs:\n",
    "        x = math_dataset[idx] + [math_stoi[\"P\"]]*block_size \n",
    "        X.append(x[:block_size])\n",
    "        ignore1_idx = torch.argwhere(torch.tensor(x[:block_size]) == 2)[0][0].item()\n",
    "        ignore2_idx = torch.argwhere(torch.tensor(x[:block_size]) == 3)[0][0].item()\n",
    "        Y.append([-1]*ignore1_idx + x[ignore1_idx+1:ignore2_idx] + [-1]*(len(x[:block_size]) - ignore2_idx + 1))\n",
    "    X = torch.tensor(X).to(torch.long)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12, 11, 11, 1, 10, 9, 8, 2, 12, 9, 9, 7, 11, 9, 0]\n",
      "877*654=855375.\n"
     ]
    }
   ],
   "source": [
    "print(math_dataset[0])\n",
    "print(math_decode(math_dataset[0]))\n",
    "#digits reversed for output just like how it was done for addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 253 #Bug created when training prior, but since model was trained for hours probs fine to keep\n",
    "model = Transformer(len(math_vocab), embed_dim, block_size, num_heads, num_blocks)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(eval_iters):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            X,Y = X.to(device), Y.to(device)\n",
    "            logits  = model(X)\n",
    "            loss = F.cross_entropy(logits.permute(0, 2, 1), Y, ignore_index = -1)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So I trained the model in another file for good number of hours, so I'll import those weights and show it can train here for a few epochs to higlight the code work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"/Users/bhavverma/Documents/karpathy_notes_and_solutions/video_7_dependencies/rev_model_weights.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Train Loss 0.6741361021995544, Val Loss 1.2719918489456177\n",
      "Iteration 100, Train Loss 0.6755880117416382, Val Loss 1.2324095964431763\n",
      "Iteration 200, Train Loss 0.6660304069519043, Val Loss 1.2309340238571167\n",
      "Iteration 300, Train Loss 0.6705098152160645, Val Loss 1.2488014698028564\n",
      "Iteration 400, Train Loss 0.681050181388855, Val Loss 1.281855821609497\n"
     ]
    }
   ],
   "source": [
    "for i in range(500):\n",
    "    x,y = get_batch('train')\n",
    "    x,y = x.to(device), y.to(device)\n",
    "    logits = model(x)\n",
    "    loss = F.cross_entropy(logits.permute(0, 2, 1), y, ignore_index = -1)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (i % 100) == 0:\n",
    "        estimated_losses = estimate_loss(200)\n",
    "        print(f\"Iteration {i}, Train Loss {estimated_losses['train']}, Val Loss {estimated_losses['val']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def mul_calculator(num1,num2):\n",
    "    pred_idx = 2\n",
    "    encoded_input = math_encode(f\"{str(num1).zfill(3)}*{str(num2).zfill(3)}=\")\n",
    "    decoded_output = []\n",
    "    while pred_idx != 0:\n",
    "        transformer_input = torch.tensor(encoded_input).view(1,-1)\n",
    "        model.eval()\n",
    "        output = model(transformer_input.to(device))\n",
    "        model.train()\n",
    "        pred_idx = torch.argmax(output[0,-1,:]).item()\n",
    "        encoded_input += [pred_idx]\n",
    "        decoded_output += [pred_idx]\n",
    "    return int(math_decode(decoded_output[::-1][1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mul_calculator(9,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['325*925=526003.', '938*943=435488.', '910*201=019281.', '906*477=261234.', '995*033=538230.', '356*342=257121.', '462*384=804771.', '722*699=876405.', '935*419=567193.', '074*004=692000.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:15<00:00, 13.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"Testing Performance on 1000 trained instances\"\n",
    "seen_examples = math_double_dataset_mixed[:600] + math_triple_dataset_mixed[:2000] +  math_triple_dataset_exclusive[:7000]\n",
    "random.seed(42)\n",
    "random.shuffle(seen_examples)\n",
    "\n",
    "print(seen_examples[:10])\n",
    "\n",
    "amount_correct = 0\n",
    "for ex in tqdm(seen_examples[:1000]):\n",
    "    num1, num2 = ex.split(\"=\")[0].split(\"*\")\n",
    "    transformer_output = int(mul_calculator(num1, num2))\n",
    "    actual_output = int(num1) * int(num2)\n",
    "    if actual_output == transformer_output:\n",
    "        amount_correct += 1\n",
    "\n",
    "print(amount_correct/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['097*057=925500.', '094*005=074000.', '053*077=180400.', '925*874=054808.', '472*336=295851.', '826*016=612310.', '590*593=078943.', '655*928=048706.', '826*489=419304.', '879*105=592290.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/1000 [00:00<03:05,  5.37it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:13<00:00, 13.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"Testing Performance on 1000 Train instances\"\n",
    "unseen_examples = math_double_dataset_mixed[600:] + math_triple_dataset_mixed[2000:6000] +  math_triple_dataset_exclusive[7000:11000]\n",
    "random.seed(42)\n",
    "random.shuffle(unseen_examples)\n",
    "\n",
    "print(unseen_examples[:10])\n",
    "\n",
    "amount_correct = 0\n",
    "for ex in tqdm(unseen_examples[:1000]):\n",
    "    num1, num2 = ex.split(\"=\")[0].split(\"*\")\n",
    "    transformer_output = int(mul_calculator(num1, num2))\n",
    "    actual_output = int(num1) * int(num2)\n",
    "    if actual_output == transformer_output:\n",
    "        amount_correct += 1\n",
    "\n",
    "print(amount_correct/1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that using the same method for addition does not work multiplication, it gives a horrible result. Instead lets aim to use chain of thought traces, where each training instance, is more like a step by step solution to a multiplication problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create Chain of Thought Template \"\"\"\n",
    "\n",
    "def mul_cot_format(num1, num2):\n",
    "    str_num1, str_num2 = str(num1), str(num2)\n",
    "\n",
    "    a_1 = num1*int(str_num2.zfill(3)[-1])\n",
    "    b_1 = a_1\n",
    "    c_1 = b_1\n",
    "\n",
    "    a_2 = num1*int(str_num2.zfill(3)[-2])\n",
    "    b_2 = a_2*10\n",
    "    c_2 = c_1 + b_2\n",
    "\n",
    "    a_3 = num1*int(str_num2.zfill(3)[-3])\n",
    "    b_3 = a_3*100\n",
    "    c_3 = c_2 + b_3\n",
    "\n",
    "    a = [a_1, a_2, a_3]\n",
    "    b = [b_1, b_2, b_3]\n",
    "    c = [c_1, c_2, c_3]\n",
    "\n",
    "    k = [1,10,100]\n",
    "\n",
    "    sol = str(num1*num2)\n",
    "\n",
    "    input_str = f\"In:{str_num1}*{str_num2}\\n\"\n",
    "    total_target = [f\"Target:[{','.join(str_num1)}]has{len(str_num1)}digits.[{','.join(str_num2)}]has{len(str_num2)}digits.\"]\n",
    "    for i in range(len(str_num2)):\n",
    "        past_c = 0 if i == 0 else c[i-1]\n",
    "        total_target.append(f\"[{','.join(str_num1)}]*{str_num2[::-1][i]},A=[{','.join(str(a[i]).zfill(len(str_num1)))}],k={k[i]},B=[{','.join(str(b[i]).zfill(len(str_num1)+i))}],C={str(past_c)}+{str(b[i])}={str(c[i])}\" + '\\n')\n",
    "    total_target.append(f\",END{''.join(sol)};\")\n",
    "    target = \"\".join(total_target)\n",
    "\n",
    "    return input_str + target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In:12*360\\nTarget:[1,2]has2digits.[3,6,0]has3digits.[1,2]*0,A=[0,0],k=1,B=[0,0],C=0+0=0\\n[1,2]*6,A=[7,2],k=10,B=[7,2,0],C=0+720=720\\n[1,2]*3,A=[3,6],k=100,B=[3,6,0,0],C=720+3600=4320\\n,END4320;'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mul_cot_format(12, 360)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Need to recreate MUL Math Dataset so that formatting works with COT template\"\"\"\n",
    "math_single_dataset = []\n",
    "nums = torch.arange(0, 10)\n",
    "for num1 in nums:\n",
    "    for num2 in nums:\n",
    "        math_single_dataset.append((num1,num2))\n",
    "\n",
    "math_double_dataset_mixed = []\n",
    "nums = torch.arange(0, 99)\n",
    "for num1 in nums:\n",
    "    for num2 in nums[:10]:\n",
    "        math_double_dataset_mixed.append((num1,num2))\n",
    "\n",
    "math_double_dataset_exclusive = []\n",
    "nums = torch.arange(10, 99)\n",
    "for num1 in nums:\n",
    "    for num2 in nums:\n",
    "        math_double_dataset_exclusive.append((num1,num2))\n",
    "        \n",
    "\n",
    "math_triple_dataset_mixed = []\n",
    "nums = torch.arange(0, 999)\n",
    "for num1 in nums:\n",
    "    for num2 in nums[:99]:\n",
    "        out = num1 * num2\n",
    "        math_triple_dataset_mixed.append((num1,num2))\n",
    "\n",
    "math_triple_dataset_exclusive = []\n",
    "nums = torch.arange(100, 999)\n",
    "for num1 in nums:\n",
    "    for num2 in nums:\n",
    "        out = num1 * num2\n",
    "        math_triple_dataset_exclusive.append((num1,num2))\n",
    "\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(math_single_dataset)\n",
    "random.shuffle(math_double_dataset_mixed)\n",
    "random.shuffle(math_triple_dataset_mixed)\n",
    "random.shuffle(math_double_dataset_exclusive)\n",
    "random.shuffle(math_triple_dataset_exclusive)\n",
    "\n",
    "math_dataset = math_single_dataset + math_double_dataset_mixed[:300] + math_double_dataset_mixed[:600] + math_triple_dataset_mixed[:2000] +  math_triple_dataset_exclusive[:7000]\n",
    "random.shuffle(math_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "math_vocab = list(sorted(set([\".\", \"P\"] + [str(num) for num in torch.arange(0, 10).tolist()] + list(set(mul_cot_format(12,36))))))\n",
    "math_stoi = {char:idx for idx, char in enumerate(math_vocab)}\n",
    "math_itos = {idx:char for char, idx in math_stoi.items()}\n",
    "math_encode = lambda cot_input: [math_stoi[char] for char in cot_input]\n",
    "math_decode = lambda m_output: \"\".join([math_itos[idx] for idx in m_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, '*': 1, '+': 2, ',': 3, '.': 4, '0': 5, '1': 6, '2': 7, '3': 8, '4': 9, '5': 10, '6': 11, '7': 12, '8': 13, '9': 14, ':': 15, ';': 16, '=': 17, 'A': 18, 'B': 19, 'C': 20, 'D': 21, 'E': 22, 'I': 23, 'N': 24, 'P': 25, 'T': 26, '[': 27, ']': 28, 'a': 29, 'd': 30, 'e': 31, 'g': 32, 'h': 33, 'i': 34, 'k': 35, 'n': 36, 'r': 37, 's': 38, 't': 39}\n"
     ]
    }
   ],
   "source": [
    "print(math_stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important indices: \n",
    "15 -> \":\", this is delimiter we will use during evaluation, in which only the chars before will be given\n",
    "16 -> \";\", this is stop token\n",
    "25 -> \"P\", this is padding token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In:12*36\\nTarget:[1,2]has2digits.[3,6]has2digits.[1,2]*6,A=[7,2],k=1,B=[7,2],C=0+72=72\\n[1,2]*3,A=[3,6],k=10,B=[3,6,0],C=72+360=432\\n,END432;'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math_decode(math_encode((mul_cot_format(12,36))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "math_dataset = [math_encode(mul_cot_format(num_pairs[0].item(),num_pairs[1].item())) for num_pairs in math_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243\n"
     ]
    }
   ],
   "source": [
    "#Setting block size to longest COT template in training data + 10 (give breathing room)\n",
    "longest_length = 0 \n",
    "for i in math_dataset:\n",
    "    if len(i) >= longest_length:\n",
    "        longest_length = len(i)\n",
    "print(longest_length)\n",
    "block_size = longest_length + 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    if split == \"train\":\n",
    "        idxs = torch.randint(0, int(len(math_dataset)*0.9), (batch_size,))\n",
    "    else:\n",
    "        idxs = torch.randint(int(len(math_dataset)*0.9), len(math_dataset), (batch_size,))\n",
    "    X = []\n",
    "    Y = []\n",
    "    for idx in idxs:\n",
    "        x = math_dataset[idx] + [math_stoi[\"P\"]]*block_size\n",
    "        ignore_idx1 = torch.argwhere(torch.tensor(x[:block_size]) == 15)[1][0].item()\n",
    "        ignore_idx2 = torch.argwhere(torch.tensor(x[:block_size]) == 25)[0][0].item()\n",
    "        X.append(x[:block_size])\n",
    "        Y.append([-1]*(len(x[:ignore_idx1])) + x[ignore_idx1 + 1 : ignore_idx2] + [-1]*(len(x[:block_size]) - ignore_idx2 + 1)) #-1 targets for anything up to and including target: and -1 for all pad tokens\n",
    "    X = torch.tensor(X).to(torch.long)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(len(math_vocab), embed_dim, block_size, num_heads, num_blocks)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(eval_iters):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            X,Y = X.to(device), Y.to(device)\n",
    "            logits  = model(X)\n",
    "            loss = F.cross_entropy(logits.permute(0, 2, 1), Y, ignore_index = -1)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, I trained the model in another file for good number of hours, so I'll import those weights and show it can train here for a few epochs to higlight the code work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"/Users/bhavverma/Documents/karpathy_notes_and_solutions/video_7_dependencies/cot_model_weights.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Train Loss 0.005612213164567947, Val Loss 0.010330012068152428\n",
      "Iteration 100, Train Loss 0.004963007289916277, Val Loss 0.010579774156212807\n",
      "Iteration 200, Train Loss 0.005102494265884161, Val Loss 0.009807861410081387\n",
      "Iteration 300, Train Loss 0.004419269040226936, Val Loss 0.008675486780703068\n",
      "Iteration 400, Train Loss 0.0068982369266450405, Val Loss 0.010805675759911537\n"
     ]
    }
   ],
   "source": [
    "for i in range(500):\n",
    "    x,y = get_batch('train')\n",
    "    x,y = x.to(device), y.to(device)\n",
    "    logits = model(x)\n",
    "    loss = F.cross_entropy(logits.permute(0, 2, 1), y, ignore_index = -1)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (i % 100) == 0:\n",
    "        estimated_losses = estimate_loss(200)\n",
    "        print(f\"Iteration {i}, Train Loss {estimated_losses['train']}, Val Loss {estimated_losses['val']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def mul_calculator(num1,num2):\n",
    "    encoded_input = math_encode(mul_cot_format(num1, num2))\n",
    "    ignore_idx1 = torch.argwhere(torch.tensor(encoded_input) == 15)[1][0].item()\n",
    "    encoded_input = encoded_input[:ignore_idx1+1]\n",
    "    print(math_decode(encoded_input))\n",
    "    pred_idx = None\n",
    "    decoded_output = []\n",
    "    while pred_idx != 16:\n",
    "        transformer_input = torch.tensor(encoded_input).view(1,-1)\n",
    "        model.eval()\n",
    "        output = model(transformer_input.to(device))\n",
    "        model.train()\n",
    "        pred_idx = torch.argmax(output[0,-1,:]).item()\n",
    "        encoded_input += [pred_idx]\n",
    "        decoded_output += [pred_idx]\n",
    "    return math_decode(decoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In:460*205\n",
      "Target:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[4,6,0]has3digits.[2,0,5]has3digits.[4,6,0]*5,A=[2,3,0,0],k=1,B=[2,3,0,0],C=0+2300=2300\\n[4,6,0]*0,A=[0,0,0],k=10,B=[0,0,0,0],C=2300+0=2300\\n[4,6,0]*2,A=[9,2,0],k=100,B=[9,2,0,0,0],C=2300+92000=94300\\n,END94300;'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Showing with print statement the initial prompt, will redefine the fuc without print statement\n",
    "mul_calculator(460, 205)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94300"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "460*205"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def mul_calculator(num1,num2):\n",
    "    encoded_input = math_encode(mul_cot_format(num1, num2))\n",
    "    ignore_idx1 = torch.argwhere(torch.tensor(encoded_input) == 15)[1][0].item()\n",
    "    encoded_input = encoded_input[:ignore_idx1+1]\n",
    "    pred_idx = None\n",
    "    decoded_output = []\n",
    "    while pred_idx != 16:\n",
    "        transformer_input = torch.tensor(encoded_input).view(1,-1)\n",
    "        model.eval()\n",
    "        output = model(transformer_input.to(device))\n",
    "        model.train()\n",
    "        pred_idx = torch.argmax(output[0,-1,:]).item()\n",
    "        encoded_input += [pred_idx]\n",
    "        decoded_output += [pred_idx]\n",
    "    return math_decode(decoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(tensor(325), tensor(925)), (tensor(938), tensor(943)), (tensor(910), tensor(201)), (tensor(906), tensor(477)), (tensor(995), tensor(33)), (tensor(356), tensor(342)), (tensor(462), tensor(384)), (tensor(722), tensor(699)), (tensor(935), tensor(419)), (tensor(74), tensor(4))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [56:53<00:00,  3.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"Testing Performance on 1000 trained instances\"\n",
    "seen_examples = math_double_dataset_mixed[:600] + math_triple_dataset_mixed[:2000] +  math_triple_dataset_exclusive[:7000]\n",
    "random.seed(42)\n",
    "random.shuffle(seen_examples)\n",
    "\n",
    "print(seen_examples[:10])\n",
    "\n",
    "amount_correct = 0\n",
    "for ex in tqdm(seen_examples[:1000]):\n",
    "    num1, num2 = ex[0].item(), ex[1].item()\n",
    "    transformer_output = mul_calculator(num1, num2)\n",
    "    if not(\"END\" in transformer_output):\n",
    "        continue\n",
    "    else:\n",
    "        transformer_output = transformer_output.split(\"END\")[1][:-1]\n",
    "    actual_output = num1 * num2\n",
    "    if str(actual_output) == str(transformer_output):\n",
    "        amount_correct += 1\n",
    "\n",
    "print(amount_correct/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(tensor(97), tensor(57)), (tensor(94), tensor(5)), (tensor(53), tensor(77)), (tensor(925), tensor(874)), (tensor(472), tensor(336)), (tensor(826), tensor(16)), (tensor(590), tensor(593)), (tensor(655), tensor(928)), (tensor(826), tensor(489)), (tensor(879), tensor(105))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [30:26<00:00,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"Testing Performance on 1000 Train instances\"\n",
    "unseen_examples = math_double_dataset_mixed[600:] + math_triple_dataset_mixed[2000:6000] +  math_triple_dataset_exclusive[7000:11000]\n",
    "random.seed(42)\n",
    "random.shuffle(unseen_examples)\n",
    "\n",
    "print(unseen_examples[:10])\n",
    "\n",
    "amount_correct = 0\n",
    "for ex in tqdm(unseen_examples[:1000]):\n",
    "    num1, num2 = ex[0].item(), ex[1].item()\n",
    "    transformer_output = mul_calculator(num1, num2)\n",
    "    if not(\"END\" in transformer_output):\n",
    "        continue\n",
    "    else:\n",
    "        transformer_output = transformer_output.split(\"END\")[1][:-1]\n",
    "    actual_output = num1 * num2\n",
    "    if str(actual_output) == str(transformer_output):\n",
    "        amount_correct += 1\n",
    "\n",
    "print(amount_correct/1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can clearly see COT tracing provided substantial benefits to allowing the transformer to learn multiplication, with it increasing from an aprox 4% accuracy to 64% accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_stack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
